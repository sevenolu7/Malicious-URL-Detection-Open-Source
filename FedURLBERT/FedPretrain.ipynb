{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a54f5c-0d29-4b71-9800-08324e26e4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./bert_tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "# 创建分词器\n",
    "bwpt = tokenizers.BertWordPieceTokenizer()\n",
    "filepath = \"./dataset.txt\"\n",
    "#训练分词器\n",
    "bwpt.train(\n",
    "    files=[filepath],\n",
    "    vocab_size=1000,\n",
    "    min_frequency=1,\n",
    "    limit_alphabet=1000\n",
    ")\n",
    "\n",
    "bwpt.save_model('./bert_tokenizer/')\n",
    "#output： ['./pretrained_models/vocab.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9532b52d-9f7e-4b76-9e17-ecd8831fd23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "bert_tokenizer = BertTokenizer(vocab_file=\"./bert_tokenizer/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262012e6-351f-473f-a0ab-55603ed6e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_595/2670194097.py:3: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"./train.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381c77be-1298-4f0b-bfe1-d93b17d05d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"./bert-base-uncased-model/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.33.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 1000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(1000, 768, padding_idx=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,DataCollatorForLanguageModeling,HfArgumentParser,Trainer,TrainingArguments,set_seed,\n",
    ")\n",
    "# 自己修改部分配置参数\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": None,\n",
    "    \"revision\": 'main',\n",
    "    \"use_auth_token\": None,\n",
    "    #      \"hidden_size\": 512,\n",
    "    #     \"num_attention_heads\": 4,\n",
    "    \"hidden_dropout_prob\": 0.2,\n",
    "    \"vocab_size\": 1000\n",
    "}\n",
    "\n",
    "config = AutoConfig.from_pretrained('./bert-base-uncased-model/', **config_kwargs)\n",
    "print(config)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_config(\n",
    "    config=config,\n",
    ")\n",
    "model.resize_token_embeddings(len(bert_tokenizer))\n",
    "#output:Embedding(863, 768, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e28dd48-6b9d-4a11-9fbb-e730c8b9cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model_client = model.bert.embeddings\n",
    "model_server1 = model.bert.encoder\n",
    "model_server2 = model.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93d6cd0-ebd3-4638-8dc9-242b51e09236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from pandas import DataFrame\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90675179-4c30-4b64-9638-3a9ed69a76e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100 80GB PCIe\n",
      "---------SFLV1 Bert PreTrain on Phishing----------\n"
     ]
    }
   ],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(torch.cuda.get_device_name(0))    \n",
    "\n",
    "#===================================================================\n",
    "program = \"SFLV1 Bert PreTrain on Phishing\"\n",
    "print(f\"---------{program}----------\")              # this is to identify the program in the slurm outputs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9e0f91e-240c-48ec-9639-c81926a90a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk)) \n",
    "def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))     \n",
    "\n",
    "#===================================================================\n",
    "\n",
    "num_users = 5\n",
    "epochs = 10\n",
    "frac = 1\n",
    "lr = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294b6109-c7f4-41ff-90b7-14d8ea3cf4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert_client_side(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(1000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#=====================================================================================================\n",
    "#                           Client-side Model definition\n",
    "#=====================================================================================================\n",
    "# Model at client side\n",
    "\n",
    "class Bert_client_side(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_client_side, self).__init__()\n",
    "        self.embeddings = copy.deepcopy(model_client)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        return embedding_output\n",
    "\n",
    "net_glob_client = Bert_client_side()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We use\",torch.cuda.device_count(), \"GPUs\")\n",
    "    net_glob_client = nn.DataParallel(net_glob_client)\n",
    "\n",
    "net_glob_client.to(device)\n",
    "print(net_glob_client) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc6f0c8d-2480-4762-96bc-70c838fd4d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bert_server_side(\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================================================\n",
    "#                           Server-side Model definition\n",
    "#=====================================================================================================\n",
    "# Model at server side\n",
    "\n",
    "class Bert_server_side(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_server_side, self).__init__()\n",
    "        self.encoder = copy.deepcopy(model_server1)\n",
    "        self.cls = copy.deepcopy(model_server2)\n",
    "\n",
    "    def forward(self, embedding_output):\n",
    "        output_encoder = self.encoder(embedding_output).last_hidden_state\n",
    "        output = self.cls(output_encoder)\n",
    "        return output\n",
    "\n",
    "net_glob_server = Bert_server_side()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We use\",torch.cuda.device_count(), \"GPUs\")\n",
    "    net_glob_server = nn.DataParallel(net_glob_server)   # to use the multiple GPUs \n",
    "\n",
    "net_glob_server.to(device)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94fc731b-e8f1-43f7-b64a-a497ba5ff75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================================================\n",
    "# For Server Side Loss and Accuracy\n",
    "loss_train_collect = []\n",
    "acc_train_collect = []\n",
    "loss_test_collect = []\n",
    "acc_test_collect = []\n",
    "batch_acc_train = []\n",
    "batch_loss_train = []\n",
    "batch_acc_test = []\n",
    "batch_loss_test = []\n",
    "count1 = 0\n",
    "count2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2ed3daf-ce00-4163-86d8-e9f91d73dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================\n",
    "#                                  Server Side Program\n",
    "#====================================================================================================\n",
    "# Federated averaging: FedAvg\n",
    "import datetime\n",
    "def FedAvg(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[k] += w[i][k]\n",
    "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
    "    return w_avg\n",
    "\n",
    "\n",
    "def calculate_accuracy(fx, y):\n",
    "    preds = fx.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(y.view_as(preds)).sum()\n",
    "    acc = 100.00 *correct.float()/preds.shape[0]\n",
    "    return acc\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3520d573-98e9-498e-9cc6-67ed328ab131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print train - test together in each round-- these are made global\n",
    "acc_avg_all_user_train = 0\n",
    "loss_avg_all_user_train = 0\n",
    "loss_train_collect_user = []\n",
    "acc_train_collect_user = []\n",
    "loss_test_collect_user = []\n",
    "acc_test_collect_user = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5464310f-223a-4b67-81e1-ca4b20e5f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_glob_server = net_glob_server.state_dict()\n",
    "w_locals_server = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9abc6cff-c29c-44b1-b0f3-08802e0462ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client idx collector\n",
    "idx_collect = []\n",
    "l_epoch_check = False\n",
    "fed_check = False\n",
    "# Initialization of net_model_server and net_server (server-side model)\n",
    "net_model_server = [net_glob_server for i in range(num_users)]\n",
    "net_server = copy.deepcopy(net_model_server[0]).to(device)\n",
    "#optimizer_server = torch.optim.Adam(net_server.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad96b24a-464a-4ddb-add9-cf98fb31158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Server-side function associated with Training\n",
    "def train_server(fx_client, labels, l_epoch_count, l_epoch, idx, len_batch):\n",
    "    global net_model_server, criterion, device, batch_acc_train, batch_loss_train, l_epoch_check, fed_check\n",
    "    global loss_train_collect, acc_train_collect, count1, acc_avg_all_user_train, loss_avg_all_user_train, idx_collect, w_locals_server, w_glob_server, net_server\n",
    "    global loss_train_collect_user, acc_train_collect_user, lr\n",
    "    train_start_time = time.time() - t0\n",
    "    net_server = copy.deepcopy(net_model_server[idx]).to(device)\n",
    "    net_server.train()\n",
    "    optimizer_server = torch.optim.Adam(net_server.parameters(), lr = lr)\n",
    "\n",
    "\n",
    "    # train and update\n",
    "    optimizer_server.zero_grad()\n",
    "\n",
    "    fx_client = fx_client.to(device)\n",
    "\n",
    "    #---------forward prop-------------\n",
    "    output = net_server(fx_client)\n",
    "    loss = criterion(output.view(-1, len(bert_tokenizer)), labels.view(-1))\n",
    "    #--------backward prop--------------\n",
    "    loss.backward()\n",
    "    dfx_client = fx_client.grad.clone().detach()\n",
    "    optimizer_server.step()\n",
    "\n",
    "    batch_loss_train.append(loss.item())\n",
    "\n",
    "    # Update the server-side model for the current batch\n",
    "    net_model_server[idx] = copy.deepcopy(net_server)\n",
    "\n",
    "    # count1: to track the completion of the local batch associated with one client\n",
    "    count1 += 1\n",
    "    if count1 == len_batch:\n",
    "\n",
    "        loss_avg_train = sum(batch_loss_train)/len(batch_loss_train)\n",
    "\n",
    "        batch_loss_train = []\n",
    "        count1 = 0\n",
    "\n",
    "        train_time = time.time() - train_start_time\n",
    "        elapsed_train = format_time(train_time)\n",
    "        prRed('Client{} Train => Local Epoch: {} \\tLoss: {:.20f} \\tTrain cost: {:}'.format(idx, l_epoch_count, loss_avg_train, elapsed_train))\n",
    "\n",
    "        # copy the last trained model in the batch\n",
    "        w_server = net_server.state_dict()\n",
    "\n",
    "        # If one local epoch is completed, after this a new client will come\n",
    "        if l_epoch_count == l_epoch-1:\n",
    "\n",
    "            l_epoch_check = True                # to evaluate_server function - to check local epoch has completed or not\n",
    "            # We store the state of the net_glob_server()\n",
    "            w_locals_server.append(copy.deepcopy(w_server))\n",
    "\n",
    "            # we store the last accuracy in the last batch of the epoch and it is not the average of all local epochs\n",
    "            # this is because we work on the last trained model and its accuracy (not earlier cases)\n",
    "\n",
    "            loss_avg_train_all = loss_avg_train\n",
    "\n",
    "            # accumulate accuracy and loss for each new user\n",
    "            loss_train_collect_user.append(loss_avg_train_all)\n",
    "\n",
    "            # collect the id of each new user\n",
    "            if idx not in idx_collect:\n",
    "                idx_collect.append(idx)\n",
    "                #print(idx_collect)\n",
    "\n",
    "        # This is for federation process--------------------  所有客户端的学习都完成一轮，重新更新统计参数\n",
    "        if len(idx_collect) == num_users:\n",
    "            fed_check = True                                                  # to evaluate_server function  - to check fed check has hitted\n",
    "            # Federation process at Server-Side------------------------- output print and update is done in evaluate_server()\n",
    "            # for nicer display\n",
    "\n",
    "            w_glob_server = FedAvg(w_locals_server)   # server端权重更新\n",
    "\n",
    "            # server-side global model update and distribute that model to all clients ------------------------------\n",
    "            net_glob_server.load_state_dict(w_glob_server)\n",
    "            net_model_server = [net_glob_server for i in range(num_users)]\n",
    "\n",
    "            w_locals_server = []\n",
    "            idx_collect = []\n",
    "\n",
    "            loss_avg_all_user_train = sum(loss_train_collect_user)/len(loss_train_collect_user)\n",
    "\n",
    "            loss_train_collect.append(loss_avg_all_user_train)\n",
    "\n",
    "            loss_train_collect_user = []\n",
    "\n",
    "    # send gradients to the client\n",
    "    return dfx_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0f03d59-a12c-4c82-a0e1-56c2e2b8c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server-side functions associated with Testing\n",
    "def evaluate_server(fx_client, labels, idx, len_batch, ell):\n",
    "    global net_model_server, criterion, batch_acc_test, batch_loss_test, check_fed, net_server, net_glob_server\n",
    "    global loss_test_collect, acc_test_collect, count2, num_users, acc_avg_train_all, loss_avg_train_all, w_glob_server, l_epoch_check, fed_check\n",
    "    global loss_test_collect_user, acc_test_collect_user, acc_avg_all_user_train, loss_avg_all_user_train\n",
    "\n",
    "    eval_start_time = time.time() - t0\n",
    "    net = copy.deepcopy(net_model_server[idx]).to(device)\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fx_client = fx_client.to(device)\n",
    "        #---------forward prop-------------\n",
    "        output = net(fx_client)\n",
    "        loss = criterion(output.view(-1, len(bert_tokenizer)), labels.view(-1))\n",
    "        # calculate loss\n",
    "\n",
    "        batch_loss_test.append(loss.item())\n",
    "\n",
    "        count2 += 1\n",
    "        if count2 == len_batch:\n",
    "            loss_avg_test = sum(batch_loss_test)/len(batch_loss_test)\n",
    "            eval_time = time.time() - eval_start_time\n",
    "            elapsed_eval = format_time(eval_time)\n",
    "            batch_loss_test = []\n",
    "            count2 = 0\n",
    "\n",
    "            prGreen('Client{} Test =>                 \\tLoss: {:.20f} \\tTest cost: {:}'.format(idx, loss_avg_test, elapsed_eval))\n",
    "\n",
    "            # if a local epoch is completed\n",
    "            if l_epoch_check:\n",
    "                l_epoch_check = False\n",
    "\n",
    "                loss_avg_test_all = loss_avg_test   # 一个客户端的测试效果\n",
    "\n",
    "                loss_test_collect_user.append(loss_avg_test_all)\n",
    "\n",
    "            # if federation is happened----------\n",
    "            if fed_check:\n",
    "                fed_check = False\n",
    "                print(\"------------------------------------------------\")\n",
    "                print(\"------ Federation process at Server-Side ------- \")\n",
    "                print(\"------------------------------------------------\")\n",
    "\n",
    "                loss_avg_all_user = sum(loss_test_collect_user)/len(loss_test_collect_user)\n",
    "\n",
    "                loss_test_collect.append(loss_avg_all_user)\n",
    "                loss_test_collect_user= []\n",
    "\n",
    "                print(\"====================== SERVER V1==========================\")\n",
    "                print(' Train: Round {:3d} | Avg Loss {:.20f}'.format(ell, loss_avg_all_user_train))\n",
    "                print(' Test: Round {:3d} | Avg Loss {:.20f}'.format(ell, loss_avg_all_user))\n",
    "                print(\"==========================================================\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5976f62f-5ba9-4a3a-9579-404ef3732dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================================\n",
    "#                                       Clients-side Program\n",
    "#==============================================================================================================\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        domain, atten_mask, labels = self.dataset[self.idxs[item]]\n",
    "        return domain, atten_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0036d36d-8ac9-4047-91eb-c47995730e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client-side functions associated with Training and Testing\n",
    "class Client(object):\n",
    "    def __init__(self, net_client_model, idx, lr, device, dataset_train = None, dataset_test = None, idxs = None, idxs_test = None):\n",
    "        self.idx = idx\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.local_ep = 1\n",
    "        #self.selected_clients = []\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size = 64, shuffle = True)\n",
    "        self.ldr_test = DataLoader(DatasetSplit(dataset_test, idxs_test), batch_size = 64, shuffle = True)\n",
    "\n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        optimizer_client = torch.optim.Adam(net.parameters(), lr = self.lr)\n",
    "\n",
    "        for iter in range(self.local_ep):\n",
    "            len_batch = len(self.ldr_train)\n",
    "            for batch_idx, (ids, mask, labels) in enumerate(self.ldr_train):\n",
    "                ids, mask, labels = ids.to(self.device), mask.to(self.device), labels.to(device)\n",
    "                optimizer_client.zero_grad()\n",
    "                #---------forward prop-------------\n",
    "                fx = net(ids)\n",
    "                client_fx = fx.clone().detach().requires_grad_(True)\n",
    "\n",
    "                # Sending activations to server and receiving gradients from server\n",
    "                dfx = train_server(client_fx, labels, iter, self.local_ep, self.idx, len_batch)\n",
    "\n",
    "                #--------backward prop -------------\n",
    "                fx.backward(dfx)\n",
    "                optimizer_client.step()\n",
    "\n",
    "\n",
    "            #prRed('Client{} Train => Epoch: {}'.format(self.idx, ell))\n",
    "\n",
    "        return net.state_dict()\n",
    "\n",
    "    def evaluate(self, net, ell):\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            len_batch = len(self.ldr_test)\n",
    "            for batch_idx, (ids, mask, labels) in enumerate(self.ldr_test):\n",
    "                ids, mask, labels = ids.to(self.device), mask.to(self.device), labels.to(device)\n",
    "                #---------forward prop-------------\n",
    "                fx = net(ids)\n",
    "\n",
    "                # Sending activations to server\n",
    "                evaluate_server(fx, labels, self.idx, len_batch, ell)\n",
    "\n",
    "            #prRed('Client{} Test => Epoch: {}'.format(self.idx, ell))\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93f4f403-76aa-40c9-86ba-19d28b0d69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================================================\n",
    "# dataset_iid() will create a dictionary to collect the indices of the data samples randomly for each client\n",
    "# IID HAM10000 datasets will be created based on this\n",
    "def dataset_iid(dataset, num_users):\n",
    "    \n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a89b353-53a1-4b08-a397-96603bbc4f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_595/2868560574.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv('./train.csv')\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfd82dd6-e04d-4395-afd4-6643f023da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_domain = df_train.domain.values\n",
    "test_data_domain = df_test.domain.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c40c1b53-b320-422b-a350-4a2aa0eeee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "labels_train = []\n",
    "for sent in train_data_domain:\n",
    "    encoded_dict = bert_tokenizer.encode_plus(\n",
    "        sent,                      # Sentence to encode.\n",
    "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "        max_length = 64,           # Pad & truncate all sentences.\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,   # Construct attn. masks.\n",
    "        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_train.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_train.append(encoded_dict['attention_mask'])\n",
    "    labels = encoded_dict['input_ids']\n",
    "    labels = torch.where(encoded_dict['input_ids']==bert_tokenizer.mask_token_id, labels, -100)\n",
    "    labels_train.append(labels)\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "labels_train = torch.cat(labels_train, dim=0)\n",
    "\n",
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "labels_test = []\n",
    "\n",
    "for sent in test_data_domain:\n",
    "    encoded_dict = bert_tokenizer.encode_plus(\n",
    "        sent,                      # Sentence to encode.\n",
    "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "        max_length = 64,           # Pad & truncate all sentences.\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,   # Construct attn. masks.\n",
    "        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_test.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_test.append(encoded_dict['attention_mask'])\n",
    "    labels = encoded_dict['input_ids']\n",
    "    labels = torch.where(encoded_dict['input_ids']==bert_tokenizer.mask_token_id, labels, -100)\n",
    "    labels_test.append(labels)\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "labels_test = torch.cat(labels_test, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20726654-8e0e-4753-946c-afa620e144c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, input_ids_train)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, input_ids_test)\n",
    "\n",
    "dict_users = dataset_iid(dataset_train, num_users)\n",
    "dict_users_test = dataset_iid(dataset_test, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6e51010-47bf-4779-984e-5a6fa40a8865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.18370094124576652472 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00141082876607970318 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.18283404332225372535 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00000776537967761115 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.18329204777504487511 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00000254157559148638 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.18325415548582982272 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00003106730097719890 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.18288099350534692267 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00001869482072130779 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   0 | Avg Loss 0.18319243626684839077\n",
      " Test: Round   0 | Avg Loss 0.00029417956860946153\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.00001067552025120477 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00000517938440704924 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.00000842549523781599 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00003673063811457933 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.00001417217558078576 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00000406421480679409 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.00001659177431961822 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00001018750780441380 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.00000891446365776005 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00000053657198536072 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   1 | Avg Loss 0.00001175588580943696\n",
      " Test: Round   1 | Avg Loss 0.00001133966342363943\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.00000809444026561401 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00007072092709131539 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.00000310282450582910 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00000003345067745414 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.00000227992120120923 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00000962750382634642 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.00001009302961662684 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00079693311115806864 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.00000350763450921754 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00000556241499257881 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   2 | Avg Loss 0.00000541557001969935\n",
      " Test: Round   2 | Avg Loss 0.00017657548154915268\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.00000585019425966315 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00000002906002463642 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.00001270758429392505 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00000028529721845326 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.00000205137141357170 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00000276777216964452 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.00000143666944743437 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00000000979067270654 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.00000104700609137343 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00000568746952472723 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   3 | Avg Loss 0.00000461856510119354\n",
      " Test: Round   3 | Avg Loss 0.00000175587792203359\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.00000888028282948294 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00000012980717361463 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.00000138291214685271 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00000071289962312646 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.00000032061637347303 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00000603863754385037 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.00000075016576880005 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00000000482959001179 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.00000503306455120534 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00000000698796251736 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   4 | Avg Loss 0.00000327340833396281\n",
      " Test: Round   4 | Avg Loss 0.00000137863237862412\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.00000108145491851230 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00000106708120730453 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.00000414800439931096 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00000000555886010726 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.00000549833850297343 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00000005920647260605 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.00000046028277464272 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00000000313976742507 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.00000022755866856583 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00000070667454777670 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   5 | Avg Loss 0.00000228312785280105\n",
      " Test: Round   5 | Avg Loss 0.00000036833217104392\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.00000018256318929399 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00000048148005024308 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.00000035790981048578 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00000000275245523117 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.00000100681390420299 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00000094211185075440 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.00000515443271311211 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00000004468829471047 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.00000439840548128290 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00000000537474553866 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   6 | Avg Loss 0.00000222002501967555\n",
      " Test: Round   6 | Avg Loss 0.00000029528147929556\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.00000097022407842395 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00000008431484264174 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.00000423486881302537 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00000000471399695503 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.00000042320317667865 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00000000300160970151 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.00000517386800615105 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00000005487460922949 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.00000016724842651206 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00000054660923608367 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   7 | Avg Loss 0.00000219388250015822\n",
      " Test: Round   7 | Avg Loss 0.00000013870285892229\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.00000681218701949149 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00000008907788583505 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.00000069416088827579 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00000000553901374292 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.00000558820441245178 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00000064664297911032 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.00000119308447075150 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00000012767649456438 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.00000026313766835928 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00000214143129867495 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   8 | Avg Loss 0.00000291015489186597\n",
      " Test: Round   8 | Avg Loss 0.00000060207353438552\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "\u001B[91m Client3 Train => Local Epoch: 0 \tLoss: 0.00000307081936705877 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client3 Test =>                 \tLoss: 0.00000043013819680378 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client2 Train => Local Epoch: 0 \tLoss: 0.00000101065498104269 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client2 Test =>                 \tLoss: 0.00000221362080595325 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client0 Train => Local Epoch: 0 \tLoss: 0.00000185316465522511 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client0 Test =>                 \tLoss: 0.00000002550427806268 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client1 Train => Local Epoch: 0 \tLoss: 0.00001141064190178565 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client1 Test =>                 \tLoss: 0.00000038206594599367 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[91m Client4 Train => Local Epoch: 0 \tLoss: 0.00000865451135123774 \tTrain cost: 19634 days, 5:14:26\u001B[00m\n",
      "\u001B[92m Client4 Test =>                 \tLoss: 0.00000251058588139585 \tTest cost: 19634 days, 5:14:26\u001B[00m\n",
      "------------------------------------------------\n",
      "------ Federation process at Server-Side ------- \n",
      "------------------------------------------------\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   9 | Avg Loss 0.00000519995845126999\n",
      " Test: Round   9 | Avg Loss 0.00000111238302164185\n",
      "==========================================================\n",
      "-----------------------------------------------------------\n",
      "------ FedServer: Federation process at Client-Side ------- \n",
      "-----------------------------------------------------------\n",
      "Training and Evaluation completed! total time cost: 6:22:08\n"
     ]
    }
   ],
   "source": [
    "#------------ Training And Testing  -----------------\n",
    "net_glob_client.train()\n",
    "#copy weights\n",
    "w_glob_client = net_glob_client.state_dict()\n",
    "# Federation takes place after certain local epochs in train() client-side\n",
    "# this epoch is global epoch, also known as rounds\n",
    "t0 = time.time()\n",
    "for iter in range(epochs):\n",
    "    m = max(int(frac * num_users), 1)\n",
    "    idxs_users = np.random.choice(range(num_users), m, replace = False)\n",
    "    w_locals_client = []\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local = Client(net_glob_client, idx, lr, device, dataset_train = dataset_train, dataset_test = dataset_test, idxs = dict_users[idx], idxs_test = dict_users_test[idx])\n",
    "        # Training ------------------\n",
    "        w_client = local.train(net = copy.deepcopy(net_glob_client).to(device))\n",
    "        w_locals_client.append(copy.deepcopy(w_client))\n",
    "\n",
    "        # Testing -------------------\n",
    "        local.evaluate(net = copy.deepcopy(net_glob_client).to(device), ell= iter)\n",
    "\n",
    "\n",
    "    # Ater serving all clients for its local epochs------------\n",
    "    # Fed  Server: Federation process at Client-Side-----------\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"------ FedServer: Federation process at Client-Side ------- \")\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    w_glob_client = FedAvg(w_locals_client)\n",
    "\n",
    "    # Update client-side global model\n",
    "    net_glob_client.load_state_dict(w_glob_client)\n",
    "elapsed = format_time(time.time() - t0)\n",
    "#===================================================================================\n",
    "\n",
    "print(\"Training and Evaluation completed! total time cost: {:}\".format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac5370dd-30f4-4cea-9c98-9d3a11b281ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(w_glob_server, \"./FedBert/FedEmbedding.pt\")\n",
    "torch.save(w_glob_client, \"./FedBert/FedTransformer.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
