{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "df_train = pd.read_csv(\"./fine_tuning.csv\")\n",
    "\n",
    "train_data_domain = df_train.domain.values\n",
    "train_data_label = df_train.label.values\n",
    "train_data_label = train_data_label.tolist()\n",
    "train_data_label = [0 if item == 2 else 1 for item in train_data_label]\n",
    "train_data_label = np.array(train_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=\"./bert_tokenizer/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "\n",
    "for sent in train_data_domain:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,                      # Sentence to encode.\n",
    "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "        max_length = 64,           # Pad & truncate all sentences.\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,   # Construct attn. masks.\n",
    "        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "    )\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_train.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_train.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "labels_train = torch.tensor(train_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111,998 training samples\n",
      "48,000 test samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.7 * len(dataset_train))\n",
    "test_size = len(dataset_train) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset_train, [train_size, test_size])\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} test samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "构造MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "EmbeddingPath = \"./FedBert/FedTransformer.pt\"\n",
    "TransformerPath = \"./FedBert/FedEmbedding.pt\"\n",
    "num_users = 10\n",
    "frac = 0.5\n",
    "local_epochs = 5\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"./bert-base-uncased-model/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1000\n",
      "}\n",
      "\n",
      "BertPooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,DataCollatorForLanguageModeling,HfArgumentParser,Trainer,TrainingArguments,set_seed,\n",
    ")\n",
    "# 自己修改部分配置参数\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": None,\n",
    "    \"revision\": 'main',\n",
    "    \"use_auth_token\": None,\n",
    "    #      \"hidden_size\": 512,\n",
    "    #     \"num_attention_heads\": 4,\n",
    "    \"hidden_dropout_prob\": 0.2,\n",
    "    \"vocab_size\": 1000 # 自己设置词汇大小\n",
    "}\n",
    "# 将模型的配置参数载入\n",
    "config = AutoConfig.from_pretrained('./bert-base-uncased-model/', **config_kwargs)\n",
    "print(config)\n",
    "# 载入预训练模型\n",
    "model = AutoModelForMaskedLM.from_config(\n",
    "    config=config,\n",
    ")\n",
    "model.resize_token_embeddings(config_kwargs[\"vocab_size\"])\n",
    "\n",
    "embedding = model.bert.embeddings\n",
    "\n",
    "class Bert_Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_Embedding, self).__init__()\n",
    "        self.embeddings = copy.deepcopy(embedding)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        embedding_output = self.embeddings(input_ids, attn_mask)\n",
    "        return embedding_output\n",
    "\n",
    "embedding_model = Bert_Embedding()\n",
    "embedding_model.load_state_dict(torch.load(EmbeddingPath))\n",
    "\n",
    "encoder = model.bert.encoder\n",
    "cls = model.cls\n",
    "\n",
    "class Bert_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_Encoder, self).__init__()\n",
    "        self.encoder = copy.deepcopy(encoder)\n",
    "        self.cls = copy.deepcopy(cls)\n",
    "\n",
    "    def forward(self, embedding_output):\n",
    "        output_encoder = self.encoder(embedding_output).last_hidden_state\n",
    "        return output_encoder\n",
    "encoder_model = Bert_Encoder()\n",
    "encoder_model.load_state_dict(torch.load(TransformerPath))\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "class Pooler_Config:\n",
    "    def __init__(self, entries: dict={}):\n",
    "        for k, v in entries.items():\n",
    "            if isinstance(v, dict):\n",
    "                self.__dict__[k] = Pooler_Config(v)\n",
    "            else:\n",
    "                self.__dict__[k] = v\n",
    "\n",
    "config_pooler = {\"hidden_size\": 768}\n",
    "config_pooler = Pooler_Config(config_pooler)\n",
    "pooler = BertPooler(config_pooler)\n",
    "print(pooler)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_classes=2, freeze_bert=False):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = Bert_Embedding()\n",
    "        self.encoder = Bert_Encoder()\n",
    "        self.pooler = copy.deepcopy(pooler)\n",
    "        if freeze_bert:\n",
    "            for p in self.embedding.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_size, num_classes, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        embedding_outputs = self.embedding(input_ids, attn_mask)\n",
    "        encoder_outputs = self.encoder(embedding_outputs)\n",
    "        pooler_outputs = self.pooler(encoder_outputs)\n",
    "        #它代表了一句话的embedding\n",
    "        logits = self.fc(pooler_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "iid数据分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def dataset_iid(dataset, num_users):\n",
    "\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "# train_dataset[:][:][1]\n",
    "def dirichlet_split_noniid(train_labels, num_users):\n",
    "\n",
    "    alpha = 0.7\n",
    "    n_classes = 2\n",
    "\n",
    "    label_distribution = np.random.dirichlet([alpha]*num_users, n_classes)\n",
    "\n",
    "    class_idcs = [np.argwhere(train_labels == y).flatten()\n",
    "                  for y in range(n_classes)]\n",
    "\n",
    "    # 记录N个client分别对应的样本索引集合\n",
    "    client_idcs = [[] for _ in range(num_users)]\n",
    "    for k_idcs, fracs in zip(class_idcs, label_distribution):\n",
    "        # np.split按照比例fracs将类别为k的样本索引k_idcs划分为了N个子集\n",
    "        # i表示第i个client，idcs表示其对应的样本索引集合idcs\n",
    "        for i, idcs in enumerate(np.split(k_idcs,\n",
    "                                          (np.cumsum(fracs)[:-1]*len(k_idcs)).\n",
    "                                                  astype(int))):\n",
    "            client_idcs[i] += [idcs]\n",
    "\n",
    "    dict_users = [np.concatenate(idcs) for idcs in client_idcs]\n",
    "\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "dict_user_train = dirichlet_split_noniid(train_dataset[:][:][:][2], num_users)\n",
    "dict_user_test = dataset_iid(test_dataset, num_users)\n",
    "\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (embedding): Bert_Embedding(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(1000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder): Bert_Encoder(\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): Sequential()\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=768, out_features=2, bias=False)\n",
      "    (2): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net_glob = MyModel()\n",
    "net_glob.encoder.cls = nn.Sequential()\n",
    "print(net_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 训练loss记录\n",
    "loss_train_collect = {}\n",
    "# 训练acc记录\n",
    "acc_train_collect = {}\n",
    "loss_test_collect = {}\n",
    "# 测试acc记录\n",
    "acc_test_collect = {}\n",
    "# 训练TPR记录\n",
    "TPR_train_collect = {}\n",
    "# 测试TPR记录\n",
    "TPR_test_collect = {}\n",
    "# 训练FPR记录\n",
    "FPR_train_collect = {}\n",
    "# 测试FPR记录\n",
    "FPR_test_collect = {}\n",
    "# 训练测试F1-score记录\n",
    "f1_train_collect = {}\n",
    "f1_test_collect = {}\n",
    "# 训练测试AUC记录\n",
    "AUC_train_collect = {}\n",
    "AUC_test_collect = {}\n",
    "# 训练测试ROC曲线记录\n",
    "ROC_train_collect = {}\n",
    "ROC_test_collect = {}\n",
    "# 本地测试记录\n",
    "local_test = {}\n",
    "local_testing = {}\n",
    "\n",
    "loss_collect = []\n",
    "acc_collect = []\n",
    "TPR_collect = []\n",
    "FPR_collect = []\n",
    "F1_collect = []\n",
    "AUC_collect = []\n",
    "\n",
    "count1 = 0\n",
    "count2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def FedAvg(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[k] += w[i][k]\n",
    "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "idx_collect = []\n",
    "l_epoch_check = False\n",
    "fed_check = False\n",
    "# Initialization of net_model_server and net_server (server-side model)\n",
    "net_model = [net_glob for i in range(num_users)]\n",
    "net_server = copy.deepcopy(net_model[0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        domain, mask, label = self.dataset[self.idxs[item]]\n",
    "        return domain, mask, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    return np.sum(preds == labels) / len(labels)\n",
    "\n",
    "def tpr_calculate(preds, labels):\n",
    "    return recall_score(labels, preds,zero_division=1)\n",
    "\n",
    "def fpr_calculate(preds, labels):\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    #print(conf_matrix)\n",
    "    fp = conf_matrix[0, 1]  # 0 表示负类别，1 表示正类别\n",
    "    tn = conf_matrix[0, 0]\n",
    "    fpr = fp / (fp + tn)\n",
    "    return fpr\n",
    "\n",
    "def f1_score_calculate(preds, labels):\n",
    "    return f1_score(labels, preds, zero_division=1)\n",
    "\n",
    "def AUC_calculate(preds, labels):\n",
    "    return roc_auc_score(labels, preds)\n",
    "\n",
    "def roc_curve_calculate(preds, labels):\n",
    "    return roc_curve(labels, preds)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Client(object):\n",
    "    def __init__(self, device, idx, lr, local_epochs, batch_size, dataset_train = None, dataset_test = None, idxs = None, idxs_test = None):\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.idx = idx\n",
    "        self.local_ep = local_epochs\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size = batch_size, shuffle = True)\n",
    "        self.ldr_test = DataLoader(DatasetSplit(dataset_test, idxs_test), batch_size = batch_size, shuffle= True)\n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        optimizer_client = torch.optim.Adam(net.parameters(), lr = self.lr)\n",
    "\n",
    "        TPR_train_collect[self.idx] = []\n",
    "        FPR_train_collect[self.idx] = []\n",
    "        f1_train_collect[self.idx] = []\n",
    "        AUC_train_collect[self.idx] = []\n",
    "        loss_train_collect[self.idx] = []\n",
    "        acc_train_collect[self.idx] = []\n",
    "\n",
    "        epoch_loss = []\n",
    "        epoch_accuracy = []\n",
    "        for iter in range(self.local_ep):\n",
    "            tmp_t0 = time.time()\n",
    "            batch_loss_train = []\n",
    "            batch_acc_train = []\n",
    "            batch_tpr_train = []\n",
    "            batch_fpr_train = []\n",
    "            batch_f1_train = []\n",
    "            batch_auc_train = []\n",
    "            for batch_idx, (ids, attn_mask, b_labels) in enumerate(self.ldr_train):\n",
    "                ids, attn_mask, b_labels = ids.to(self.device), attn_mask.to(self.device) , b_labels.to(self.device)\n",
    "                optimizer_client.zero_grad()\n",
    "                b_labels = b_labels.unsqueeze(1)\n",
    "                b_labels = b_labels.repeat(1,2)\n",
    "                for i in range(len(b_labels)):\n",
    "                    b_labels[i][1] = 1-b_labels[i][0]\n",
    "                logits = net(ids, attn_mask)\n",
    "                BCEloss = criterion(logits, b_labels.float())\n",
    "                BCEloss.backward()\n",
    "                optimizer_client.step()\n",
    "                batch_loss_train.append(BCEloss.item())\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                logits = np.argmax(logits, axis=1).flatten()\n",
    "                label_ids = np.argmax(label_ids,axis=1).flatten()\n",
    "                accuracy = flat_accuracy(logits, label_ids)\n",
    "                tpr = tpr_calculate(logits, label_ids)\n",
    "                if len(set(label_ids)) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    fpr = fpr_calculate(logits, label_ids)\n",
    "                    batch_fpr_train.append(fpr)\n",
    "                f1 = f1_score_calculate(logits, label_ids)\n",
    "                if len(set(label_ids)) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    auc = AUC_calculate(logits, label_ids)\n",
    "                    batch_auc_train.append(auc)\n",
    "                batch_acc_train.append(accuracy)\n",
    "                batch_tpr_train.append(tpr)\n",
    "                batch_f1_train.append(f1)\n",
    "            elapsed = format_time(time.time()-tmp_t0)\n",
    "            epoch_avg_loss = sum(batch_loss_train)/len(batch_loss_train)\n",
    "            epoch_avg_acc = sum(batch_acc_train)/len(batch_acc_train)\n",
    "            epoch_avg_tpr = sum(batch_tpr_train)/len(batch_tpr_train)\n",
    "            epoch_avg_fpr = sum(batch_fpr_train)/len(batch_fpr_train)\n",
    "            epoch_avg_f1 = sum(batch_f1_train)/len(batch_f1_train)\n",
    "            epoch_avg_auc = sum(batch_auc_train)/len(batch_auc_train)\n",
    "            epoch_loss.append(sum(batch_loss_train)/len(batch_loss_train))\n",
    "            epoch_accuracy.append(sum(batch_acc_train)/len(batch_acc_train))\n",
    "            loss_train_collect[self.idx].append(epoch_avg_loss)\n",
    "            acc_train_collect[self.idx].append(epoch_avg_acc)\n",
    "            TPR_train_collect[self.idx].append(epoch_avg_tpr)\n",
    "            FPR_train_collect[self.idx].append(epoch_avg_fpr)\n",
    "            f1_train_collect[self.idx].append(epoch_avg_f1)\n",
    "            AUC_train_collect[self.idx].append(epoch_avg_auc)\n",
    "            loss_collect.append(epoch_avg_loss)\n",
    "            acc_collect.append(epoch_avg_acc)\n",
    "            TPR_collect.append(epoch_avg_tpr)\n",
    "            FPR_collect.append(epoch_avg_fpr)\n",
    "            F1_collect.append(epoch_avg_f1)\n",
    "            AUC_collect.append(epoch_avg_auc)\n",
    "\n",
    "            print('Client{} Local Train => Local Epoch: {} \\tLoss: {:.10f} \\tAcc: {:.10f} \\tTPR:{:.10f} \\tFPR:{:.10f} \\tF1:{:.10f} \\t AUC:{:.10f}\\tTrain cost: {:}'.format(self.idx, iter, epoch_avg_loss, \\\n",
    "                                                                                                                                                                           epoch_avg_acc, epoch_avg_tpr, epoch_avg_fpr, epoch_avg_f1, epoch_avg_auc, elapsed))\n",
    "        net_glob.load_state_dict(net.state_dict())\n",
    "        return net.state_dict(), sum(epoch_loss) / len(epoch_loss), sum(epoch_accuracy) / len(epoch_accuracy)\n",
    "\n",
    "    def evaluate(self, net, ell):\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_t0 = time.time()\n",
    "            len_batch = len(self.ldr_test)\n",
    "\n",
    "            batch_acc_test = []\n",
    "            batch_loss_test = []\n",
    "            batch_tpr_test = []\n",
    "            batch_fpr_test = []\n",
    "            batch_f1_test = []\n",
    "            batch_auc_test = []\n",
    "            for batch_idx, (ids, attn_mask, b_labels) in enumerate(self.ldr_test):\n",
    "                ids, attn_mask, b_labels = ids.to(self.device), attn_mask.to(self.device) , b_labels.to(self.device)\n",
    "                b_labels = b_labels.unsqueeze(1)\n",
    "                b_labels = b_labels.repeat(1,2)\n",
    "                for i in range(len(b_labels)):\n",
    "                    b_labels[i][1] = 1-b_labels[i][0]\n",
    "                logits = net(ids, attn_mask)\n",
    "                BCEloss = criterion(logits, b_labels.float())\n",
    "                batch_loss_test.append(BCEloss.item())\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                logits = np.argmax(logits, axis=1).flatten()\n",
    "                label_ids = np.argmax(label_ids,axis=1).flatten()\n",
    "                accuracy = flat_accuracy(logits, label_ids)\n",
    "                tpr = tpr_calculate(logits, label_ids)\n",
    "                if len(set(label_ids)) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    fpr = fpr_calculate(logits, label_ids)\n",
    "                    batch_fpr_test.append(fpr)\n",
    "                f1 = f1_score_calculate(logits, label_ids)\n",
    "                if len(set(label_ids)) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    auc = AUC_calculate(logits, label_ids)\n",
    "                    batch_auc_test.append(auc)\n",
    "                batch_acc_test.append(accuracy)\n",
    "                batch_tpr_test.append(tpr)\n",
    "                batch_f1_test.append(f1)\n",
    "            elapsed = format_time(time.time()-tmp_t0)\n",
    "            test_avg_loss = sum(batch_loss_test) / len(batch_loss_test)\n",
    "            test_avg_acc = sum(batch_acc_test) / len(batch_acc_test)\n",
    "            test_avg_tpr = sum(batch_tpr_test)/len(batch_tpr_test)\n",
    "            test_avg_fpr = sum(batch_fpr_test) / len(batch_fpr_test)\n",
    "            test_avg_f1 = sum(batch_f1_test)/len(batch_f1_test)\n",
    "            test_avg_auc = sum(batch_auc_test)/len(batch_auc_test)\n",
    "            local_test[\"loss\"].append(test_avg_loss)\n",
    "            local_test[\"acc\"].append(test_avg_acc)\n",
    "            local_test[\"tpr\"].append(test_avg_tpr)\n",
    "            local_test[\"fpr\"].append(test_avg_fpr)\n",
    "            local_test[\"f1\"].append(test_avg_f1)\n",
    "            local_test[\"auc\"].append(test_avg_auc)\n",
    "            print('Client{} Test =>                 \\tLoss: {:.10f} \\tAcc: {:.10f} \\tTPR:{:.10f} \\tFPR:{:.10f} \\tF1:{:.10f} \\tAUC:{:.10f} \\ttest cost: {:}'.format(self.idx, test_avg_loss, test_avg_acc, test_avg_tpr, test_avg_fpr, test_avg_f1, test_avg_auc, elapsed))\n",
    "\n",
    "        return test_avg_loss, test_avg_acc, test_avg_tpr, test_avg_fpr, test_avg_f1, test_avg_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test Begin!\n",
      "============== Round 0:  =============\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.4946515006 \tAcc: 0.7845784024 \tTPR:0.9554121697 \tFPR:0.7764274251 \tF1:0.8689496695 \t AUC:0.5893596704\tTrain cost: 0:00:33\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.4707483554 \tAcc: 0.8028846154 \tTPR:0.9532991431 \tFPR:0.7089138277 \tF1:0.8795974830 \t AUC:0.6221233686\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.4134785727 \tAcc: 0.8381102071 \tTPR:0.9658915116 \tFPR:0.5922620782 \tF1:0.9000396764 \t AUC:0.6867597063\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.3652202301 \tAcc: 0.8578032544 \tTPR:0.9597872379 \tFPR:0.4843348959 \tF1:0.9104992223 \t AUC:0.7377261710\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.3129755075 \tAcc: 0.8798076923 \tTPR:0.9617996245 \tFPR:0.3984608616 \tF1:0.9233734644 \t AUC:0.7816127044\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.3871208485 \tAcc: 0.8304166667 \tTPR:0.8959629931 \tFPR:0.2333625660 \tF1:0.8328517191 \tAUC:0.8313002136 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.3913482722 \tAcc: 0.8164323963 \tTPR:0.7986354059 \tFPR:0.1688838719 \tF1:0.7880314019 \t AUC:0.8148757670\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.3827778329 \tAcc: 0.8286479250 \tTPR:0.8225230906 \tFPR:0.1715604533 \tF1:0.7984977831 \t AUC:0.8254813187\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.3649857182 \tAcc: 0.8341281794 \tTPR:0.8359145471 \tFPR:0.1722239449 \tF1:0.8159501517 \t AUC:0.8318453011\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.3567200497 \tAcc: 0.8393992637 \tTPR:0.8333530449 \tFPR:0.1611487553 \tF1:0.8168435882 \t AUC:0.8361021448\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.3495902012 \tAcc: 0.8381024096 \tTPR:0.8310862645 \tFPR:0.1546326814 \tF1:0.8129945044 \t AUC:0.8382267915\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.3303258603 \tAcc: 0.8552083333 \tTPR:0.8629632123 \tFPR:0.1521854514 \tF1:0.8512694224 \tAUC:0.8553888805 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.2082446209 \tAcc: 0.9164747807 \tTPR:0.4425200733 \tFPR:0.0251009098 \tF1:0.4697021861 \t AUC:0.7033092546\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.1988453771 \tAcc: 0.9201206140 \tTPR:0.4894174093 \tFPR:0.0249894732 \tF1:0.5093285874 \t AUC:0.7249199310\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.1876428143 \tAcc: 0.9259228801 \tTPR:0.5270997359 \tFPR:0.0241673116 \tF1:0.5425031752 \t AUC:0.7479580500\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.1752329752 \tAcc: 0.9286732456 \tTPR:0.5546458739 \tFPR:0.0247861588 \tF1:0.5661431438 \t AUC:0.7589115586\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.1653395779 \tAcc: 0.9341191520 \tTPR:0.5886028729 \tFPR:0.0234091863 \tF1:0.6068661566 \t AUC:0.7779847679\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.6511282745 \tAcc: 0.7572916667 \tTPR:0.5194258514 \tFPR:0.0127313028 \tF1:0.6682212816 \tAUC:0.7533472743 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.3326421393 \tAcc: 0.8750000000 \tTPR:0.9414720143 \tFPR:0.3842828798 \tF1:0.9122990904 \t AUC:0.7785945672\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.2754726849 \tAcc: 0.8998161765 \tTPR:0.9685388771 \tFPR:0.3440605030 \tF1:0.9364264093 \t AUC:0.8122391871\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.2638987090 \tAcc: 0.9042804622 \tTPR:0.9718440652 \tFPR:0.3514906720 \tF1:0.9396760426 \t AUC:0.8101766966\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.2580057220 \tAcc: 0.9030330882 \tTPR:0.9744891792 \tFPR:0.3374098124 \tF1:0.9380123754 \t AUC:0.8185396834\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.2725537606 \tAcc: 0.9021796218 \tTPR:0.9681682861 \tFPR:0.3444135230 \tF1:0.9386625503 \t AUC:0.8118773815\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.3161906469 \tAcc: 0.8675000000 \tTPR:0.9700656393 \tFPR:0.2351753716 \tF1:0.8782085771 \tAUC:0.8674451339 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.2728091806 \tAcc: 0.8906250000 \tTPR:0.9475222219 \tFPR:0.2366436195 \tF1:0.9198844983 \t AUC:0.8551231700\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.2807021617 \tAcc: 0.8849206349 \tTPR:0.9503077929 \tFPR:0.2466812463 \tF1:0.9163717211 \t AUC:0.8518132733\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.2658225780 \tAcc: 0.8938492063 \tTPR:0.9508026711 \tFPR:0.2280259423 \tF1:0.9225146310 \t AUC:0.8613883644\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.2598305906 \tAcc: 0.8968253968 \tTPR:0.9537790302 \tFPR:0.2202294234 \tF1:0.9243512620 \t AUC:0.8667748034\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.2526668211 \tAcc: 0.8984375000 \tTPR:0.9492464290 \tFPR:0.2085180692 \tF1:0.9245158829 \t AUC:0.8703641799\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2624271608 \tAcc: 0.8933333333 \tTPR:0.9184390684 \tFPR:0.1300284772 \tF1:0.8921123089 \tAUC:0.8942052956 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.3894385582 \tAcc: 0.8407500000 \tTPR:0.8333713529 \tFPR:0.1526966338 \tF1:0.8245326618 \tAUC:0.8403373596\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 1:  =============\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.3062729590 \tAcc: 0.8653781794 \tTPR:0.8668909252 \tFPR:0.1373380000 \tF1:0.8468962139 \t AUC:0.8647764626\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.2931993969 \tAcc: 0.8808149264 \tTPR:0.8789839445 \tFPR:0.1204050216 \tF1:0.8660772519 \t AUC:0.8792894615\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.2938529051 \tAcc: 0.8701054217 \tTPR:0.8721524420 \tFPR:0.1298922695 \tF1:0.8527075899 \t AUC:0.8711300863\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.2927649515 \tAcc: 0.8776355422 \tTPR:0.8960011508 \tFPR:0.1309672677 \tF1:0.8636708745 \t AUC:0.8825169415\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.2879939108 \tAcc: 0.8744143240 \tTPR:0.8778452573 \tFPR:0.1235603368 \tF1:0.8546502406 \t AUC:0.8771424602\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2845562604 \tAcc: 0.8843750000 \tTPR:0.8482902441 \tFPR:0.0830270678 \tF1:0.8753388318 \tAUC:0.8826315882 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.2182509731 \tAcc: 0.9053848870 \tTPR:0.5929704633 \tFPR:0.0401923648 \tF1:0.6124406193 \t AUC:0.7763890493\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.2097437670 \tAcc: 0.9086158192 \tTPR:0.6368611458 \tFPR:0.0410019109 \tF1:0.6435389686 \t AUC:0.7974152565\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.2012354170 \tAcc: 0.9118467514 \tTPR:0.6617499654 \tFPR:0.0430239361 \tF1:0.6583593836 \t AUC:0.8074301573\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.1974681239 \tAcc: 0.9165607345 \tTPR:0.6712343056 \tFPR:0.0383365511 \tF1:0.6732198009 \t AUC:0.8159832034\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.1899288597 \tAcc: 0.9196857345 \tTPR:0.6837881437 \tFPR:0.0366960805 \tF1:0.6836858548 \t AUC:0.8221946989\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.3192903910 \tAcc: 0.8687500000 \tTPR:0.7769157954 \tFPR:0.0411590072 \tF1:0.8484263090 \tAUC:0.8678783941 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.1545858647 \tAcc: 0.9371253655 \tTPR:0.6335677156 \tFPR:0.0248885926 \tF1:0.6322501124 \t AUC:0.7979644159\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.1475866361 \tAcc: 0.9400493421 \tTPR:0.6529203375 \tFPR:0.0241261966 \tF1:0.6513382940 \t AUC:0.8120832060\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.1392033111 \tAcc: 0.9444718567 \tTPR:0.6638268124 \tFPR:0.0225548747 \tF1:0.6691720603 \t AUC:0.8153119591\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.1328073570 \tAcc: 0.9474141082 \tTPR:0.6937401374 \tFPR:0.0201128017 \tF1:0.6910430984 \t AUC:0.8314853949\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.1334788362 \tAcc: 0.9464912281 \tTPR:0.6945912234 \tFPR:0.0214585163 \tF1:0.6956251759 \t AUC:0.8326743377\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.5384496892 \tAcc: 0.8100000000 \tTPR:0.6277469402 \tFPR:0.0118707209 \tF1:0.7564687999 \tAUC:0.8079381096 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.2825615970 \tAcc: 0.8952205882 \tTPR:0.9547615619 \tFPR:0.3137626263 \tF1:0.9322574669 \t AUC:0.8204994678\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.2226307906 \tAcc: 0.9175420168 \tTPR:0.9680517823 \tFPR:0.2688633787 \tF1:0.9482371979 \t AUC:0.8495942018\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.2189973537 \tAcc: 0.9141938025 \tTPR:0.9665539371 \tFPR:0.2808531746 \tF1:0.9462042516 \t AUC:0.8428503812\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.2185497236 \tAcc: 0.9208902311 \tTPR:0.9743812867 \tFPR:0.3012510307 \tF1:0.9509756709 \t AUC:0.8365651280\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.2019995345 \tAcc: 0.9143251050 \tTPR:0.9694778039 \tFPR:0.2805761699 \tF1:0.9455099772 \t AUC:0.8444508170\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2592707734 \tAcc: 0.8945833333 \tTPR:0.9655276328 \tFPR:0.1769516729 \tF1:0.9009425497 \tAUC:0.8942879800 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.2327703900 \tAcc: 0.9083581349 \tTPR:0.9528884899 \tFPR:0.1890101466 \tF1:0.9322436347 \t AUC:0.8819391716\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.2169417219 \tAcc: 0.9145585317 \tTPR:0.9583475117 \tFPR:0.1745454097 \tF1:0.9371190267 \t AUC:0.8919010510\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.2077153265 \tAcc: 0.9162946429 \tTPR:0.9581710389 \tFPR:0.1731398231 \tF1:0.9382150665 \t AUC:0.8925156079\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.2097714224 \tAcc: 0.9211309524 \tTPR:0.9590271907 \tFPR:0.1524026979 \tF1:0.9413196050 \t AUC:0.9033122464\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.2044245912 \tAcc: 0.9227430556 \tTPR:0.9581746544 \tFPR:0.1488219453 \tF1:0.9435080186 \t AUC:0.9046763546\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2272373193 \tAcc: 0.9095833333 \tTPR:0.9027098393 \tFPR:0.0830237146 \tF1:0.9062034300 \tAUC:0.9098430624 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.3257608866 \tAcc: 0.8734583333 \tTPR:0.8242380904 \tFPR:0.0792064367 \tF1:0.8574759841 \tAUC:0.8725158269\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 2:  =============\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.2240566227 \tAcc: 0.9039491299 \tTPR:0.9072387689 \tFPR:0.0981717932 \tF1:0.8886249418 \t AUC:0.9045334879\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.2169016699 \tAcc: 0.9126506024 \tTPR:0.9113925578 \tFPR:0.0853607570 \tF1:0.8999325471 \t AUC:0.9130159004\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.2149605512 \tAcc: 0.9147004685 \tTPR:0.9162396565 \tFPR:0.0906891081 \tF1:0.9027681439 \t AUC:0.9127752742\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.2079524256 \tAcc: 0.9154534806 \tTPR:0.9081051043 \tFPR:0.0786063850 \tF1:0.8998173350 \t AUC:0.9147493596\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.2071470926 \tAcc: 0.9179216867 \tTPR:0.9203740224 \tFPR:0.0866217477 \tF1:0.9046020716 \t AUC:0.9168761374\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2438191565 \tAcc: 0.9035416667 \tTPR:0.8671329602 \tFPR:0.0605627954 \tF1:0.8966733840 \tAUC:0.9032850824 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.2284178012 \tAcc: 0.9102473837 \tTPR:0.9488664997 \tFPR:0.1600594804 \tF1:0.9298547126 \t AUC:0.8944035096\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.2120272317 \tAcc: 0.9178422044 \tTPR:0.9527052623 \tFPR:0.1459924308 \tF1:0.9356783636 \t AUC:0.9033564158\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.2032190975 \tAcc: 0.9211584289 \tTPR:0.9537707364 \tFPR:0.1389907960 \tF1:0.9382417101 \t AUC:0.9073899702\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1950285688 \tAcc: 0.9238788995 \tTPR:0.9563543329 \tFPR:0.1350057381 \tF1:0.9405051828 \t AUC:0.9106742974\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1878851080 \tAcc: 0.9269930676 \tTPR:0.9587643380 \tFPR:0.1301245136 \tF1:0.9429687078 \t AUC:0.9143199122\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.2104619277 \tAcc: 0.9204166667 \tTPR:0.9293911033 \tFPR:0.0901683805 \tF1:0.9173259465 \tAUC:0.9196113614 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1653680866 \tAcc: 0.9396264793 \tTPR:0.9751734506 \tFPR:0.1796155872 \tF1:0.9606360347 \t AUC:0.8977420971\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1583878929 \tAcc: 0.9405510355 \tTPR:0.9744274327 \tFPR:0.1681067037 \tF1:0.9612552592 \t AUC:0.9031687880\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1569915434 \tAcc: 0.9423076923 \tTPR:0.9738658467 \tFPR:0.1695546400 \tF1:0.9623625201 \t AUC:0.9021556034\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1541037559 \tAcc: 0.9440643491 \tTPR:0.9761155903 \tFPR:0.1646800356 \tF1:0.9635884199 \t AUC:0.9057177774\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1492154147 \tAcc: 0.9467455621 \tTPR:0.9771246816 \tFPR:0.1582801554 \tF1:0.9652728432 \t AUC:0.9093883235\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.2302880623 \tAcc: 0.9168750000 \tTPR:0.9584386188 \tFPR:0.1238118993 \tF1:0.9165610393 \tAUC:0.9173133598 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1804318573 \tAcc: 0.9353918651 \tTPR:0.9640936534 \tFPR:0.1250310371 \tF1:0.9513164832 \t AUC:0.9195313082\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1816781169 \tAcc: 0.9289434524 \tTPR:0.9659169783 \tFPR:0.1519508464 \tF1:0.9473887236 \t AUC:0.9069830660\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1717168033 \tAcc: 0.9346478175 \tTPR:0.9662375836 \tFPR:0.1317574669 \tF1:0.9505916194 \t AUC:0.9172400583\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1612616271 \tAcc: 0.9413442460 \tTPR:0.9747364235 \tFPR:0.1297244189 \tF1:0.9564381098 \t AUC:0.9225060023\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1632630055 \tAcc: 0.9382440476 \tTPR:0.9685447156 \tFPR:0.1278008863 \tF1:0.9538736152 \t AUC:0.9203719146\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2122816669 \tAcc: 0.9172916667 \tTPR:0.9140785942 \tFPR:0.0770575779 \tF1:0.9149484590 \tAUC:0.9185105081 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.2138515686 \tAcc: 0.9113157661 \tTPR:0.9229920001 \tFPR:0.0972932052 \tF1:0.9051924997 \t AUC:0.9128493975\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.2075768353 \tAcc: 0.9144485425 \tTPR:0.9252928884 \tFPR:0.0955721924 \tF1:0.9086939832 \t AUC:0.9148603480\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.2009160930 \tAcc: 0.9171786127 \tTPR:0.9267180880 \tFPR:0.0909804589 \tF1:0.9107047244 \t AUC:0.9178688146\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1913285265 \tAcc: 0.9221471738 \tTPR:0.9302790555 \tFPR:0.0859363855 \tF1:0.9162655523 \t AUC:0.9221713350\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1887124468 \tAcc: 0.9221082919 \tTPR:0.9321395832 \tFPR:0.0866210770 \tF1:0.9166052391 \t AUC:0.9227592531\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2054419197 \tAcc: 0.9218750000 \tTPR:0.8854716443 \tFPR:0.0438082533 \tF1:0.9148871840 \tAUC:0.9208316955 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2204585466 \tAcc: 0.9160000000 \tTPR:0.9109025842 \tFPR:0.0790817813 \tF1:0.9120792026 \tAUC:0.9159104014\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 3:  =============\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0970540284 \tAcc: 0.9663690476 \tTPR:0.9879974471 \tFPR:0.3293939394 \tF1:0.9817650906 \t AUC:0.8290402742\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0855100966 \tAcc: 0.9692708333 \tTPR:0.9898132926 \tFPR:0.3266975309 \tF1:0.9832998673 \t AUC:0.8312813045\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0791594860 \tAcc: 0.9712053571 \tTPR:0.9913231747 \tFPR:0.2944968553 \tF1:0.9843572219 \t AUC:0.8481349731\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0771757304 \tAcc: 0.9729166667 \tTPR:0.9916766584 \tFPR:0.2487878788 \tF1:0.9853139886 \t AUC:0.8716342379\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0793210616 \tAcc: 0.9703125000 \tTPR:0.9911176439 \tFPR:0.3015731293 \tF1:0.9839193442 \t AUC:0.8444550303\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.2077442453 \tAcc: 0.9227083333 \tTPR:0.9597280742 \tFPR:0.1197035427 \tF1:0.9256988303 \tAUC:0.9200122658 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1417437857 \tAcc: 0.9476656627 \tTPR:0.9785537197 \tFPR:0.1785809105 \tF1:0.9673050160 \t AUC:0.9000012138\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.1346452065 \tAcc: 0.9517131024 \tTPR:0.9796522160 \tFPR:0.1681976214 \tF1:0.9698257262 \t AUC:0.9056965604\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.1331040456 \tAcc: 0.9498305723 \tTPR:0.9778072507 \tFPR:0.1602436803 \tF1:0.9684460366 \t AUC:0.9087817852\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.1294674770 \tAcc: 0.9540662651 \tTPR:0.9808435762 \tFPR:0.1572057684 \tF1:0.9712177086 \t AUC:0.9117899667\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.1237581063 \tAcc: 0.9567959337 \tTPR:0.9828944488 \tFPR:0.1545344799 \tF1:0.9731124164 \t AUC:0.9141799845\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1852151460 \tAcc: 0.9297916667 \tTPR:0.9566222985 \tFPR:0.0966197813 \tF1:0.9306663035 \tAUC:0.9300012586 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1550574805 \tAcc: 0.9436945266 \tTPR:0.9761865790 \tFPR:0.1691803758 \tF1:0.9634920868 \t AUC:0.9035031016\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1480328875 \tAcc: 0.9450813609 \tTPR:0.9763799107 \tFPR:0.1556611209 \tF1:0.9640671953 \t AUC:0.9103243502\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1450175389 \tAcc: 0.9477625740 \tTPR:0.9784617923 \tFPR:0.1559917580 \tF1:0.9661105786 \t AUC:0.9112030614\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1390034334 \tAcc: 0.9493343195 \tTPR:0.9796938345 \tFPR:0.1572849093 \tF1:0.9669446994 \t AUC:0.9111743347\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1457984648 \tAcc: 0.9479474852 \tTPR:0.9776315586 \tFPR:0.1516384621 \tF1:0.9656978619 \t AUC:0.9129633607\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.2114230640 \tAcc: 0.9262500000 \tTPR:0.9580362143 \tFPR:0.1018774514 \tF1:0.9242999753 \tAUC:0.9280793815 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.2095217597 \tAcc: 0.9172674858 \tTPR:0.9280790741 \tFPR:0.0913029210 \tF1:0.9112090818 \t AUC:0.9183880766\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1971081920 \tAcc: 0.9200058878 \tTPR:0.9318593928 \tFPR:0.0905367780 \tF1:0.9140471831 \t AUC:0.9206613074\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1966277217 \tAcc: 0.9173396952 \tTPR:0.9296932134 \tFPR:0.0926965792 \tF1:0.9118464010 \t AUC:0.9184983171\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1886495844 \tAcc: 0.9221971649 \tTPR:0.9322123155 \tFPR:0.0884022481 \tF1:0.9170949770 \t AUC:0.9219050337\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1860810151 \tAcc: 0.9249272352 \tTPR:0.9349388632 \tFPR:0.0848626182 \tF1:0.9195088001 \t AUC:0.9250381225\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2034271441 \tAcc: 0.9212500000 \tTPR:0.8827284174 \tFPR:0.0405198138 \tF1:0.9158933320 \tAUC:0.9211043018 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1618065830 \tAcc: 0.9387400794 \tTPR:0.9664990807 \tFPR:0.1196819481 \tF1:0.9543607188 \t AUC:0.9234085663\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1623772569 \tAcc: 0.9397321429 \tTPR:0.9727026644 \tFPR:0.1308648362 \tF1:0.9556855720 \t AUC:0.9209189141\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1644606598 \tAcc: 0.9412202381 \tTPR:0.9710190564 \tFPR:0.1235344682 \tF1:0.9558475993 \t AUC:0.9237422941\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1587611214 \tAcc: 0.9434523810 \tTPR:0.9728497489 \tFPR:0.1127823118 \tF1:0.9578351731 \t AUC:0.9300337186\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1471404954 \tAcc: 0.9394841270 \tTPR:0.9698546675 \tFPR:0.1257011867 \tF1:0.9550607171 \t AUC:0.9220767404\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2132727468 \tAcc: 0.9170833333 \tTPR:0.9106399068 \tFPR:0.0751421187 \tF1:0.9148364501 \tAUC:0.9177488941 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2042164692 \tAcc: 0.9234166667 \tTPR:0.9335509822 \tFPR:0.0867725416 \tF1:0.9222789782 \tAUC:0.9233892203\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 4:  =============\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.1193506806 \tAcc: 0.9496527778 \tTPR:0.7161293385 \tFPR:0.0212968169 \tF1:0.7121106666 \t AUC:0.8440166122\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.1127803281 \tAcc: 0.9554550439 \tTPR:0.7571074677 \tFPR:0.0192865212 \tF1:0.7491386785 \t AUC:0.8658151412\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.1102732713 \tAcc: 0.9560855263 \tTPR:0.7543615984 \tFPR:0.0192275053 \tF1:0.7486853119 \t AUC:0.8655596005\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.1073827816 \tAcc: 0.9558022661 \tTPR:0.7569508261 \tFPR:0.0189349849 \tF1:0.7551182255 \t AUC:0.8670216347\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.1067583749 \tAcc: 0.9554459064 \tTPR:0.7441402327 \tFPR:0.0176176754 \tF1:0.7392981115 \t AUC:0.8598037142\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.5994051131 \tAcc: 0.8160416667 \tTPR:0.6342666693 \tFPR:0.0085202198 \tF1:0.7657020493 \tAUC:0.8128732247 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1820850757 \tAcc: 0.9235274431 \tTPR:0.9260667931 \tFPR:0.0774522160 \tF1:0.9125449417 \t AUC:0.9243072886\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1633824543 \tAcc: 0.9354082999 \tTPR:0.9347591673 \tFPR:0.0636943839 \tF1:0.9238174112 \t AUC:0.9355323917\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1634675348 \tAcc: 0.9365378179 \tTPR:0.9411536093 \tFPR:0.0681106771 \tF1:0.9274370540 \t AUC:0.9365214661\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1648117539 \tAcc: 0.9337349398 \tTPR:0.9329513096 \tFPR:0.0692536512 \tF1:0.9230205900 \t AUC:0.9318488292\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.1524626700 \tAcc: 0.9405120482 \tTPR:0.9428786811 \tFPR:0.0599802398 \tF1:0.9304015325 \t AUC:0.9414492207\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2010852971 \tAcc: 0.9245833333 \tTPR:0.8953592206 \tFPR:0.0448153112 \tF1:0.9190996639 \tAUC:0.9252719547 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1953453187 \tAcc: 0.9206585496 \tTPR:0.9310955555 \tFPR:0.0888987496 \tF1:0.9152331435 \t AUC:0.9210984030\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1877454738 \tAcc: 0.9236385754 \tTPR:0.9351128593 \tFPR:0.0864417857 \tF1:0.9186090691 \t AUC:0.9243355368\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1804844935 \tAcc: 0.9250799858 \tTPR:0.9313172848 \tFPR:0.0806842227 \tF1:0.9194707824 \t AUC:0.9253165311\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1813608067 \tAcc: 0.9270129755 \tTPR:0.9342710659 \tFPR:0.0793492161 \tF1:0.9210562550 \t AUC:0.9274609249\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1769664277 \tAcc: 0.9286876777 \tTPR:0.9383202657 \tFPR:0.0794909695 \tF1:0.9236902794 \t AUC:0.9294146481\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2116593496 \tAcc: 0.9197916667 \tTPR:0.8726815501 \tFPR:0.0361762553 \tF1:0.9124542837 \tAUC:0.9182526474 \ttest cost: 0:00:05\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0778038721 \tAcc: 0.9726190476 \tTPR:0.9921847792 \tFPR:0.2198275862 \tF1:0.9848360076 \t AUC:0.8863132479\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0656673573 \tAcc: 0.9784970238 \tTPR:0.9921517098 \tFPR:0.2111111111 \tF1:0.9883148832 \t AUC:0.8900842832\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0638851555 \tAcc: 0.9752232143 \tTPR:0.9896243465 \tFPR:0.2175757576 \tF1:0.9864625706 \t AUC:0.8864049466\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0564564827 \tAcc: 0.9770833333 \tTPR:0.9949386885 \tFPR:0.2353801170 \tF1:0.9875978573 \t AUC:0.8796460934\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0551779453 \tAcc: 0.9795386905 \tTPR:0.9925347026 \tFPR:0.1675757576 \tF1:0.9887479144 \t AUC:0.9124242317\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.2125280199 \tAcc: 0.9308333333 \tTPR:0.9346493576 \tFPR:0.0725858023 \tF1:0.9295820100 \tAUC:0.9310317776 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1400042700 \tAcc: 0.9425317797 \tTPR:0.7921497053 \tFPR:0.0290774396 \tF1:0.7852390868 \t AUC:0.8806478837\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1305372308 \tAcc: 0.9456567797 \tTPR:0.8041952006 \tFPR:0.0282039991 \tF1:0.7921136887 \t AUC:0.8871588281\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.1261865997 \tAcc: 0.9467690678 \tTPR:0.8124325768 \tFPR:0.0280767500 \tF1:0.8000079759 \t AUC:0.8913763432\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.1249911371 \tAcc: 0.9477401130 \tTPR:0.8079802260 \tFPR:0.0271826400 \tF1:0.8041932919 \t AUC:0.8898532823\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.1188864741 \tAcc: 0.9495233051 \tTPR:0.8189512151 \tFPR:0.0263507098 \tF1:0.8066189241 \t AUC:0.8960438096\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.2887075661 \tAcc: 0.8908333333 \tTPR:0.8002676435 \tFPR:0.0218941536 \tF1:0.8744844585 \tAUC:0.8891867450 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.3026770692 \tAcc: 0.8964166667 \tTPR:0.8274448882 \tFPR:0.0367983485 \tF1:0.8802644931 \tAUC:0.8953232699\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 5:  =============\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1839016023 \tAcc: 0.9287027311 \tTPR:0.9706609331 \tFPR:0.2344188550 \tF1:0.9545928228 \t AUC:0.8681210390\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.1701269594 \tAcc: 0.9387473739 \tTPR:0.9795180793 \tFPR:0.2129844362 \tF1:0.9606201674 \t AUC:0.8832668215\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.1660873233 \tAcc: 0.9387473739 \tTPR:0.9728266851 \tFPR:0.2074688209 \tF1:0.9604423223 \t AUC:0.8826789321\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.1392019291 \tAcc: 0.9510241597 \tTPR:0.9869077915 \tFPR:0.1911401099 \tF1:0.9683330225 \t AUC:0.8978838408\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.1608956259 \tAcc: 0.9397321429 \tTPR:0.9769657643 \tFPR:0.2148809524 \tF1:0.9620160810 \t AUC:0.8810424060\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.1865186508 \tAcc: 0.9310416667 \tTPR:0.9472644959 \tFPR:0.0861110771 \tF1:0.9313427994 \tAUC:0.9305767094 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1714217501 \tAcc: 0.9334526230 \tTPR:0.9630217391 \tFPR:0.1193242436 \tF1:0.9479088980 \t AUC:0.9218487478\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1663076399 \tAcc: 0.9354419411 \tTPR:0.9650767617 \tFPR:0.1177170818 \tF1:0.9495045525 \t AUC:0.9236798400\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1631339174 \tAcc: 0.9366209505 \tTPR:0.9649911395 \tFPR:0.1143238874 \tF1:0.9503069147 \t AUC:0.9253336260\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1574202855 \tAcc: 0.9391789428 \tTPR:0.9666683184 \tFPR:0.1105362884 \tF1:0.9522870217 \t AUC:0.9280660150\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1555428222 \tAcc: 0.9398017764 \tTPR:0.9677824817 \tFPR:0.1112706081 \tF1:0.9530229383 \t AUC:0.9282559368\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.2291072843 \tAcc: 0.9137500000 \tTPR:0.8802291693 \tFPR:0.0528760634 \tF1:0.9089032041 \tAUC:0.9136765529 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1785705263 \tAcc: 0.9286071365 \tTPR:0.9388746970 \tFPR:0.0792377269 \tF1:0.9238212514 \t AUC:0.9298184850\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1763840061 \tAcc: 0.9297513775 \tTPR:0.9408497349 \tFPR:0.0816496969 \tF1:0.9243505705 \t AUC:0.9296000190\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1700470821 \tAcc: 0.9325786527 \tTPR:0.9384564741 \tFPR:0.0739276735 \tF1:0.9253703432 \t AUC:0.9322644003\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1651037576 \tAcc: 0.9330369046 \tTPR:0.9407056019 \tFPR:0.0730465634 \tF1:0.9285171349 \t AUC:0.9338295193\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1611068090 \tAcc: 0.9349060167 \tTPR:0.9470340896 \tFPR:0.0770522270 \tF1:0.9310593283 \t AUC:0.9349909313\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2184794726 \tAcc: 0.9245833333 \tTPR:0.8890980447 \tFPR:0.0402516039 \tF1:0.9193091527 \tAUC:0.9244232204 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.1048269286 \tAcc: 0.9576937135 \tTPR:0.7729477840 \tFPR:0.0185377594 \tF1:0.7626043265 \t AUC:0.8744858241\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0993936626 \tAcc: 0.9594298246 \tTPR:0.7914462081 \tFPR:0.0182778127 \tF1:0.7787221958 \t AUC:0.8829558034\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0975628008 \tAcc: 0.9608461257 \tTPR:0.7899517312 \tFPR:0.0169628420 \tF1:0.7850900318 \t AUC:0.8834937550\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0929444791 \tAcc: 0.9616593567 \tTPR:0.7977263761 \tFPR:0.0173127065 \tF1:0.7856723187 \t AUC:0.8877843962\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0920658585 \tAcc: 0.9621619152 \tTPR:0.7890733779 \tFPR:0.0165071599 \tF1:0.7848970896 \t AUC:0.8842398556\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.5399408102 \tAcc: 0.8304166667 \tTPR:0.6646431225 \tFPR:0.0089047818 \tF1:0.7886074914 \tAUC:0.8278691704 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1287254011 \tAcc: 0.9490642655 \tTPR:0.8139048883 \tFPR:0.0255965367 \tF1:0.8127107910 \t AUC:0.8936254965\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1214611756 \tAcc: 0.9488347458 \tTPR:0.8128907815 \tFPR:0.0266007452 \tF1:0.8061781577 \t AUC:0.8923454061\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.1184976364 \tAcc: 0.9504237288 \tTPR:0.8249181688 \tFPR:0.0259590221 \tF1:0.8190234370 \t AUC:0.8994795734\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.1148886749 \tAcc: 0.9542549435 \tTPR:0.8358776424 \tFPR:0.0243343657 \tF1:0.8274036898 \t AUC:0.9053053817\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.1090180920 \tAcc: 0.9539371469 \tTPR:0.8269617075 \tFPR:0.0230629845 \tF1:0.8259551479 \t AUC:0.9017042648\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.2930536228 \tAcc: 0.8977083333 \tTPR:0.8149154484 \tFPR:0.0203224986 \tF1:0.8842009658 \tAUC:0.8972964749 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2934199682 \tAcc: 0.8995000000 \tTPR:0.8392300562 \tFPR:0.0416932049 \tF1:0.8864727227 \tAUC:0.8987684256\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 6:  =============\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1395488795 \tAcc: 0.9480399408 \tTPR:0.9781773567 \tFPR:0.1536090799 \tF1:0.9659794590 \t AUC:0.9122841384\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1332714671 \tAcc: 0.9503513314 \tTPR:0.9779586176 \tFPR:0.1473803399 \tF1:0.9677117272 \t AUC:0.9152891388\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1310335377 \tAcc: 0.9518306213 \tTPR:0.9789397241 \tFPR:0.1387126268 \tF1:0.9683915925 \t AUC:0.9200823020\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1242208629 \tAcc: 0.9537721893 \tTPR:0.9801888683 \tFPR:0.1375458864 \tF1:0.9697912889 \t AUC:0.9212920976\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1228954273 \tAcc: 0.9541420118 \tTPR:0.9809356636 \tFPR:0.1364428598 \tF1:0.9700052365 \t AUC:0.9222181165\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.2028087259 \tAcc: 0.9272916667 \tTPR:0.9530984786 \tFPR:0.0993066898 \tF1:0.9251329430 \tAUC:0.9268958944 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1540412623 \tAcc: 0.9403579523 \tTPR:0.9680631781 \tFPR:0.1097479014 \tF1:0.9533842699 \t AUC:0.9291576383\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1493396978 \tAcc: 0.9422931109 \tTPR:0.9686386796 \tFPR:0.1063936699 \tF1:0.9548338509 \t AUC:0.9311225048\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1450754305 \tAcc: 0.9441616118 \tTPR:0.9696081634 \tFPR:0.1022013260 \tF1:0.9563569961 \t AUC:0.9337034187\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1432873186 \tAcc: 0.9449344254 \tTPR:0.9705905330 \tFPR:0.1021636736 \tF1:0.9568258404 \t AUC:0.9342134297\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1394830160 \tAcc: 0.9461384315 \tTPR:0.9705882349 \tFPR:0.0975090618 \tF1:0.9576034759 \t AUC:0.9365395866\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1981828134 \tAcc: 0.9291666667 \tTPR:0.9197474548 \tFPR:0.0580628718 \tF1:0.9263867426 \tAUC:0.9308422915 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1206945176 \tAcc: 0.9498587571 \tTPR:0.8133539797 \tFPR:0.0267980907 \tF1:0.8140055423 \t AUC:0.8932779445\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1161934136 \tAcc: 0.9519244350 \tTPR:0.8110815581 \tFPR:0.0243410474 \tF1:0.8091748443 \t AUC:0.8931026655\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.1132588435 \tAcc: 0.9522422316 \tTPR:0.8245390752 \tFPR:0.0245800056 \tF1:0.8137696475 \t AUC:0.8992297018\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.1064117563 \tAcc: 0.9556320621 \tTPR:0.8353589364 \tFPR:0.0229840482 \tF1:0.8363483060 \t AUC:0.9061874441\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.1032475678 \tAcc: 0.9589512712 \tTPR:0.8492532264 \tFPR:0.0221299906 \tF1:0.8380522473 \t AUC:0.9133480955\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.2565114909 \tAcc: 0.9212500000 \tTPR:0.8740010657 \tFPR:0.0320306405 \tF1:0.9144728406 \tAUC:0.9209852126 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1541992379 \tAcc: 0.9445684524 \tTPR:0.9761318413 \tFPR:0.1249450382 \tF1:0.9595865454 \t AUC:0.9255934015\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1437072454 \tAcc: 0.9476686508 \tTPR:0.9764185652 \tFPR:0.1230458199 \tF1:0.9614410399 \t AUC:0.9266863727\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1421499156 \tAcc: 0.9474206349 \tTPR:0.9751105137 \tFPR:0.1164402445 \tF1:0.9615884549 \t AUC:0.9293351346\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1431734711 \tAcc: 0.9465525794 \tTPR:0.9708967957 \tFPR:0.1031236125 \tF1:0.9596612767 \t AUC:0.9338865916\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1353606612 \tAcc: 0.9490327381 \tTPR:0.9772722671 \tFPR:0.1091843011 \tF1:0.9625271849 \t AUC:0.9340439830\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.1975815421 \tAcc: 0.9272916667 \tTPR:0.9342040651 \tFPR:0.0819948672 \tF1:0.9253769792 \tAUC:0.9261045990 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1088754474 \tAcc: 0.9622552711 \tTPR:0.9862218749 \tFPR:0.1339554104 \tF1:0.9764805745 \t AUC:0.9261332323\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.1072595565 \tAcc: 0.9611257530 \tTPR:0.9846300098 \tFPR:0.1390111562 \tF1:0.9755411835 \t AUC:0.9228094268\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.1074991993 \tAcc: 0.9607492470 \tTPR:0.9859097709 \tFPR:0.1394618434 \tF1:0.9754524536 \t AUC:0.9232239638\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0984217608 \tAcc: 0.9627259036 \tTPR:0.9857490745 \tFPR:0.1370135303 \tF1:0.9767145716 \t AUC:0.9243677721\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0969410445 \tAcc: 0.9658320783 \tTPR:0.9875168450 \tFPR:0.1204823623 \tF1:0.9784700096 \t AUC:0.9335172413\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1915543216 \tAcc: 0.9337500000 \tTPR:0.9798605206 \tFPR:0.1126144506 \tF1:0.9349755860 \tAUC:0.9336230350 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2093277788 \tAcc: 0.9277500000 \tTPR:0.9321823170 \tFPR:0.0768019040 \tF1:0.9252690183 \tAUC:0.9276902065\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 7:  =============\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1407900585 \tAcc: 0.9457197374 \tTPR:0.9706506801 \tFPR:0.0994165042 \tF1:0.9575268593 \t AUC:0.9356170879\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1383690821 \tAcc: 0.9473299393 \tTPR:0.9726969244 \tFPR:0.1000989932 \tF1:0.9589463993 \t AUC:0.9362989656\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1344671325 \tAcc: 0.9472903613 \tTPR:0.9717651075 \tFPR:0.0966383179 \tF1:0.9586476894 \t AUC:0.9375633948\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1321535200 \tAcc: 0.9500524930 \tTPR:0.9729401925 \tFPR:0.0914664995 \tF1:0.9607702168 \t AUC:0.9407368465\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1286301873 \tAcc: 0.9510127816 \tTPR:0.9734556712 \tFPR:0.0898043201 \tF1:0.9615693922 \t AUC:0.9418256755\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.2001330374 \tAcc: 0.9295833333 \tTPR:0.9480034655 \tFPR:0.0881812953 \tF1:0.9286519848 \tAUC:0.9299110851 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1406378240 \tAcc: 0.9472966270 \tTPR:0.9743165660 \tFPR:0.1054863126 \tF1:0.9602484669 \t AUC:0.9344151267\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1439688497 \tAcc: 0.9450644841 \tTPR:0.9755389725 \tFPR:0.1138705045 \tF1:0.9584848597 \t AUC:0.9308342340\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1321562286 \tAcc: 0.9487847222 \tTPR:0.9749860689 \tFPR:0.1085989952 \tF1:0.9615825568 \t AUC:0.9331935369\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1295610482 \tAcc: 0.9490327381 \tTPR:0.9746700509 \tFPR:0.1002019806 \tF1:0.9614827703 \t AUC:0.9372340352\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1268017524 \tAcc: 0.9569692460 \tTPR:0.9824633245 \tFPR:0.0967721371 \tF1:0.9684817473 \t AUC:0.9428455937\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2101450087 \tAcc: 0.9254166667 \tTPR:0.9242784892 \tFPR:0.0740434299 \tF1:0.9224935332 \tAUC:0.9251175296 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1516987113 \tAcc: 0.9421854083 \tTPR:0.9527762561 \tFPR:0.0653193523 \tF1:0.9338240328 \t AUC:0.9437284519\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1383336903 \tAcc: 0.9450301205 \tTPR:0.9471163345 \tFPR:0.0550547099 \tF1:0.9366179221 \t AUC:0.9460308123\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1374529947 \tAcc: 0.9451974565 \tTPR:0.9529016167 \tFPR:0.0594252644 \tF1:0.9378899215 \t AUC:0.9467381761\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1328401882 \tAcc: 0.9442352744 \tTPR:0.9486359011 \tFPR:0.0601535773 \tF1:0.9344688846 \t AUC:0.9442411619\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.1304302365 \tAcc: 0.9532714190 \tTPR:0.9516068308 \tFPR:0.0465613290 \tF1:0.9474054908 \t AUC:0.9525227509\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2076235897 \tAcc: 0.9268750000 \tTPR:0.8911339551 \tFPR:0.0361393077 \tF1:0.9223050974 \tAUC:0.9274973237 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1241838071 \tAcc: 0.9561760355 \tTPR:0.9821934328 \tFPR:0.1312135359 \tF1:0.9712911665 \t AUC:0.9254635292\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1194021249 \tAcc: 0.9559911243 \tTPR:0.9818075073 \tFPR:0.1296355736 \tF1:0.9711894083 \t AUC:0.9260859669\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1142865682 \tAcc: 0.9587647929 \tTPR:0.9832034278 \tFPR:0.1229157020 \tF1:0.9730274269 \t AUC:0.9301438629\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1068269531 \tAcc: 0.9605214497 \tTPR:0.9838208138 \tFPR:0.1204983708 \tF1:0.9742880044 \t AUC:0.9316612215\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1076114670 \tAcc: 0.9619082840 \tTPR:0.9838646936 \tFPR:0.1135032210 \tF1:0.9752268942 \t AUC:0.9351807363\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1917269191 \tAcc: 0.9314583333 \tTPR:0.9291842929 \tFPR:0.0669896863 \tF1:0.9268698652 \tAUC:0.9310973033 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1682699875 \tAcc: 0.9312094517 \tTPR:0.9404345633 \tFPR:0.0763624919 \tF1:0.9262981697 \t AUC:0.9320360357\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1608337779 \tAcc: 0.9353087229 \tTPR:0.9440364751 \tFPR:0.0722226478 \tF1:0.9299542310 \t AUC:0.9359069136\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1559480468 \tAcc: 0.9375722094 \tTPR:0.9462556787 \tFPR:0.0712555619 \tF1:0.9330026157 \t AUC:0.9375000584\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1496557028 \tAcc: 0.9403189433 \tTPR:0.9479401168 \tFPR:0.0665000164 \tF1:0.9356357616 \t AUC:0.9407200502\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1481590846 \tAcc: 0.9394246578 \tTPR:0.9487591972 \tFPR:0.0686562330 \tF1:0.9353331283 \t AUC:0.9400514821\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2655674246 \tAcc: 0.9095833333 \tTPR:0.8385529685 \tFPR:0.0230508549 \tF1:0.8983401759 \tAUC:0.9077510568 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2150391959 \tAcc: 0.9245833333 \tTPR:0.9062306342 \tFPR:0.0576809148 \tF1:0.9197321313 \tAUC:0.9242748597\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 8:  =============\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1131298742 \tAcc: 0.9516596045 \tTPR:0.8317311533 \tFPR:0.0267474908 \tF1:0.8279133998 \t AUC:0.9022534901\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1103268471 \tAcc: 0.9556320621 \tTPR:0.8380236995 \tFPR:0.0230767155 \tF1:0.8347613888 \t AUC:0.9072440638\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.1040421672 \tAcc: 0.9578389831 \tTPR:0.8508180065 \tFPR:0.0231080774 \tF1:0.8366349175 \t AUC:0.9134311521\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0982356065 \tAcc: 0.9588453390 \tTPR:0.8524221228 \tFPR:0.0220468472 \tF1:0.8455160353 \t AUC:0.9151876378\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0957934308 \tAcc: 0.9602224576 \tTPR:0.8574140314 \tFPR:0.0215535765 \tF1:0.8462052789 \t AUC:0.9175251536\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.3087030746 \tAcc: 0.8964583333 \tTPR:0.8122911955 \tFPR:0.0199126874 \tF1:0.8830199729 \tAUC:0.8961892541 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1092085993 \tAcc: 0.9603727410 \tTPR:0.9850751868 \tFPR:0.1451066169 \tF1:0.9751818572 \t AUC:0.9199842849\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.1035295366 \tAcc: 0.9627259036 \tTPR:0.9857533789 \tFPR:0.1287359645 \tF1:0.9764612146 \t AUC:0.9285087072\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0977626003 \tAcc: 0.9644201807 \tTPR:0.9868104162 \tFPR:0.1282823169 \tF1:0.9777340794 \t AUC:0.9292441258\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0955622570 \tAcc: 0.9659262048 \tTPR:0.9878950877 \tFPR:0.1279168623 \tF1:0.9787282837 \t AUC:0.9299891127\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0908052519 \tAcc: 0.9674322289 \tTPR:0.9870073772 \tFPR:0.1133714094 \tF1:0.9794813733 \t AUC:0.9368179839\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1783113062 \tAcc: 0.9370833333 \tTPR:0.9613981444 \tFPR:0.0870571224 \tF1:0.9373562556 \tAUC:0.9371705110 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1441736650 \tAcc: 0.9427710843 \tTPR:0.9459347097 \tFPR:0.0598074700 \tF1:0.9336498436 \t AUC:0.9430636199\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1308865724 \tAcc: 0.9506777108 \tTPR:0.9522816369 \tFPR:0.0498081342 \tF1:0.9436751125 \t AUC:0.9512367513\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1315485938 \tAcc: 0.9510542169 \tTPR:0.9575314146 \tFPR:0.0541367892 \tF1:0.9451666061 \t AUC:0.9516973127\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1225107139 \tAcc: 0.9480421687 \tTPR:0.9546314290 \tFPR:0.0572792237 \tF1:0.9414364322 \t AUC:0.9486761027\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.1183769176 \tAcc: 0.9527275770 \tTPR:0.9545029460 \tFPR:0.0500211640 \tF1:0.9430364431 \t AUC:0.9522408910\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2512302947 \tAcc: 0.9212500000 \tTPR:0.8707824591 \tFPR:0.0319230337 \tF1:0.9132772737 \tAUC:0.9194297127 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0982157084 \tAcc: 0.9599232456 \tTPR:0.7866872041 \tFPR:0.0184007356 \tF1:0.7748329278 \t AUC:0.8810959086\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0915546368 \tAcc: 0.9616684942 \tTPR:0.7926778752 \tFPR:0.0173646265 \tF1:0.7863201278 \t AUC:0.8845343032\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0886171564 \tAcc: 0.9647295322 \tTPR:0.8157535041 \tFPR:0.0158138342 \tF1:0.8052022517 \t AUC:0.8974800174\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0870621658 \tAcc: 0.9636330409 \tTPR:0.8078297596 \tFPR:0.0170631016 \tF1:0.8002086166 \t AUC:0.8936675233\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0823111995 \tAcc: 0.9661458333 \tTPR:0.8187343358 \tFPR:0.0157384391 \tF1:0.8105507407 \t AUC:0.8990484124\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.4639998901 \tAcc: 0.8504166667 \tTPR:0.7049508026 \tFPR:0.0084722498 \tF1:0.8167721979 \tAUC:0.8482392764 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1226338769 \tAcc: 0.9546967456 \tTPR:0.9783930564 \tFPR:0.1241778169 \tF1:0.9699797936 \t AUC:0.9270755620\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1133187406 \tAcc: 0.9571930473 \tTPR:0.9814259757 \tFPR:0.1233615873 \tF1:0.9718481865 \t AUC:0.9290046363\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1089366546 \tAcc: 0.9599667160 \tTPR:0.9826407719 \tFPR:0.1164501259 \tF1:0.9739127057 \t AUC:0.9330695675\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1045939363 \tAcc: 0.9607988166 \tTPR:0.9834184523 \tFPR:0.1161348707 \tF1:0.9743121170 \t AUC:0.9336171891\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1021111509 \tAcc: 0.9629252959 \tTPR:0.9846056815 \tFPR:0.1066221530 \tF1:0.9756150117 \t AUC:0.9389689240\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1954525412 \tAcc: 0.9352083333 \tTPR:0.9540055108 \tFPR:0.0846823794 \tF1:0.9338143694 \tAUC:0.9346615657 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2795394214 \tAcc: 0.9080833333 \tTPR:0.8606856225 \tFPR:0.0464094945 \tF1:0.8968480139 \tAUC:0.9071380640\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 9:  =============\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1262397903 \tAcc: 0.9518072289 \tTPR:0.9622601388 \tFPR:0.0534611586 \tF1:0.9440868218 \t AUC:0.9543994901\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1273978432 \tAcc: 0.9500920348 \tTPR:0.9522336448 \tFPR:0.0494021667 \tF1:0.9421415640 \t AUC:0.9514157390\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1155659138 \tAcc: 0.9529367470 \tTPR:0.9545650640 \tFPR:0.0480098540 \tF1:0.9474882497 \t AUC:0.9532776050\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1091186448 \tAcc: 0.9604668675 \tTPR:0.9618974134 \tFPR:0.0410616780 \tF1:0.9544919780 \t AUC:0.9604178677\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.1006488803 \tAcc: 0.9646084337 \tTPR:0.9673402649 \tFPR:0.0379171221 \tF1:0.9585309847 \t AUC:0.9647115714\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2802620039 \tAcc: 0.9227083333 \tTPR:0.8703466381 \tFPR:0.0260082179 \tF1:0.9136857263 \tAUC:0.9221692101 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1294205163 \tAcc: 0.9503628683 \tTPR:0.9737863142 \tFPR:0.0927608287 \tF1:0.9610408660 \t AUC:0.9405127427\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1249681184 \tAcc: 0.9514210605 \tTPR:0.9740230539 \tFPR:0.0883147131 \tF1:0.9617460556 \t AUC:0.9428541704\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1222398222 \tAcc: 0.9538582356 \tTPR:0.9762393950 \tFPR:0.0858706418 \tF1:0.9637325343 \t AUC:0.9451843766\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1174982178 \tAcc: 0.9543435875 \tTPR:0.9759168307 \tFPR:0.0848999736 \tF1:0.9642234359 \t AUC:0.9455084285\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1157809402 \tAcc: 0.9552913778 \tTPR:0.9775248851 \tFPR:0.0853267281 \tF1:0.9650483767 \t AUC:0.9460990785\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.2094793688 \tAcc: 0.9266666667 \tTPR:0.9080879081 \tFPR:0.0573694967 \tF1:0.9223608941 \tAUC:0.9253592057 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1594946993 \tAcc: 0.9362668859 \tTPR:0.9491681504 \tFPR:0.0746941234 \tF1:0.9313473808 \t AUC:0.9372370135\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1513289507 \tAcc: 0.9402939477 \tTPR:0.9521507365 \tFPR:0.0689272897 \tF1:0.9367751629 \t AUC:0.9416117234\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1480962339 \tAcc: 0.9398190322 \tTPR:0.9485349912 \tFPR:0.0692115135 \tF1:0.9349278860 \t AUC:0.9396617389\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1416189785 \tAcc: 0.9427018530 \tTPR:0.9526579475 \tFPR:0.0651713685 \tF1:0.9388041941 \t AUC:0.9437432895\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1403967330 \tAcc: 0.9439432990 \tTPR:0.9547183950 \tFPR:0.0655355633 \tF1:0.9391384599 \t AUC:0.9445914158\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.1989497386 \tAcc: 0.9300000000 \tTPR:0.8896456287 \tFPR:0.0321913087 \tF1:0.9233451969 \tAUC:0.9287271600 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1363702902 \tAcc: 0.9505208333 \tTPR:0.9787702944 \tFPR:0.1103638172 \tF1:0.9635671260 \t AUC:0.9342032386\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1291390107 \tAcc: 0.9508928571 \tTPR:0.9750079545 \tFPR:0.0957876614 \tF1:0.9633453576 \t AUC:0.9396101465\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1210803349 \tAcc: 0.9528769841 \tTPR:0.9756189650 \tFPR:0.0941760125 \tF1:0.9645940820 \t AUC:0.9407214762\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1178030473 \tAcc: 0.9577132937 \tTPR:0.9819141649 \tFPR:0.0904005551 \tF1:0.9687167229 \t AUC:0.9457568049\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1105614675 \tAcc: 0.9590773810 \tTPR:0.9794268729 \tFPR:0.0854687242 \tF1:0.9695454281 \t AUC:0.9469790743\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2001620236 \tAcc: 0.9316666667 \tTPR:0.9274149398 \tFPR:0.0640437533 \tF1:0.9293109083 \tAUC:0.9316855933 \ttest cost: 0:00:05\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0584132527 \tAcc: 0.9817708333 \tTPR:0.9949899635 \tFPR:0.1698113208 \tF1:0.9900594161 \t AUC:0.9125532812\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0462507253 \tAcc: 0.9817708333 \tTPR:0.9936561291 \tFPR:0.2163333333 \tF1:0.9899936754 \t AUC:0.8883395108\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0441597624 \tAcc: 0.9822916667 \tTPR:0.9939006138 \tFPR:0.1629629630 \tF1:0.9903501605 \t AUC:0.9154193225\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0419996079 \tAcc: 0.9838541667 \tTPR:0.9938544505 \tFPR:0.1455128205 \tF1:0.9911847931 \t AUC:0.9242990420\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0381715634 \tAcc: 0.9848958333 \tTPR:0.9960266337 \tFPR:0.1716049383 \tF1:0.9918130425 \t AUC:0.9122794570\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.2166873354 \tAcc: 0.9366666667 \tTPR:0.9665345228 \tFPR:0.0916622217 \tF1:0.9369100198 \tAUC:0.9374361506 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2211080941 \tAcc: 0.9295416667 \tTPR:0.9124059275 \tFPR:0.0542549997 \tF1:0.9251225491 \tAUC:0.9290754639\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 10:  =============\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0525033042 \tAcc: 0.9802083333 \tTPR:0.9933810329 \tFPR:0.1729166667 \tF1:0.9892187056 \t AUC:0.9102748093\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0433665164 \tAcc: 0.9842261905 \tTPR:0.9943135555 \tFPR:0.1425595238 \tF1:0.9912459516 \t AUC:0.9259529464\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0390809786 \tAcc: 0.9875000000 \tTPR:0.9960839891 \tFPR:0.1218614719 \tF1:0.9931446023 \t AUC:0.9369332581\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0373810103 \tAcc: 0.9848958333 \tTPR:0.9948248439 \tFPR:0.1166666667 \tF1:0.9915788376 \t AUC:0.9391279451\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0328823322 \tAcc: 0.9875000000 \tTPR:0.9967001529 \tFPR:0.1354545455 \tF1:0.9932405942 \t AUC:0.9307569016\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.3081322778 \tAcc: 0.9256250000 \tTPR:0.9843813191 \tFPR:0.1344124581 \tF1:0.9278140485 \tAUC:0.9249844305 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1714388506 \tAcc: 0.9454438025 \tTPR:0.9731826018 \tFPR:0.1674899505 \tF1:0.9655220291 \t AUC:0.9028463257\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.1495556616 \tAcc: 0.9377626050 \tTPR:0.9744926576 \tFPR:0.2239795918 \tF1:0.9604666640 \t AUC:0.8752565329\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.1315333056 \tAcc: 0.9543723739 \tTPR:0.9820791430 \tFPR:0.1468859514 \tF1:0.9709506036 \t AUC:0.9175965958\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.1387659068 \tAcc: 0.9520089286 \tTPR:0.9843953390 \tFPR:0.1688388992 \tF1:0.9694071042 \t AUC:0.9077782199\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.1376911633 \tAcc: 0.9499080882 \tTPR:0.9800494772 \tFPR:0.1469130076 \tF1:0.9675008322 \t AUC:0.9165682348\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.1746486669 \tAcc: 0.9352083333 \tTPR:0.9644731941 \tFPR:0.0950911883 \tF1:0.9363328026 \tAUC:0.9346910029 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1205424895 \tAcc: 0.9577477811 \tTPR:0.9816151196 \tFPR:0.1218225395 \tF1:0.9723056272 \t AUC:0.9306108526\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1083617144 \tAcc: 0.9618158284 \tTPR:0.9846134797 \tFPR:0.1121289690 \tF1:0.9749948380 \t AUC:0.9362194267\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1101743282 \tAcc: 0.9613535503 \tTPR:0.9854326871 \tFPR:0.1125025533 \tF1:0.9746888916 \t AUC:0.9364650669\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1024463739 \tAcc: 0.9632026627 \tTPR:0.9856874749 \tFPR:0.1119960817 \tF1:0.9758915095 \t AUC:0.9368244614\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0953846071 \tAcc: 0.9657914201 \tTPR:0.9865451869 \tFPR:0.1056442898 \tF1:0.9776796071 \t AUC:0.9404304859\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1859954386 \tAcc: 0.9360416667 \tTPR:0.9585552224 \tFPR:0.0855942531 \tF1:0.9344817922 \tAUC:0.9364804846 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1232360816 \tAcc: 0.9516369048 \tTPR:0.9741985264 \tFPR:0.0968107387 \tF1:0.9636541310 \t AUC:0.9386938939\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1211591003 \tAcc: 0.9579613095 \tTPR:0.9812542751 \tFPR:0.0892376473 \tF1:0.9690695403 \t AUC:0.9460083139\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1118085346 \tAcc: 0.9549851190 \tTPR:0.9753626208 \tFPR:0.0903077846 \tF1:0.9660162514 \t AUC:0.9423790002\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1064188690 \tAcc: 0.9590773810 \tTPR:0.9774590012 \tFPR:0.0783436636 \tF1:0.9693807629 \t AUC:0.9495576688\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1018108590 \tAcc: 0.9624255952 \tTPR:0.9828263566 \tFPR:0.0818478810 \tF1:0.9718770916 \t AUC:0.9504892378\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2209245244 \tAcc: 0.9266666667 \tTPR:0.9135036664 \tFPR:0.0607155620 \tF1:0.9241338778 \tAUC:0.9263940522 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1198433861 \tAcc: 0.9541977736 \tTPR:0.9753043465 \tFPR:0.0848655080 \tF1:0.9640378928 \t AUC:0.9452194193\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1144927849 \tAcc: 0.9553726170 \tTPR:0.9756294790 \tFPR:0.0819147165 \tF1:0.9650478982 \t AUC:0.9468573812\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1121446915 \tAcc: 0.9561037695 \tTPR:0.9766758767 \tFPR:0.0810872182 \tF1:0.9655446060 \t AUC:0.9477943292\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1062306859 \tAcc: 0.9584597054 \tTPR:0.9782494051 \tFPR:0.0769700320 \tF1:0.9673702048 \t AUC:0.9506396866\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1066618617 \tAcc: 0.9599220104 \tTPR:0.9795390040 \tFPR:0.0752572299 \tF1:0.9683564581 \t AUC:0.9521408870\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.2134732498 \tAcc: 0.9302083333 \tTPR:0.9226656919 \tFPR:0.0585926154 \tF1:0.9273682663 \tAUC:0.9320365383 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2206348315 \tAcc: 0.9307500000 \tTPR:0.9487158188 \tFPR:0.0868812154 \tF1:0.9300261575 \tAUC:0.9309173017\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 11:  =============\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1053477241 \tAcc: 0.9570091808 \tTPR:0.8669062497 \tFPR:0.0262049938 \tF1:0.8430717929 \t AUC:0.9199725207\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0964310293 \tAcc: 0.9613700565 \tTPR:0.8621757323 \tFPR:0.0205558787 \tF1:0.8561491349 \t AUC:0.9204183806\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0917962501 \tAcc: 0.9623940678 \tTPR:0.8668516277 \tFPR:0.0204689977 \tF1:0.8616461320 \t AUC:0.9230027195\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0836900222 \tAcc: 0.9667549435 \tTPR:0.8856948215 \tFPR:0.0180642596 \tF1:0.8753831293 \t AUC:0.9333267973\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0815836900 \tAcc: 0.9669844633 \tTPR:0.8855582663 \tFPR:0.0180756950 \tF1:0.8806810977 \t AUC:0.9334161671\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.3210217498 \tAcc: 0.9018750000 \tTPR:0.8230774930 \tFPR:0.0187402564 \tF1:0.8893985722 \tAUC:0.9021686183 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1278706647 \tAcc: 0.9538690476 \tTPR:0.9754641095 \tFPR:0.0879920937 \tF1:0.9658000966 \t AUC:0.9437360079\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1238309344 \tAcc: 0.9516369048 \tTPR:0.9781825960 \tFPR:0.1048772423 \tF1:0.9638947499 \t AUC:0.9366526769\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1123934072 \tAcc: 0.9590773810 \tTPR:0.9778809071 \tFPR:0.0776213963 \tF1:0.9694786146 \t AUC:0.9501297554\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1022165481 \tAcc: 0.9631696429 \tTPR:0.9815053044 \tFPR:0.0735738138 \tF1:0.9725583095 \t AUC:0.9539657453\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0954612063 \tAcc: 0.9654017857 \tTPR:0.9808715951 \tFPR:0.0675718228 \tF1:0.9737390433 \t AUC:0.9566498861\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2238010946 \tAcc: 0.9293750000 \tTPR:0.9258233244 \tFPR:0.0654268570 \tF1:0.9280662409 \tAUC:0.9301982337 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1333843437 \tAcc: 0.9514307229 \tTPR:0.9599989703 \tFPR:0.0527491832 \tF1:0.9448458971 \t AUC:0.9536248935\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1202913226 \tAcc: 0.9521419009 \tTPR:0.9444619873 \tFPR:0.0428846855 \tF1:0.9423661913 \t AUC:0.9507886509\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0964493328 \tAcc: 0.9634789157 \tTPR:0.9682749293 \tFPR:0.0390178729 \tF1:0.9587305375 \t AUC:0.9646285282\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0926273251 \tAcc: 0.9661144578 \tTPR:0.9706954731 \tFPR:0.0396606414 \tF1:0.9607352089 \t AUC:0.9655174159\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0952325988 \tAcc: 0.9634789157 \tTPR:0.9616868072 \tFPR:0.0370799398 \tF1:0.9590935383 \t AUC:0.9623034337\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2708391233 \tAcc: 0.9145833333 \tTPR:0.8502714319 \tFPR:0.0208973750 \tF1:0.9041138117 \tAUC:0.9146870285 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0898841558 \tAcc: 0.9632675439 \tTPR:0.8054372041 \tFPR:0.0174252090 \tF1:0.7988544459 \t AUC:0.8919732519\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0842335390 \tAcc: 0.9653691520 \tTPR:0.8151803119 \tFPR:0.0158654086 \tF1:0.8078956910 \t AUC:0.8977264997\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0806692668 \tAcc: 0.9668768275 \tTPR:0.8319020932 \tFPR:0.0158564249 \tF1:0.8206450624 \t AUC:0.9063944639\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0794578105 \tAcc: 0.9672423246 \tTPR:0.8284426344 \tFPR:0.0149531553 \tF1:0.8201186260 \t AUC:0.9053427105\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0743512020 \tAcc: 0.9702576754 \tTPR:0.8491946406 \tFPR:0.0138577259 \tF1:0.8283489107 \t AUC:0.9153972923\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.3916412737 \tAcc: 0.8793750000 \tTPR:0.7681912451 \tFPR:0.0142467633 \tF1:0.8564875380 \tAUC:0.8769722409 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1026723961 \tAcc: 0.9641378012 \tTPR:0.9851793304 \tFPR:0.1165063183 \tF1:0.9774052754 \t AUC:0.9343365060\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0909452946 \tAcc: 0.9664909639 \tTPR:0.9872661338 \tFPR:0.1222059830 \tF1:0.9789960014 \t AUC:0.9325108400\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0862808986 \tAcc: 0.9679028614 \tTPR:0.9881331451 \tFPR:0.1156127340 \tF1:0.9799025651 \t AUC:0.9362602056\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0823495456 \tAcc: 0.9691265060 \tTPR:0.9885520594 \tFPR:0.1118866826 \tF1:0.9806097194 \t AUC:0.9383326884\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0794607258 \tAcc: 0.9699736446 \tTPR:0.9892008199 \tFPR:0.1042180962 \tF1:0.9811696006 \t AUC:0.9424913619\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1709517205 \tAcc: 0.9397916667 \tTPR:0.9357187144 \tFPR:0.0557254984 \tF1:0.9373485276 \tAUC:0.9399966080 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2756509924 \tAcc: 0.9130000000 \tTPR:0.8606164418 \tFPR:0.0350073500 \tF1:0.9030829381 \tAUC:0.9128045459\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 12:  =============\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0831884683 \tAcc: 0.9661458333 \tTPR:0.8298987684 \tFPR:0.0158694523 \tF1:0.8164594925 \t AUC:0.9039219084\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0783705771 \tAcc: 0.9682383041 \tTPR:0.8285528636 \tFPR:0.0143357632 \tF1:0.8219975825 \t AUC:0.9053173115\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0759396554 \tAcc: 0.9698921784 \tTPR:0.8399906015 \tFPR:0.0141737090 \tF1:0.8306322755 \t AUC:0.9107461571\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0733352316 \tAcc: 0.9693804825 \tTPR:0.8432208995 \tFPR:0.0142636714 \tF1:0.8284734272 \t AUC:0.9117509898\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0700206322 \tAcc: 0.9728618421 \tTPR:0.8547166527 \tFPR:0.0123069613 \tF1:0.8444088887 \t AUC:0.9197974810\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.4659245982 \tAcc: 0.8647916667 \tTPR:0.7387397351 \tFPR:0.0117113877 \tF1:0.8373177628 \tAUC:0.8635141737 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0895427649 \tAcc: 0.9645480226 \tTPR:0.8693835041 \tFPR:0.0184040542 \tF1:0.8682336539 \t AUC:0.9253047158\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0799533412 \tAcc: 0.9677259887 \tTPR:0.8770368128 \tFPR:0.0169821338 \tF1:0.8732154128 \t AUC:0.9300273395\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0775857123 \tAcc: 0.9715218927 \tTPR:0.9038864227 \tFPR:0.0159112854 \tF1:0.8974292282 \t AUC:0.9439875686\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0733638337 \tAcc: 0.9701977401 \tTPR:0.8999316205 \tFPR:0.0169329589 \tF1:0.8844446413 \t AUC:0.9410716881\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0689825664 \tAcc: 0.9735522599 \tTPR:0.9091415568 \tFPR:0.0148946388 \tF1:0.9005049469 \t AUC:0.9471234590\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.3915519089 \tAcc: 0.8947916667 \tTPR:0.7987236791 \tFPR:0.0127135608 \tF1:0.8776423860 \tAUC:0.8930050592 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1522312866 \tAcc: 0.9395607448 \tTPR:0.9467123737 \tFPR:0.0672296121 \tF1:0.9347838633 \t AUC:0.9397413808\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1439155538 \tAcc: 0.9421463962 \tTPR:0.9483912265 \tFPR:0.0643287661 \tF1:0.9381343136 \t AUC:0.9420312302\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1384955567 \tAcc: 0.9446681701 \tTPR:0.9542833774 \tFPR:0.0635882086 \tF1:0.9407016948 \t AUC:0.9453475844\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1328503956 \tAcc: 0.9456263331 \tTPR:0.9506635463 \tFPR:0.0602669492 \tF1:0.9403996471 \t AUC:0.9451982985\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1279624667 \tAcc: 0.9488563144 \tTPR:0.9554040149 \tFPR:0.0575208419 \tF1:0.9452263092 \t AUC:0.9489415865\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.1995182433 \tAcc: 0.9354166667 \tTPR:0.9093524075 \tFPR:0.0378851157 \tF1:0.9314280725 \tAUC:0.9357336459 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1102508530 \tAcc: 0.9602440828 \tTPR:0.9841857542 \tFPR:0.1192317409 \tF1:0.9739273856 \t AUC:0.9324535434\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1096544363 \tAcc: 0.9612610947 \tTPR:0.9808676081 \tFPR:0.1064251987 \tF1:0.9723498422 \t AUC:0.9372212047\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1038310826 \tAcc: 0.9603365385 \tTPR:0.9843470244 \tFPR:0.1155571664 \tF1:0.9739339077 \t AUC:0.9343949290\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0925126777 \tAcc: 0.9646819527 \tTPR:0.9832993240 \tFPR:0.0959941559 \tF1:0.9766843006 \t AUC:0.9436278056\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0885171451 \tAcc: 0.9683801775 \tTPR:0.9877004236 \tFPR:0.0946174910 \tF1:0.9792915849 \t AUC:0.9465232177\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.2087386429 \tAcc: 0.9354166667 \tTPR:0.9469072234 \tFPR:0.0749963676 \tF1:0.9331484013 \tAUC:0.9359554279 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0938021624 \tAcc: 0.9647966867 \tTPR:0.9866605373 \tFPR:0.1306511761 \tF1:0.9779102959 \t AUC:0.9280046806\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0884723766 \tAcc: 0.9681852410 \tTPR:0.9885117241 \tFPR:0.1189422860 \tF1:0.9800077962 \t AUC:0.9347847191\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0850444508 \tAcc: 0.9671498494 \tTPR:0.9875074886 \tFPR:0.1114594993 \tF1:0.9792638632 \t AUC:0.9380239946\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0797369154 \tAcc: 0.9700677711 \tTPR:0.9882827975 \tFPR:0.1023025251 \tF1:0.9810534792 \t AUC:0.9429901362\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0748234220 \tAcc: 0.9714796687 \tTPR:0.9886500563 \tFPR:0.1004835359 \tF1:0.9821314674 \t AUC:0.9440832602\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1754461646 \tAcc: 0.9416666667 \tTPR:0.9541864770 \tFPR:0.0718868864 \tF1:0.9406153492 \tAUC:0.9411497953 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2882359116 \tAcc: 0.9144166667 \tTPR:0.8695819044 \tFPR:0.0418386636 \tF1:0.9040303944 \tAUC:0.9138716204\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 13:  =============\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0535493526 \tAcc: 0.9812500000 \tTPR:0.9936612826 \tFPR:0.1598104056 \tF1:0.9895476054 \t AUC:0.9168626394\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0378157430 \tAcc: 0.9869791667 \tTPR:0.9955455191 \tFPR:0.1534893268 \tF1:0.9929536856 \t AUC:0.9212206913\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0354779432 \tAcc: 0.9869791667 \tTPR:0.9955852016 \tFPR:0.1245454545 \tF1:0.9929363830 \t AUC:0.9356032918\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0336746577 \tAcc: 0.9885416667 \tTPR:0.9965671512 \tFPR:0.1280864198 \tF1:0.9937551318 \t AUC:0.9346283556\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0324269783 \tAcc: 0.9875000000 \tTPR:0.9961242044 \tFPR:0.1200617284 \tF1:0.9932302724 \t AUC:0.9381052679\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.2369392920 \tAcc: 0.9381250000 \tTPR:0.9728888887 \tFPR:0.0963538511 \tF1:0.9383617501 \tAUC:0.9382675188 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0811818457 \tAcc: 0.9669314972 \tTPR:0.8872702000 \tFPR:0.0190288618 \tF1:0.8753876734 \t AUC:0.9339609952\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0688329271 \tAcc: 0.9719632768 \tTPR:0.9044655596 \tFPR:0.0155678442 \tF1:0.8971270184 \t AUC:0.9441774530\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0665006305 \tAcc: 0.9722634181 \tTPR:0.9125212985 \tFPR:0.0161837091 \tF1:0.8982965387 \t AUC:0.9480448872\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0646228274 \tAcc: 0.9739936441 \tTPR:0.9023540490 \tFPR:0.0138210196 \tF1:0.8971330863 \t AUC:0.9439891114\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0577776943 \tAcc: 0.9773128531 \tTPR:0.9140020055 \tFPR:0.0118620041 \tF1:0.9073109586 \t AUC:0.9508256882\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.4228202956 \tAcc: 0.9004166667 \tTPR:0.8147962798 \tFPR:0.0168396317 \tF1:0.8848907351 \tAUC:0.8989783241 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0975670637 \tAcc: 0.9651731928 \tTPR:0.9862355001 \tFPR:0.1250663911 \tF1:0.9781777697 \t AUC:0.9305845545\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0881432301 \tAcc: 0.9667733434 \tTPR:0.9877961227 \tFPR:0.1211039295 \tF1:0.9791323824 \t AUC:0.9333460966\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0815571059 \tAcc: 0.9702560241 \tTPR:0.9890997293 \tFPR:0.1085389343 \tF1:0.9811739669 \t AUC:0.9402803975\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0762101723 \tAcc: 0.9720444277 \tTPR:0.9892280573 \tFPR:0.0978836705 \tF1:0.9823529202 \t AUC:0.9456559215\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0752118527 \tAcc: 0.9731739458 \tTPR:0.9895244082 \tFPR:0.0937475677 \tF1:0.9831164189 \t AUC:0.9478884203\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1909474698 \tAcc: 0.9400000000 \tTPR:0.9631173598 \tFPR:0.0845291292 \tF1:0.9405682228 \tAUC:0.9392941153 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1655798399 \tAcc: 0.9433429622 \tTPR:0.9751681057 \tFPR:0.1930181921 \tF1:0.9634476788 \t AUC:0.8910749568\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.1322489065 \tAcc: 0.9476759454 \tTPR:0.9796743112 \tFPR:0.1665532880 \tF1:0.9660659540 \t AUC:0.9065605116\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.1302842045 \tAcc: 0.9545036765 \tTPR:0.9854985482 \tFPR:0.1624269580 \tF1:0.9709725363 \t AUC:0.9115357951\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.1305623204 \tAcc: 0.9508928571 \tTPR:0.9832688200 \tFPR:0.1690759637 \tF1:0.9688514493 \t AUC:0.9070964281\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.1137016571 \tAcc: 0.9566045168 \tTPR:0.9853169516 \tFPR:0.1571183777 \tF1:0.9713769756 \t AUC:0.9140992870\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.1822227725 \tAcc: 0.9356250000 \tTPR:0.9229202895 \tFPR:0.0518786667 \tF1:0.9341679263 \tAUC:0.9355208114 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1165356041 \tAcc: 0.9536210317 \tTPR:0.9733285154 \tFPR:0.0888819514 \tF1:0.9648461885 \t AUC:0.9422232820\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1055012247 \tAcc: 0.9620535714 \tTPR:0.9821247889 \tFPR:0.0778097758 \tF1:0.9715146354 \t AUC:0.9521575065\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1059675922 \tAcc: 0.9614335317 \tTPR:0.9819467119 \tFPR:0.0784428204 \tF1:0.9708258811 \t AUC:0.9517519458\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0985845414 \tAcc: 0.9636656746 \tTPR:0.9775750090 \tFPR:0.0681686845 \tF1:0.9715923548 \t AUC:0.9547031622\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0944997358 \tAcc: 0.9650297619 \tTPR:0.9809481510 \tFPR:0.0689480328 \tF1:0.9743361691 \t AUC:0.9560000591\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2177238766 \tAcc: 0.9308333333 \tTPR:0.9147787281 \tFPR:0.0545306623 \tF1:0.9267907341 \tAUC:0.9301240329 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2501307413 \tAcc: 0.9290000000 \tTPR:0.9177003092 \tFPR:0.0608263882 \tF1:0.9249558737 \tAUC:0.9284369605\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 14:  =============\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1367305807 \tAcc: 0.9442571321 \tTPR:0.9525136116 \tFPR:0.0635530821 \tF1:0.9392954364 \t AUC:0.9444802648\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1318248168 \tAcc: 0.9477926146 \tTPR:0.9566263772 \tFPR:0.0593538334 \tF1:0.9439732077 \t AUC:0.9486362719\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1229840568 \tAcc: 0.9493395619 \tTPR:0.9570754409 \tFPR:0.0579382551 \tF1:0.9455383724 \t AUC:0.9495685929\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1229561125 \tAcc: 0.9517474671 \tTPR:0.9605904623 \tFPR:0.0551970669 \tF1:0.9482168307 \t AUC:0.9526966977\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1144009898 \tAcc: 0.9547274929 \tTPR:0.9613832082 \tFPR:0.0510181687 \tF1:0.9513379735 \t AUC:0.9551825197\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2360400681 \tAcc: 0.9283333333 \tTPR:0.8918971980 \tFPR:0.0360988244 \tF1:0.9226237216 \tAUC:0.9278991868 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0698297512 \tAcc: 0.9698975989 \tTPR:0.8952525660 \tFPR:0.0171580490 \tF1:0.8911698494 \t AUC:0.9385996199\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0633106006 \tAcc: 0.9748411017 \tTPR:0.9146540841 \tFPR:0.0141637477 \tF1:0.9004762941 \t AUC:0.9497574773\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0597161436 \tAcc: 0.9767831921 \tTPR:0.9172045312 \tFPR:0.0126213663 \tF1:0.9109562139 \t AUC:0.9519377557\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0530316037 \tAcc: 0.9789018362 \tTPR:0.9254315756 \tFPR:0.0120916135 \tF1:0.9226319887 \t AUC:0.9563513126\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0527147261 \tAcc: 0.9798728814 \tTPR:0.9313559322 \tFPR:0.0112199975 \tF1:0.9249696508 \t AUC:0.9599707378\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.6200982811 \tAcc: 0.8725000000 \tTPR:0.7558455569 \tFPR:0.0104833298 \tF1:0.8517263370 \tAUC:0.8726811135 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1274386721 \tAcc: 0.9481646825 \tTPR:0.9693856915 \tFPR:0.0947656180 \tF1:0.9613903047 \t AUC:0.9373100368\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1099396277 \tAcc: 0.9577132937 \tTPR:0.9779575513 \tFPR:0.0837116752 \tF1:0.9688105280 \t AUC:0.9471229380\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1053526840 \tAcc: 0.9561011905 \tTPR:0.9794554716 \tFPR:0.0902068811 \tF1:0.9671188542 \t AUC:0.9446242952\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1000471038 \tAcc: 0.9606894841 \tTPR:0.9775000364 \tFPR:0.0721353426 \tF1:0.9705696995 \t AUC:0.9530488130\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1007474982 \tAcc: 0.9629216270 \tTPR:0.9815840921 \tFPR:0.0772282612 \tF1:0.9724634587 \t AUC:0.9521779155\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2059864770 \tAcc: 0.9327083333 \tTPR:0.9227193006 \tFPR:0.0558685063 \tF1:0.9288588847 \tAUC:0.9334253972 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1198221467 \tAcc: 0.9543310892 \tTPR:0.9752685280 \tFPR:0.0837538508 \tF1:0.9642691153 \t AUC:0.9457573386\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1119931108 \tAcc: 0.9574181776 \tTPR:0.9770825653 \tFPR:0.0783283845 \tF1:0.9663357759 \t AUC:0.9493770904\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1072757791 \tAcc: 0.9588659012 \tTPR:0.9777489661 \tFPR:0.0751475452 \tF1:0.9675889040 \t AUC:0.9513007105\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1025929260 \tAcc: 0.9603427876 \tTPR:0.9785807750 \tFPR:0.0729278046 \tF1:0.9689090477 \t AUC:0.9528264852\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1018889100 \tAcc: 0.9611676776 \tTPR:0.9790495001 \tFPR:0.0709563790 \tF1:0.9692228812 \t AUC:0.9540465606\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1832334225 \tAcc: 0.9372916667 \tTPR:0.9461220802 \tFPR:0.0700645494 \tF1:0.9358186491 \tAUC:0.9380287654 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0784222419 \tAcc: 0.9683205409 \tTPR:0.8494221665 \tFPR:0.0159652288 \tF1:0.8366157628 \t AUC:0.9151552676\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0732418283 \tAcc: 0.9698464912 \tTPR:0.8488298292 \tFPR:0.0150751837 \tF1:0.8422698055 \t AUC:0.9146006635\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0665826876 \tAcc: 0.9729532164 \tTPR:0.8665686206 \tFPR:0.0123478836 \tF1:0.8535043619 \t AUC:0.9245794243\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0643646518 \tAcc: 0.9742781433 \tTPR:0.8696005059 \tFPR:0.0116457659 \tF1:0.8569349054 \t AUC:0.9273156073\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0608472324 \tAcc: 0.9756030702 \tTPR:0.8756143832 \tFPR:0.0117197485 \tF1:0.8616316352 \t AUC:0.9297832711\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.5544544529 \tAcc: 0.8418750000 \tTPR:0.6855130300 \tFPR:0.0089058174 \tF1:0.8024494067 \tAUC:0.8383036063 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.3599625403 \tAcc: 0.9025416667 \tTPR:0.8404194331 \tFPR:0.0362842054 \tF1:0.8882953998 \tAUC:0.9020676138\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 15:  =============\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1097894326 \tAcc: 0.9580535095 \tTPR:0.9775624986 \tFPR:0.0780238032 \tF1:0.9669053540 \t AUC:0.9497693477\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1031025972 \tAcc: 0.9611676776 \tTPR:0.9793178706 \tFPR:0.0710681853 \tF1:0.9694255730 \t AUC:0.9541248426\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1002242541 \tAcc: 0.9615467938 \tTPR:0.9795396426 \tFPR:0.0726749609 \tF1:0.9696227541 \t AUC:0.9534323409\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.0969150443 \tAcc: 0.9629695207 \tTPR:0.9805597949 \tFPR:0.0696511707 \tF1:0.9709596044 \t AUC:0.9554543121\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.0946827682 \tAcc: 0.9640922877 \tTPR:0.9806515780 \tFPR:0.0668805073 \tF1:0.9717662071 \t AUC:0.9568855354\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1983926848 \tAcc: 0.9329166667 \tTPR:0.9359169318 \tFPR:0.0697331451 \tF1:0.9306586105 \tAUC:0.9330918933 \ttest cost: 0:00:05\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0433647090 \tAcc: 0.9833333333 \tTPR:0.9955299539 \tFPR:0.1718181818 \tF1:0.9909084503 \t AUC:0.9116527021\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0379422157 \tAcc: 0.9848958333 \tTPR:0.9944678942 \tFPR:0.1233974359 \tF1:0.9917411570 \t AUC:0.9357106441\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0306593102 \tAcc: 0.9901041667 \tTPR:0.9960318785 \tFPR:0.0703030303 \tF1:0.9945699232 \t AUC:0.9626840549\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0238956914 \tAcc: 0.9906250000 \tTPR:0.9971968689 \tFPR:0.1132075472 \tF1:0.9949233390 \t AUC:0.9421043597\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0215406752 \tAcc: 0.9927083333 \tTPR:0.9983512545 \tFPR:0.0636904762 \tF1:0.9959887848 \t AUC:0.9672715054\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.2695311942 \tAcc: 0.9368750000 \tTPR:0.9787961368 \tFPR:0.1053455232 \tF1:0.9377939972 \tAUC:0.9367253068 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1105362513 \tAcc: 0.9618055556 \tTPR:0.9831993214 \tFPR:0.0838653563 \tF1:0.9710523443 \t AUC:0.9496669825\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1048517494 \tAcc: 0.9585813492 \tTPR:0.9788153217 \tFPR:0.0775895236 \tF1:0.9689582081 \t AUC:0.9506128990\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0968772086 \tAcc: 0.9661458333 \tTPR:0.9788060627 \tFPR:0.0628303575 \tF1:0.9747079825 \t AUC:0.9579878526\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0826437959 \tAcc: 0.9706101190 \tTPR:0.9874130092 \tFPR:0.0607530366 \tF1:0.9776992282 \t AUC:0.9633299863\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0867658123 \tAcc: 0.9673859127 \tTPR:0.9807446881 \tFPR:0.0596286533 \tF1:0.9755595371 \t AUC:0.9605580174\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2360212990 \tAcc: 0.9325000000 \tTPR:0.9185891356 \tFPR:0.0558936443 \tF1:0.9289776624 \tAUC:0.9313477456 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1300268360 \tAcc: 0.9464400773 \tTPR:0.9546660539 \tFPR:0.0613151134 \tF1:0.9421145074 \t AUC:0.9466754703\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1195769835 \tAcc: 0.9507893041 \tTPR:0.9588511593 \tFPR:0.0576183237 \tF1:0.9462974466 \t AUC:0.9506164178\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1112335849 \tAcc: 0.9557745290 \tTPR:0.9626179759 \tFPR:0.0503846833 \tF1:0.9523170160 \t AUC:0.9561166463\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1059106046 \tAcc: 0.9576269774 \tTPR:0.9652954932 \tFPR:0.0497513575 \tF1:0.9537564200 \t AUC:0.9577720678\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1037907357 \tAcc: 0.9614679613 \tTPR:0.9681138016 \tFPR:0.0447557308 \tF1:0.9587134353 \t AUC:0.9616790354\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2114875198 \tAcc: 0.9368750000 \tTPR:0.9050327086 \tFPR:0.0319012958 \tF1:0.9318804323 \tAUC:0.9365657064 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0756211172 \tAcc: 0.9696637427 \tTPR:0.8451475912 \tFPR:0.0141699813 \tF1:0.8347065895 \t AUC:0.9136342851\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0693347820 \tAcc: 0.9722130848 \tTPR:0.8533179012 \tFPR:0.0123533323 \tF1:0.8428647448 \t AUC:0.9187256126\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0651294551 \tAcc: 0.9733644006 \tTPR:0.8564824836 \tFPR:0.0118619358 \tF1:0.8487355010 \t AUC:0.9207013331\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0616266792 \tAcc: 0.9752832602 \tTPR:0.8683914648 \tFPR:0.0115027228 \tF1:0.8586681596 \t AUC:0.9264623148\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0571687173 \tAcc: 0.9758771930 \tTPR:0.8628515206 \tFPR:0.0113947370 \tF1:0.8587653753 \t AUC:0.9235563571\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.5494368301 \tAcc: 0.8508333333 \tTPR:0.7053170092 \tFPR:0.0099147961 \tF1:0.8167940731 \tAUC:0.8477011066 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2929739056 \tAcc: 0.9180000000 \tTPR:0.8887303844 \tFPR:0.0545576809 \tF1:0.9092209551 \tAUC:0.9170863517\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 16:  =============\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0864260209 \tAcc: 0.9694088855 \tTPR:0.9885003764 \tFPR:0.1085530532 \tF1:0.9807476764 \t AUC:0.9399562906\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0763139709 \tAcc: 0.9723268072 \tTPR:0.9884673444 \tFPR:0.0981022909 \tF1:0.9825191071 \t AUC:0.9451825267\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0723902473 \tAcc: 0.9732680723 \tTPR:0.9893740941 \tFPR:0.0896849636 \tF1:0.9830805077 \t AUC:0.9498445652\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0684029379 \tAcc: 0.9751506024 \tTPR:0.9903397674 \tFPR:0.0832654046 \tF1:0.9842226410 \t AUC:0.9535552564\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0637110524 \tAcc: 0.9759036145 \tTPR:0.9909419776 \tFPR:0.0906014608 \tF1:0.9847725610 \t AUC:0.9501428098\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1703200850 \tAcc: 0.9402083333 \tTPR:0.9426734713 \tFPR:0.0625071321 \tF1:0.9390613215 \tAUC:0.9400831696 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.0972361824 \tAcc: 0.9628341221 \tTPR:0.9794921575 \tFPR:0.0660718720 \tF1:0.9706007000 \t AUC:0.9567101428\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.0912069138 \tAcc: 0.9645151480 \tTPR:0.9814712712 \tFPR:0.0659396714 \tF1:0.9720403325 \t AUC:0.9577657999\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.0888004915 \tAcc: 0.9657441508 \tTPR:0.9817409357 \tFPR:0.0631817344 \tF1:0.9728569582 \t AUC:0.9592796006\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.0847871992 \tAcc: 0.9683583855 \tTPR:0.9831137620 \tFPR:0.0585400688 \tF1:0.9750297921 \t AUC:0.9622868466\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.0834035271 \tAcc: 0.9678167911 \tTPR:0.9834527669 \tFPR:0.0601912094 \tF1:0.9745795578 \t AUC:0.9616307787\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.2209969094 \tAcc: 0.9350000000 \tTPR:0.9202663457 \tFPR:0.0485727959 \tF1:0.9315888749 \tAUC:0.9358467749 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1208161242 \tAcc: 0.9533132530 \tTPR:0.9601381248 \tFPR:0.0513720742 \tF1:0.9477295488 \t AUC:0.9543830253\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.0999401531 \tAcc: 0.9604668675 \tTPR:0.9661030434 \tFPR:0.0470009930 \tF1:0.9547239389 \t AUC:0.9595510252\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0984079966 \tAcc: 0.9631024096 \tTPR:0.9680484472 \tFPR:0.0405249865 \tF1:0.9581882210 \t AUC:0.9637617304\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0877842601 \tAcc: 0.9668674699 \tTPR:0.9753376105 \tFPR:0.0431779044 \tF1:0.9622471085 \t AUC:0.9660798531\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0901778542 \tAcc: 0.9631024096 \tTPR:0.9688707006 \tFPR:0.0408152878 \tF1:0.9572153315 \t AUC:0.9640277064\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2104270854 \tAcc: 0.9387500000 \tTPR:0.9071891347 \tFPR:0.0317466917 \tF1:0.9338149733 \tAUC:0.9377212215 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0715557753 \tAcc: 0.9706038136 \tTPR:0.9053547990 \tFPR:0.0178325101 \tF1:0.8921704356 \t AUC:0.9433566778\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0671461737 \tAcc: 0.9729343220 \tTPR:0.9148439602 \tFPR:0.0156224202 \tF1:0.9024210785 \t AUC:0.9494901524\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0597857372 \tAcc: 0.9759887006 \tTPR:0.9100505662 \tFPR:0.0127222616 \tF1:0.9025734808 \t AUC:0.9481501555\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0544325743 \tAcc: 0.9776129944 \tTPR:0.9220460252 \tFPR:0.0124788880 \tF1:0.9098040104 \t AUC:0.9544504320\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0479230905 \tAcc: 0.9803495763 \tTPR:0.9382206855 \tFPR:0.0115800971 \tF1:0.9297562139 \t AUC:0.9633202942\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.3791530338 \tAcc: 0.9079166667 \tTPR:0.8331186014 \tFPR:0.0191276313 \tF1:0.8973320299 \tAUC:0.9069954851 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1291480490 \tAcc: 0.9478731559 \tTPR:0.9535463092 \tFPR:0.0566180774 \tF1:0.9430268134 \t AUC:0.9484641159\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1186576256 \tAcc: 0.9545580786 \tTPR:0.9595689888 \tFPR:0.0515242015 \tF1:0.9510663386 \t AUC:0.9540223936\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1112173751 \tAcc: 0.9560716984 \tTPR:0.9623807482 \tFPR:0.0496707301 \tF1:0.9522230456 \t AUC:0.9563550090\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1032722061 \tAcc: 0.9592211385 \tTPR:0.9663422068 \tFPR:0.0475487214 \tF1:0.9560509041 \t AUC:0.9593967427\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1023155288 \tAcc: 0.9589156372 \tTPR:0.9645979856 \tFPR:0.0472295708 \tF1:0.9564025495 \t AUC:0.9586842074\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.1981768612 \tAcc: 0.9431250000 \tTPR:0.9174303777 \tFPR:0.0299804736 \tF1:0.9391878475 \tAUC:0.9437249520 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2358147950 \tAcc: 0.9330000000 \tTPR:0.9041355862 \tFPR:0.0383869449 \tF1:0.9281970094 \tAUC:0.9328743206\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 17:  =============\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0602121031 \tAcc: 0.9756709040 \tTPR:0.9175958536 \tFPR:0.0139746903 \tF1:0.9061145968 \t AUC:0.9515764790\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0531277123 \tAcc: 0.9803142655 \tTPR:0.9304737243 \tFPR:0.0101291694 \tF1:0.9285156113 \t AUC:0.9600737983\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0493258886 \tAcc: 0.9799611582 \tTPR:0.9385638059 \tFPR:0.0120370750 \tF1:0.9242406795 \t AUC:0.9630888308\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0508192110 \tAcc: 0.9805790960 \tTPR:0.9344374944 \tFPR:0.0114481843 \tF1:0.9191918575 \t AUC:0.9613083979\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0412572073 \tAcc: 0.9830508475 \tTPR:0.9440644337 \tFPR:0.0101026704 \tF1:0.9331410479 \t AUC:0.9669016528\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.4073995850 \tAcc: 0.9081250000 \tTPR:0.8311986743 \tFPR:0.0171634363 \tF1:0.8954310772 \tAUC:0.9070176190 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1077335727 \tAcc: 0.9601516272 \tTPR:0.9818837140 \tFPR:0.1127491615 \tF1:0.9737297447 \t AUC:0.9345403975\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0967016939 \tAcc: 0.9640347633 \tTPR:0.9849988338 \tFPR:0.1099977339 \tF1:0.9762784020 \t AUC:0.9374782930\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0901417293 \tAcc: 0.9678254438 \tTPR:0.9863357849 \tFPR:0.0968513353 \tF1:0.9789337248 \t AUC:0.9447219515\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0867094071 \tAcc: 0.9671782544 \tTPR:0.9854972546 \tFPR:0.0964776647 \tF1:0.9782580193 \t AUC:0.9445097949\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0813605104 \tAcc: 0.9693971893 \tTPR:0.9879655063 \tFPR:0.0929524811 \tF1:0.9799146949 \t AUC:0.9474886573\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1927627186 \tAcc: 0.9416666667 \tTPR:0.9660885647 \tFPR:0.0813190799 \tF1:0.9412559068 \tAUC:0.9423847424 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0761439456 \tAcc: 0.9691611842 \tTPR:0.8288655667 \tFPR:0.0145852289 \tF1:0.8214715222 \t AUC:0.9052216215\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0680425226 \tAcc: 0.9730902778 \tTPR:0.8589297317 \tFPR:0.0128251799 \tF1:0.8550887215 \t AUC:0.9217927200\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0633148286 \tAcc: 0.9744608918 \tTPR:0.8667652929 \tFPR:0.0128976394 \tF1:0.8563090136 \t AUC:0.9256431775\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0595642127 \tAcc: 0.9760599415 \tTPR:0.8814339088 \tFPR:0.0116517498 \tF1:0.8694151730 \t AUC:0.9336523293\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0552101658 \tAcc: 0.9779239766 \tTPR:0.8961187691 \tFPR:0.0107824467 \tF1:0.8852571217 \t AUC:0.9411841436\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.4721869696 \tAcc: 0.8647916667 \tTPR:0.7342428194 \tFPR:0.0111633086 \tF1:0.8350506605 \tAUC:0.8615397554 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1223570872 \tAcc: 0.9539930556 \tTPR:0.9709541936 \tFPR:0.0877271208 \tF1:0.9647993160 \t AUC:0.9416135364\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1012138464 \tAcc: 0.9601934524 \tTPR:0.9763757796 \tFPR:0.0729382390 \tF1:0.9701139586 \t AUC:0.9517187703\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0903716659 \tAcc: 0.9677579365 \tTPR:0.9820541626 \tFPR:0.0603356715 \tF1:0.9753337072 \t AUC:0.9608592455\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0896140999 \tAcc: 0.9672619048 \tTPR:0.9816328467 \tFPR:0.0647308710 \tF1:0.9753208961 \t AUC:0.9584509879\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0812594177 \tAcc: 0.9697420635 \tTPR:0.9854148270 \tFPR:0.0583696396 \tF1:0.9776950969 \t AUC:0.9635225937\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2226462439 \tAcc: 0.9312500000 \tTPR:0.9232175675 \tFPR:0.0607093238 \tF1:0.9272971785 \tAUC:0.9312541219 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0759888320 \tAcc: 0.9732680723 \tTPR:0.9902646850 \tFPR:0.0978879805 \tF1:0.9831957420 \t AUC:0.9461883523\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0676634236 \tAcc: 0.9747740964 \tTPR:0.9911236393 \tFPR:0.0947752047 \tF1:0.9842424844 \t AUC:0.9481742173\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0605372650 \tAcc: 0.9781626506 \tTPR:0.9910836468 \tFPR:0.0747904160 \tF1:0.9861655461 \t AUC:0.9581331466\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0560448776 \tAcc: 0.9782567771 \tTPR:0.9915661216 \tFPR:0.0766310058 \tF1:0.9863703973 \t AUC:0.9574548179\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0559722139 \tAcc: 0.9802334337 \tTPR:0.9924140627 \tFPR:0.0660242702 \tF1:0.9874713421 \t AUC:0.9631948962\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1765613572 \tAcc: 0.9439583333 \tTPR:0.9589679143 \tFPR:0.0712494528 \tF1:0.9439027040 \tAUC:0.9438592308 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2943113748 \tAcc: 0.9179583333 \tTPR:0.8827431080 \tFPR:0.0483209203 \tF1:0.9085875054 \tAUC:0.9172110939\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 18:  =============\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0687888649 \tAcc: 0.9738328313 \tTPR:0.9892191620 \tFPR:0.0862593481 \tF1:0.9833753233 \t AUC:0.9514799070\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0607462645 \tAcc: 0.9771272590 \tTPR:0.9908865759 \tFPR:0.0767711225 \tF1:0.9855322621 \t AUC:0.9570911656\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0573913939 \tAcc: 0.9787274096 \tTPR:0.9921006809 \tFPR:0.0764844677 \tF1:0.9866076751 \t AUC:0.9578081066\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0564532149 \tAcc: 0.9785391566 \tTPR:0.9908481273 \tFPR:0.0746291443 \tF1:0.9864334257 \t AUC:0.9581094915\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0476253076 \tAcc: 0.9803275602 \tTPR:0.9911315712 \tFPR:0.0632398987 \tF1:0.9875309946 \t AUC:0.9639324398\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.2130806992 \tAcc: 0.9414583333 \tTPR:0.9712075069 \tFPR:0.0872540147 \tF1:0.9420618005 \tAUC:0.9419767461 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1248099138 \tAcc: 0.9505476804 \tTPR:0.9577861392 \tFPR:0.0554978617 \tF1:0.9471939599 \t AUC:0.9511441387\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1175774695 \tAcc: 0.9540914948 \tTPR:0.9577720628 \tFPR:0.0491199433 \tF1:0.9495772115 \t AUC:0.9543260598\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1046399564 \tAcc: 0.9583351849 \tTPR:0.9654034644 \tFPR:0.0475712215 \tF1:0.9544837166 \t AUC:0.9589161215\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1012122399 \tAcc: 0.9602848383 \tTPR:0.9685675206 \tFPR:0.0465638125 \tF1:0.9576118882 \t AUC:0.9610018540\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.0962949179 \tAcc: 0.9614929568 \tTPR:0.9675493808 \tFPR:0.0445239140 \tF1:0.9587094011 \t AUC:0.9615127334\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2059155194 \tAcc: 0.9406250000 \tTPR:0.9227228019 \tFPR:0.0409425714 \tF1:0.9369598978 \tAUC:0.9408901153 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0653894976 \tAcc: 0.9737207602 \tTPR:0.8649111204 \tFPR:0.0122414318 \tF1:0.8528181981 \t AUC:0.9245093189\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0597362216 \tAcc: 0.9752284357 \tTPR:0.8686490532 \tFPR:0.0118797206 \tF1:0.8528355175 \t AUC:0.9268116010\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0549267739 \tAcc: 0.9785270468 \tTPR:0.8965155945 \tFPR:0.0107911340 \tF1:0.8857112311 \t AUC:0.9417020912\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0504506899 \tAcc: 0.9796235380 \tTPR:0.9012629955 \tFPR:0.0100830221 \tF1:0.8850265220 \t AUC:0.9442557029\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0488461672 \tAcc: 0.9802631579 \tTPR:0.9039172004 \tFPR:0.0096889283 \tF1:0.8956952585 \t AUC:0.9456671059\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.5104371475 \tAcc: 0.8683333333 \tTPR:0.7421958742 \tFPR:0.0115611716 \tF1:0.8409928679 \tAUC:0.8653173513 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1115111180 \tAcc: 0.9585843373 \tTPR:0.9594402077 \tFPR:0.0419206095 \tF1:0.9519186468 \t AUC:0.9587597991\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.0985799621 \tAcc: 0.9615963855 \tTPR:0.9657363431 \tFPR:0.0417582595 \tF1:0.9572612143 \t AUC:0.9619890418\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0913521674 \tAcc: 0.9664909639 \tTPR:0.9693017437 \tFPR:0.0333251270 \tF1:0.9602799240 \t AUC:0.9679883084\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0849874696 \tAcc: 0.9672439759 \tTPR:0.9736689338 \tFPR:0.0364726447 \tF1:0.9619297957 \t AUC:0.9685981445\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0792611842 \tAcc: 0.9710090361 \tTPR:0.9702749189 \tFPR:0.0283345409 \tF1:0.9661705196 \t AUC:0.9709701890\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2142165571 \tAcc: 0.9366666667 \tTPR:0.9038691300 \tFPR:0.0316291134 \tF1:0.9308599336 \tAUC:0.9361200083 \ttest cost: 0:00:05\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0483788504 \tAcc: 0.9838541667 \tTPR:0.9933234293 \tFPR:0.1256410256 \tF1:0.9910299338 \t AUC:0.9336281002\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0291839260 \tAcc: 0.9878720238 \tTPR:0.9966450377 \tFPR:0.1233918129 \tF1:0.9933597892 \t AUC:0.9365383239\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0320864661 \tAcc: 0.9873511905 \tTPR:0.9953953050 \tFPR:0.1055555556 \tF1:0.9929404518 \t AUC:0.9447986985\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0228378166 \tAcc: 0.9932291667 \tTPR:0.9989068100 \tFPR:0.0839181287 \tF1:0.9963319386 \t AUC:0.9574655725\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0203991919 \tAcc: 0.9927083333 \tTPR:0.9971566536 \tFPR:0.0909090909 \tF1:0.9960837768 \t AUC:0.9529945383\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.3360677620 \tAcc: 0.9254166667 \tTPR:0.9876102601 \tFPR:0.1392380875 \tF1:0.9269240169 \tAUC:0.9241860863 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2959435370 \tAcc: 0.9225000000 \tTPR:0.9055211146 \tFPR:0.0621249917 \tF1:0.9155597033 \tAUC:0.9216980615\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 19:  =============\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0534342419 \tAcc: 0.9781158626 \tTPR:0.8940888100 \tFPR:0.0107106571 \tF1:0.8834025638 \t AUC:0.9405017313\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0493670508 \tAcc: 0.9805829678 \tTPR:0.9032622064 \tFPR:0.0094768431 \tF1:0.8961587880 \t AUC:0.9463202687\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0468316757 \tAcc: 0.9812225877 \tTPR:0.9031403741 \tFPR:0.0096294268 \tF1:0.8930454404 \t AUC:0.9455954781\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0450633577 \tAcc: 0.9817708333 \tTPR:0.9042270027 \tFPR:0.0085204635 \tF1:0.8952113719 \t AUC:0.9461870223\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0410570154 \tAcc: 0.9838724415 \tTPR:0.9186171447 \tFPR:0.0079460896 \tF1:0.9081654690 \t AUC:0.9542357592\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.4827147868 \tAcc: 0.8706250000 \tTPR:0.7499734866 \tFPR:0.0116484142 \tF1:0.8464804102 \tAUC:0.8691625362 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1074781848 \tAcc: 0.9572172619 \tTPR:0.9755879033 \tFPR:0.0824772285 \tF1:0.9679202250 \t AUC:0.9465553374\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0970477341 \tAcc: 0.9636656746 \tTPR:0.9813800205 \tFPR:0.0705860608 \tF1:0.9729574207 \t AUC:0.9553969799\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0928356546 \tAcc: 0.9614335317 \tTPR:0.9761673933 \tFPR:0.0653019765 \tF1:0.9711409039 \t AUC:0.9554327084\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0772750100 \tAcc: 0.9728422619 \tTPR:0.9874852158 \tFPR:0.0538037291 \tF1:0.9787283295 \t AUC:0.9668407433\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0750524062 \tAcc: 0.9728422619 \tTPR:0.9849234242 \tFPR:0.0526491035 \tF1:0.9798391699 \t AUC:0.9661371603\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2188485046 \tAcc: 0.9366666667 \tTPR:0.9274903020 \tFPR:0.0546975422 \tF1:0.9340169518 \tAUC:0.9363963799 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0963164519 \tAcc: 0.9632026627 \tTPR:0.9841283347 \tFPR:0.1102751456 \tF1:0.9760207860 \t AUC:0.9369265945\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0854448740 \tAcc: 0.9681952663 \tTPR:0.9868909324 \tFPR:0.0923718467 \tF1:0.9789062561 \t AUC:0.9472400931\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0783612546 \tAcc: 0.9718934911 \tTPR:0.9883886798 \tFPR:0.0805424052 \tF1:0.9814985348 \t AUC:0.9539059098\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0755634371 \tAcc: 0.9710613905 \tTPR:0.9873934879 \tFPR:0.0817764418 \tF1:0.9809440249 \t AUC:0.9528085231\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0683124099 \tAcc: 0.9744822485 \tTPR:0.9896119399 \tFPR:0.0730257579 \tF1:0.9833539918 \t AUC:0.9582930910\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1807135282 \tAcc: 0.9437500000 \tTPR:0.9669199069 \tFPR:0.0796892556 \tF1:0.9428723105 \tAUC:0.9436153257 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1149978967 \tAcc: 0.9567018072 \tTPR:0.9637231174 \tFPR:0.0480350579 \tF1:0.9492050742 \t AUC:0.9578440297\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.0948840665 \tAcc: 0.9646084337 \tTPR:0.9654004320 \tFPR:0.0364426348 \tF1:0.9583325332 \t AUC:0.9644788986\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0825922633 \tAcc: 0.9702560241 \tTPR:0.9774197597 \tFPR:0.0360151941 \tF1:0.9657390580 \t AUC:0.9707022828\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0788546203 \tAcc: 0.9691265060 \tTPR:0.9711717028 \tFPR:0.0335077043 \tF1:0.9646660298 \t AUC:0.9688319993\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0679297965 \tAcc: 0.9741884203 \tTPR:0.9808638850 \tFPR:0.0303873910 \tF1:0.9715120274 \t AUC:0.9752382470\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.2277603210 \tAcc: 0.9375000000 \tTPR:0.9061057338 \tFPR:0.0287762824 \tF1:0.9333878232 \tAUC:0.9386647257 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1591493656 \tAcc: 0.9464285714 \tTPR:0.9733725629 \tFPR:0.1566751701 \tF1:0.9661815400 \t AUC:0.9083486964\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.1305201827 \tAcc: 0.9510241597 \tTPR:0.9761376026 \tFPR:0.1282029478 \tF1:0.9684596593 \t AUC:0.9239673274\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.1269736706 \tAcc: 0.9511554622 \tTPR:0.9886468657 \tFPR:0.1858856421 \tF1:0.9689096678 \t AUC:0.9013806118\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.1115447743 \tAcc: 0.9621848739 \tTPR:0.9823763472 \tFPR:0.1231009070 \tF1:0.9763228037 \t AUC:0.9296377201\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.1051004605 \tAcc: 0.9633009454 \tTPR:0.9870363636 \tFPR:0.1228248089 \tF1:0.9762902732 \t AUC:0.9324444137\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.1512555942 \tAcc: 0.9460416667 \tTPR:0.9585042677 \tFPR:0.0673961945 \tF1:0.9461545649 \tAUC:0.9455540366 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2522585470 \tAcc: 0.9269166667 \tTPR:0.9017987394 \tFPR:0.0484415378 \tF1:0.9205824121 \tAUC:0.9266786008\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 20:  =============\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0636383763 \tAcc: 0.9736935028 \tTPR:0.9126556118 \tFPR:0.0150502545 \tF1:0.9024875496 \t AUC:0.9485545412\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0510756894 \tAcc: 0.9796080508 \tTPR:0.9289827247 \tFPR:0.0111690017 \tF1:0.9173793829 \t AUC:0.9586033689\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0458478088 \tAcc: 0.9817620056 \tTPR:0.9464072729 \tFPR:0.0113176612 \tF1:0.9327007830 \t AUC:0.9674688954\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0425027281 \tAcc: 0.9824329096 \tTPR:0.9402451676 \tFPR:0.0101893527 \tF1:0.9286512535 \t AUC:0.9647725449\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0366923290 \tAcc: 0.9861405367 \tTPR:0.9541206048 \tFPR:0.0078919918 \tF1:0.9471267338 \t AUC:0.9729182407\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.4068103284 \tAcc: 0.9072916667 \tTPR:0.8286092369 \tFPR:0.0159401760 \tF1:0.8949381505 \tAUC:0.9063345305 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.2089281771 \tAcc: 0.9386160714 \tTPR:0.9594085349 \tFPR:0.1467558235 \tF1:0.9604015014 \t AUC:0.9063263557\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.1238128226 \tAcc: 0.9531250000 \tTPR:0.9800060283 \tFPR:0.1389623274 \tF1:0.9703094448 \t AUC:0.9205218504\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.1076476218 \tAcc: 0.9566045168 \tTPR:0.9851997176 \tFPR:0.1334565236 \tF1:0.9723445113 \t AUC:0.9258715970\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.1028038311 \tAcc: 0.9644170168 \tTPR:0.9888004357 \tFPR:0.1254045558 \tF1:0.9774699247 \t AUC:0.9316979400\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0850587676 \tAcc: 0.9709821429 \tTPR:0.9862036773 \tFPR:0.0968421459 \tF1:0.9818115734 \t AUC:0.9446807657\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.1609727834 \tAcc: 0.9414583333 \tTPR:0.9565237393 \tFPR:0.0743663426 \tF1:0.9419846982 \tAUC:0.9410786983 \ttest cost: 0:00:05\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0434996995 \tAcc: 0.9859375000 \tTPR:0.9961355124 \tFPR:0.1320754717 \tF1:0.9923159701 \t AUC:0.9323644410\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0358543159 \tAcc: 0.9852678571 \tTPR:0.9945871590 \tFPR:0.1324561404 \tF1:0.9918401455 \t AUC:0.9311971890\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0286122289 \tAcc: 0.9890625000 \tTPR:0.9966821159 \tFPR:0.0964912281 \tF1:0.9940931532 \t AUC:0.9500081312\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0271446312 \tAcc: 0.9916666667 \tTPR:0.9960802647 \tFPR:0.0506802721 \tF1:0.9953833070 \t AUC:0.9725789036\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0207133743 \tAcc: 0.9916666667 \tTPR:0.9977753059 \tFPR:0.0891734052 \tF1:0.9953754915 \t AUC:0.9541540366\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.3013157748 \tAcc: 0.9291666667 \tTPR:0.9875609977 \tFPR:0.1292036421 \tF1:0.9323188001 \tAUC:0.9291786778 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0665032221 \tAcc: 0.9752447289 \tTPR:0.9899673520 \tFPR:0.0863981149 \tF1:0.9844358632 \t AUC:0.9517846186\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0563519422 \tAcc: 0.9799510542 \tTPR:0.9926505763 \tFPR:0.0731506111 \tF1:0.9874431648 \t AUC:0.9597499826\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0564227643 \tAcc: 0.9783509036 \tTPR:0.9912109807 \tFPR:0.0769030668 \tF1:0.9864507558 \t AUC:0.9571539570\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0444708359 \tAcc: 0.9834337349 \tTPR:0.9935779656 \tFPR:0.0610382673 \tF1:0.9895850020 \t AUC:0.9662698491\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0428331452 \tAcc: 0.9829631024 \tTPR:0.9926828503 \tFPR:0.0562835201 \tF1:0.9892996987 \t AUC:0.9681886120\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.2010543581 \tAcc: 0.9427083333 \tTPR:0.9463470579 \tFPR:0.0621229322 \tF1:0.9400946573 \tAUC:0.9421120629 \ttest cost: 0:00:05\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and test Begin!\")\n",
    "net_glob.train()\n",
    "w_net_glob = net_glob.state_dict()\n",
    "t0 = time.time()\n",
    "\n",
    "lr = 2e-6\n",
    "\n",
    "local_test[\"loss\"] = []\n",
    "local_test[\"acc\"] = []\n",
    "local_test[\"tpr\"] = []\n",
    "local_test[\"fpr\"] = []\n",
    "local_test[\"f1\"] = []\n",
    "local_test[\"auc\"] = []\n",
    "\n",
    "local_testing[\"loss\"] = []\n",
    "local_testing[\"acc\"] = []\n",
    "local_testing[\"tpr\"] = []\n",
    "local_testing[\"fpr\"] = []\n",
    "local_testing[\"f1\"] = []\n",
    "local_testing[\"auc\"] = []\n",
    "\n",
    "for iter in range(epochs):\n",
    "    print(\"============== Round {}:  =============\".format(iter))\n",
    "    idx_collect = []\n",
    "    m = max(int(frac * num_users) ,1)\n",
    "    idxs_users = np.random.choice(range(num_users), m, replace = False)\n",
    "    w_locals_client = []\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local = Client(device, idx, lr, local_epochs, batch_size, train_dataset, test_dataset, dict_user_train[idx], dict_user_test[idx])\n",
    "        w_client, client_loss, client_acc = local.train(net = copy.deepcopy(net_glob).to(device))\n",
    "        w_locals_client.append(copy.deepcopy(w_client))\n",
    "        loss, acc, tpr, fpr, f1, auc = local.evaluate(net = copy.deepcopy(net_glob).to(device), ell=iter)\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        acc_list.append(acc)\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "    local_testing[\"loss\"].append(sum(loss_list)/len(loss_list))\n",
    "    local_testing[\"acc\"].append(sum(acc_list)/len(acc_list))\n",
    "    local_testing[\"tpr\"].append(sum(tpr_list)/len(tpr_list))\n",
    "    local_testing[\"fpr\"].append(sum(fpr_list)/len(fpr_list))\n",
    "    local_testing[\"f1\"].append(sum(f1_list)/len(f1_list))\n",
    "    local_testing[\"auc\"].append(sum(auc_list)/len(auc_list))\n",
    "    print(\"Test =>                 \\tLoss: {:.10f} \\tAcc: {:.10f} \\tTPR:{:.10f} \\tFPR:{:.10f} \\tF1:{:.10f} \\tAUC:{:.10f}\".format(sum(loss_list)/len(loss_list), sum(acc_list)/len(acc_list), sum(tpr_list)/len(tpr_list), sum(fpr_list)/len(fpr_list), sum(f1_list)/len(f1_list), sum(auc_list)/len(auc_list)  ))\n",
    "\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"-------------- FedServer: Federation process  -------------\")\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    w_net_glob = FedAvg(w_locals_client)\n",
    "    net_glob.load_state_dict(w_net_glob)\n",
    "elapsed = format_time(time.time()-t0)\n",
    "print(\"Training and Testing completed! total time cost: {:}\".format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_collect = [i for i in range(num_users)]\n",
    "print(\"============= Final Result =============\")\n",
    "for idx in idx_collect:\n",
    "    loss_test_collect[idx] = []\n",
    "    acc_test_collect[idx] = []\n",
    "    TPR_test_collect[idx] = []\n",
    "    FPR_test_collect[idx] = []\n",
    "    f1_test_collect[idx] = []\n",
    "    AUC_test_collect[idx] = []\n",
    "    local = Client(device, idx, lr, local_epochs, batch_size, train_dataset, test_dataset, dict_user_train[idx], dict_user_test[idx])\n",
    "    loss, acc, tpr, fpr, f1, auc = local.evaluate(net = copy.deepcopy(net_glob).to(device), ell=0)\n",
    "    loss_test_collect[idx].append(loss)\n",
    "    acc_test_collect[idx].append(acc)\n",
    "    TPR_test_collect[idx].append(tpr)\n",
    "    FPR_test_collect[idx].append(fpr)\n",
    "    f1_test_collect[idx].append(f1)\n",
    "    AUC_test_collect[idx].append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "f = xlwt.Workbook('encoding = utf-8')\n",
    "sheet1 = f.add_sheet('sheet1',cell_overwrite_ok=True)\n",
    "for i in range(len(loss_collect)):\n",
    "    sheet1.write(i+1,0,loss_collect[i]) #写入数据参数对应 行, 列, 值\n",
    "for i in range(len(acc_collect)):\n",
    "    sheet1.write(i+1,1,acc_collect[i])\n",
    "for i in range(len(TPR_collect)):\n",
    "    sheet1.write(i+1,2,TPR_collect[i])\n",
    "for i in range(len(FPR_collect)):\n",
    "    sheet1.write(i+1,3,FPR_collect[i])\n",
    "for i in range(len(F1_collect)):\n",
    "    sheet1.write(i+1,4,F1_collect[i])\n",
    "for i in range(len(AUC_collect)):\n",
    "    sheet1.write(i+1,5,AUC_collect[i])\n",
    "\n",
    "f.save('result.xls')#保存.xls到当前工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(num_users):\n",
    "    f = xlwt.Workbook('encoding = utf-8')\n",
    "    sheet1 = f.add_sheet('sheet1', cell_overwrite_ok=True)\n",
    "    for j in range(len(loss_train_collect[i])):\n",
    "        sheet1.write(j+1, 0, loss_train_collect[i][j])\n",
    "    for j in range(len(acc_train_collect[i])):\n",
    "        sheet1.write(j+1, 1, acc_train_collect[i][j])\n",
    "    for j in range(len(TPR_train_collect[i])):\n",
    "        sheet1.write(j+1, 2, TPR_train_collect[i][j])\n",
    "    for j in range(len(FPR_train_collect[i])):\n",
    "        sheet1.write(j+1, 3, FPR_train_collect[i][j])\n",
    "    for j in range(len(f1_train_collect[i])):\n",
    "        sheet1.write(j+1, 4, f1_train_collect[i][j])\n",
    "    for j in range(len(AUC_train_collect[i])):\n",
    "        sheet1.write(j+1, 5, AUC_train_collect[i][j])\n",
    "\n",
    "    f.save('result_client{:}.xls'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(num_users):\n",
    "    f = xlwt.Workbook('encoding = utf-8')\n",
    "    sheet1 = f.add_sheet('sheet1', cell_overwrite_ok=True)\n",
    "    for j in range(len(loss_test_collect[i])):\n",
    "        sheet1.write(j+1, 0, loss_test_collect[i][j])\n",
    "    for j in range(len(acc_test_collect[i])):\n",
    "        sheet1.write(j+1, 1, acc_test_collect[i][j])\n",
    "    for j in range(len(TPR_test_collect[i])):\n",
    "        sheet1.write(j+1, 2, TPR_test_collect[i][j])\n",
    "    for j in range(len(FPR_test_collect[i])):\n",
    "        sheet1.write(j+1, 3, FPR_test_collect[i][j])\n",
    "    for j in range(len(f1_test_collect[i])):\n",
    "        sheet1.write(j+1, 4, f1_test_collect[i][j])\n",
    "    for j in range(len(AUC_test_collect[i])):\n",
    "        sheet1.write(j+1, 5, AUC_test_collect[i][j])\n",
    "\n",
    "    f.save('result_test_client{:}.xls'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "f = xlwt.Workbook('encoding = utf-8')\n",
    "sheet1 = f.add_sheet('sheet1',cell_overwrite_ok=True)\n",
    "for i in range(len(local_test[\"loss\"])):\n",
    "    sheet1.write(i+1,0,local_test[\"loss\"][i])\n",
    "for i in range(len(local_test[\"acc\"])):\n",
    "    sheet1.write(i+1,1,local_test[\"acc\"][i])\n",
    "for i in range(len(local_test[\"tpr\"])):\n",
    "    sheet1.write(i+1,2,local_test[\"tpr\"][i])\n",
    "for i in range(len(local_test[\"fpr\"])):\n",
    "    sheet1.write(i+1,3,local_test[\"fpr\"][i])\n",
    "for i in range(len(local_test[\"f1\"])):\n",
    "    sheet1.write(i+1,4,local_test[\"f1\"][i])\n",
    "for i in range(len(local_test[\"auc\"])):\n",
    "    sheet1.write(i+1,5,local_test[\"auc\"][i])\n",
    "\n",
    "f.save('Local_Test.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "f = xlwt.Workbook('encoding = utf-8')\n",
    "sheet1 = f.add_sheet('sheet1',cell_overwrite_ok=True)\n",
    "for i in range(len(local_testing[\"loss\"])):\n",
    "    sheet1.write(i+1,0,local_testing[\"loss\"][i])\n",
    "for i in range(len(local_testing[\"acc\"])):\n",
    "    sheet1.write(i+1,1,local_testing[\"acc\"][i])\n",
    "for i in range(len(local_testing[\"tpr\"])):\n",
    "    sheet1.write(i+1,2,local_testing[\"tpr\"][i])\n",
    "for i in range(len(local_testing[\"fpr\"])):\n",
    "    sheet1.write(i+1,3,local_testing[\"fpr\"][i])\n",
    "for i in range(len(local_testing[\"f1\"])):\n",
    "    sheet1.write(i+1,4,local_testing[\"f1\"][i])\n",
    "for i in range(len(local_testing[\"auc\"])):\n",
    "    sheet1.write(i+1,5,local_testing[\"auc\"][i])\n",
    "\n",
    "f.save('Local_Testing.xls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
