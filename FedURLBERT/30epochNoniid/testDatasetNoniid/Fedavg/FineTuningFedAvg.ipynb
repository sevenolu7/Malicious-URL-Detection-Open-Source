{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "df_train = pd.read_csv(\"./fine_tuning.csv\")\n",
    "train_data_domain = df_train.domain.values\n",
    "train_data_label = df_train.label.values\n",
    "train_data_label = train_data_label.tolist()\n",
    "train_data_label = [0 if item == 2 else 1 for item in train_data_label]\n",
    "train_data_label = np.array(train_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=\"./bert_tokenizer/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "\n",
    "for sent in train_data_domain:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,                      # Sentence to encode.\n",
    "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "        max_length = 64,           # Pad & truncate all sentences.\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,   # Construct attn. masks.\n",
    "        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "    )\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_train.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_train.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "labels_train = torch.tensor(train_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train,  attention_masks_train, labels_train) # 打包处理，所以数据第一维必须相等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "构造MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "EmbeddingPath = \"./FedBert/FedTransformer.pt\"\n",
    "TransformerPath = \"./FedBert/FedEmbedding.pt\"\n",
    "num_users = 10\n",
    "frac = 0.5\n",
    "local_epochs = 5\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"./bert-base-uncased-model/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1000\n",
      "}\n",
      "\n",
      "BertPooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,DataCollatorForLanguageModeling,HfArgumentParser,Trainer,TrainingArguments,set_seed,\n",
    ")\n",
    "# 自己修改部分配置参数\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": None,\n",
    "    \"revision\": 'main',\n",
    "    \"use_auth_token\": None,\n",
    "    #      \"hidden_size\": 512,\n",
    "    #     \"num_attention_heads\": 4,\n",
    "    \"hidden_dropout_prob\": 0.2,\n",
    "    \"vocab_size\": 1000 # 自己设置词汇大小\n",
    "}\n",
    "# 将模型的配置参数载入\n",
    "config = AutoConfig.from_pretrained('./bert-base-uncased-model/', **config_kwargs)\n",
    "print(config)\n",
    "# 载入预训练模型\n",
    "model = AutoModelForMaskedLM.from_config(\n",
    "    config=config,\n",
    ")\n",
    "model.resize_token_embeddings(config_kwargs[\"vocab_size\"])\n",
    "\n",
    "embedding = model.bert.embeddings\n",
    "\n",
    "class Bert_Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_Embedding, self).__init__()\n",
    "        self.embeddings = copy.deepcopy(embedding)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        embedding_output = self.embeddings(input_ids, attn_mask)\n",
    "        return embedding_output\n",
    "\n",
    "embedding_model = Bert_Embedding()\n",
    "embedding_model.load_state_dict(torch.load(EmbeddingPath))\n",
    "\n",
    "encoder = model.bert.encoder\n",
    "cls = model.cls\n",
    "\n",
    "class Bert_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_Encoder, self).__init__()\n",
    "        self.encoder = copy.deepcopy(encoder)\n",
    "        self.cls = copy.deepcopy(cls)\n",
    "\n",
    "    def forward(self, embedding_output):\n",
    "        output_encoder = self.encoder(embedding_output).last_hidden_state\n",
    "        return output_encoder\n",
    "encoder_model = Bert_Encoder()\n",
    "encoder_model.load_state_dict(torch.load(TransformerPath))\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "class Pooler_Config:\n",
    "    def __init__(self, entries: dict={}):\n",
    "        for k, v in entries.items():\n",
    "            if isinstance(v, dict):\n",
    "                self.__dict__[k] = Pooler_Config(v)\n",
    "            else:\n",
    "                self.__dict__[k] = v\n",
    "\n",
    "config_pooler = {\"hidden_size\": 768}\n",
    "config_pooler = Pooler_Config(config_pooler)\n",
    "pooler = BertPooler(config_pooler)\n",
    "print(pooler)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_classes=2, freeze_bert=False):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = Bert_Embedding()\n",
    "        self.encoder = Bert_Encoder()\n",
    "        self.pooler = copy.deepcopy(pooler)\n",
    "        if freeze_bert:\n",
    "            for p in self.embedding.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_size, num_classes, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        embedding_outputs = self.embedding(input_ids, attn_mask)\n",
    "        encoder_outputs = self.encoder(embedding_outputs)\n",
    "        pooler_outputs = self.pooler(encoder_outputs)\n",
    "        #它代表了一句话的embedding\n",
    "        logits = self.fc(pooler_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "iid数据分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def dataset_iid(dataset, num_users):\n",
    "\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "# train_dataset[:][:][:][2]\n",
    "def dirichlet_split_noniid(train_labels, num_users):\n",
    "    '''\n",
    "    按照参数为alpha的Dirichlet分布将样本索引集合划分为n_clients个子集\n",
    "    '''\n",
    "    alpha = 0.7\n",
    "    n_classes = 2\n",
    "    # (K, N) 类别标签分布矩阵X，记录每个类别划分到每个client去的比例\n",
    "    label_distribution = np.random.dirichlet([alpha]*num_users, n_classes)\n",
    "    # (K, ...) 记录K个类别对应的样本索引集合\n",
    "    class_idcs = [np.argwhere(train_labels == y).flatten()\n",
    "                  for y in range(n_classes)]\n",
    "\n",
    "    # 记录N个client分别对应的样本索引集合\n",
    "    client_idcs = [[] for _ in range(num_users)]\n",
    "    for k_idcs, fracs in zip(class_idcs, label_distribution):\n",
    "        # np.split按照比例fracs将类别为k的样本索引k_idcs划分为了N个子集\n",
    "        # i表示第i个client，idcs表示其对应的样本索引集合idcs\n",
    "        for i, idcs in enumerate(np.split(k_idcs,\n",
    "                                          (np.cumsum(fracs)[:-1]*len(k_idcs)).\n",
    "                                                  astype(int))):\n",
    "            client_idcs[i] += [idcs]\n",
    "\n",
    "    dict_users = [np.concatenate(idcs) for idcs in client_idcs]\n",
    "\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "dict_user_train = dirichlet_split_noniid(dataset_train[:][:][:][2], num_users)\n",
    "dict_user_test = [i for i in range(num_users)]\n",
    "dict_user_train_tmp = [i for i in range(num_users)]\n",
    "for i,item in enumerate(dict_user_train):\n",
    "    train_size = int(0.7 * len(item))\n",
    "    dict_user_train_tmp[i] = np.random.choice(item, train_size, replace = False)\n",
    "    dict_user_test[i] = np.setdiff1d(item, dict_user_train_tmp[i])\n",
    "dict_user_train = dict_user_train_tmp\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it\n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch\n",
    "# size of 16 or 32.\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (embedding): Bert_Embedding(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(1000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder): Bert_Encoder(\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): Sequential()\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=768, out_features=2, bias=False)\n",
      "    (2): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net_glob = MyModel()\n",
    "net_glob.encoder.cls = nn.Sequential()\n",
    "print(net_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 训练loss记录\n",
    "loss_train_collect = {}\n",
    "# 训练acc记录\n",
    "acc_train_collect = {}\n",
    "loss_test_collect = {}\n",
    "# 测试acc记录\n",
    "acc_test_collect = {}\n",
    "# 训练TPR记录\n",
    "TPR_train_collect = {}\n",
    "# 测试TPR记录\n",
    "TPR_test_collect = {}\n",
    "# 训练FPR记录\n",
    "FPR_train_collect = {}\n",
    "# 测试FPR记录\n",
    "FPR_test_collect = {}\n",
    "# 训练测试F1-score记录\n",
    "f1_train_collect = {}\n",
    "f1_test_collect = {}\n",
    "# 训练测试AUC记录\n",
    "AUC_train_collect = {}\n",
    "AUC_test_collect = {}\n",
    "# 训练测试ROC曲线记录\n",
    "ROC_train_collect = {}\n",
    "ROC_test_collect = {}\n",
    "# 本地测试记录\n",
    "local_test = {}\n",
    "local_testing = {}\n",
    "\n",
    "loss_collect = []\n",
    "acc_collect = []\n",
    "TPR_collect = []\n",
    "FPR_collect = []\n",
    "F1_collect = []\n",
    "AUC_collect = []\n",
    "\n",
    "count1 = 0\n",
    "count2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def FedAvg(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[k] += w[i][k]\n",
    "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "idx_collect = []\n",
    "l_epoch_check = False\n",
    "fed_check = False\n",
    "# Initialization of net_model_server and net_server (server-side model)\n",
    "net_model = [net_glob for i in range(num_users)]\n",
    "net_server = copy.deepcopy(net_model[0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        domain, mask, label = self.dataset[self.idxs[item]]\n",
    "        return domain, mask, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    return np.sum(preds == labels) / len(labels)\n",
    "\n",
    "def tpr_calculate(preds, labels):\n",
    "    return recall_score(labels, preds, zero_division=1)\n",
    "\n",
    "def fpr_calculate(preds, labels):\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    #print(conf_matrix)\n",
    "    fp = conf_matrix[0, 1]  # 0 表示负类别，1 表示正类别\n",
    "    tn = conf_matrix[0, 0]\n",
    "    fpr = fp / (fp + tn)\n",
    "    return fpr\n",
    "\n",
    "def f1_score_calculate(preds, labels):\n",
    "    return f1_score(labels, preds, zero_division=1)\n",
    "\n",
    "def AUC_calculate(preds, labels):\n",
    "    return roc_auc_score(labels, preds)\n",
    "\n",
    "def roc_curve_calculate(preds, labels):\n",
    "    return roc_curve(labels, preds)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Client(object):\n",
    "    def __init__(self, device, idx, lr, local_epochs, batch_size, dataset_train = None, idxs = None, idxs_test = None):\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.idx = idx\n",
    "        self.local_ep = local_epochs\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size = batch_size, shuffle = True)\n",
    "        self.ldr_test = DataLoader(DatasetSplit(dataset_train, idxs_test), batch_size = batch_size, shuffle= True)\n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        optimizer_client = torch.optim.Adam(net.parameters(), lr = self.lr)\n",
    "\n",
    "        TPR_train_collect[self.idx] = []\n",
    "        FPR_train_collect[self.idx] = []\n",
    "        f1_train_collect[self.idx] = []\n",
    "        AUC_train_collect[self.idx] = []\n",
    "        loss_train_collect[self.idx] = []\n",
    "        acc_train_collect[self.idx] = []\n",
    "\n",
    "        epoch_loss = []\n",
    "        epoch_accuracy = []\n",
    "        for iter in range(self.local_ep):\n",
    "            tmp_t0 = time.time()\n",
    "            batch_loss_train = []\n",
    "            batch_acc_train = []\n",
    "            batch_tpr_train = []\n",
    "            batch_fpr_train = []\n",
    "            batch_f1_train = []\n",
    "            batch_auc_train = []\n",
    "            for batch_idx, (ids, attn_mask , b_labels) in enumerate(self.ldr_train):\n",
    "                ids, attn_mask ,b_labels = ids.to(self.device), attn_mask.to(self.device), b_labels.to(self.device)\n",
    "                optimizer_client.zero_grad()\n",
    "                b_labels = b_labels.unsqueeze(1)\n",
    "                b_labels = b_labels.repeat(1,2)\n",
    "                for i in range(len(b_labels)):\n",
    "                    b_labels[i][1] = 1-b_labels[i][0]\n",
    "                logits = net(ids, attn_mask)\n",
    "                BCEloss = criterion(logits, b_labels.float())\n",
    "                BCEloss.backward()\n",
    "                optimizer_client.step()\n",
    "                batch_loss_train.append(BCEloss.item())\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                logits = np.argmax(logits, axis=1).flatten()\n",
    "                label_ids = np.argmax(label_ids,axis=1).flatten()\n",
    "                accuracy = flat_accuracy(logits, label_ids)\n",
    "                tpr = tpr_calculate(logits, label_ids)\n",
    "                if len(set(label_ids)) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    fpr = fpr_calculate(logits, label_ids)\n",
    "                    batch_fpr_train.append(fpr)\n",
    "                f1 = f1_score_calculate(logits, label_ids)\n",
    "                if len(set(label_ids)) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    auc = AUC_calculate(logits, label_ids)\n",
    "                    batch_auc_train.append(auc)\n",
    "                batch_acc_train.append(accuracy)\n",
    "                batch_tpr_train.append(tpr)\n",
    "                batch_f1_train.append(f1)\n",
    "            elapsed = format_time(time.time()-tmp_t0)\n",
    "            epoch_avg_loss = sum(batch_loss_train)/len(batch_loss_train)\n",
    "            epoch_avg_acc = sum(batch_acc_train)/len(batch_acc_train)\n",
    "            epoch_avg_tpr = sum(batch_tpr_train)/len(batch_tpr_train)\n",
    "            epoch_avg_fpr = sum(batch_fpr_train)/len(batch_fpr_train)\n",
    "            epoch_avg_f1 = sum(batch_f1_train)/len(batch_f1_train)\n",
    "            epoch_avg_auc = sum(batch_auc_train)/len(batch_auc_train)\n",
    "            epoch_loss.append(sum(batch_loss_train)/len(batch_loss_train))\n",
    "            epoch_accuracy.append(sum(batch_acc_train)/len(batch_acc_train))\n",
    "            loss_train_collect[self.idx].append(epoch_avg_loss)\n",
    "            acc_train_collect[self.idx].append(epoch_avg_acc)\n",
    "            TPR_train_collect[self.idx].append(epoch_avg_tpr)\n",
    "            FPR_train_collect[self.idx].append(epoch_avg_fpr)\n",
    "            f1_train_collect[self.idx].append(epoch_avg_f1)\n",
    "            AUC_train_collect[self.idx].append(epoch_avg_auc)\n",
    "            loss_collect.append(epoch_avg_loss)\n",
    "            acc_collect.append(epoch_avg_acc)\n",
    "            TPR_collect.append(epoch_avg_tpr)\n",
    "            FPR_collect.append(epoch_avg_fpr)\n",
    "            F1_collect.append(epoch_avg_f1)\n",
    "            AUC_collect.append(epoch_avg_auc)\n",
    "\n",
    "            print('Client{} Local Train => Local Epoch: {} \\tLoss: {:.10f} \\tAcc: {:.10f} \\tTPR:{:.10f} \\tFPR:{:.10f} \\tF1:{:.10f} \\t AUC:{:.10f}\\tTrain cost: {:}'.format(self.idx, iter, epoch_avg_loss, \\\n",
    "                                                                                                                                                                           epoch_avg_acc, epoch_avg_tpr, epoch_avg_fpr, epoch_avg_f1, epoch_avg_auc, elapsed))\n",
    "        net_glob.load_state_dict(net.state_dict())\n",
    "        return net.state_dict(), sum(epoch_loss) / len(epoch_loss), sum(epoch_accuracy) / len(epoch_accuracy)\n",
    "\n",
    "    def evaluate(self, net, ell):\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_t0 = time.time()\n",
    "            len_batch = len(self.ldr_test)\n",
    "\n",
    "            batch_acc_test = []\n",
    "            batch_loss_test = []\n",
    "            batch_tpr_test = []\n",
    "            batch_fpr_test = []\n",
    "            batch_f1_test = []\n",
    "            batch_auc_test = []\n",
    "            for batch_idx, (ids, attn_mask, b_labels) in enumerate(self.ldr_test):\n",
    "                ids, attn_mask, b_labels = ids.to(self.device), attn_mask.to(self.device), b_labels.to(self.device)\n",
    "                b_labels = b_labels.unsqueeze(1)\n",
    "                b_labels = b_labels.repeat(1,2)\n",
    "                for i in range(len(b_labels)):\n",
    "                    b_labels[i][1] = 1-b_labels[i][0]\n",
    "                logits = net(ids, attn_mask)\n",
    "                BCEloss = criterion(logits, b_labels.float())\n",
    "                batch_loss_test.append(BCEloss.item())\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                logits = np.argmax(logits, axis=1).flatten()\n",
    "                label_ids = np.argmax(label_ids,axis=1).flatten()\n",
    "                accuracy = flat_accuracy(logits, label_ids)\n",
    "                tpr = tpr_calculate(logits, label_ids)\n",
    "                if len(set(label_ids)) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    fpr = fpr_calculate(logits, label_ids)\n",
    "                    batch_fpr_test.append(fpr)\n",
    "                f1 = f1_score_calculate(logits, label_ids)\n",
    "                if len(set(label_ids)) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    auc = AUC_calculate(logits, label_ids)\n",
    "                    batch_auc_test.append(auc)\n",
    "                batch_acc_test.append(accuracy)\n",
    "                batch_tpr_test.append(tpr)\n",
    "                batch_f1_test.append(f1)\n",
    "            elapsed = format_time(time.time()-tmp_t0)\n",
    "            test_avg_loss = sum(batch_loss_test) / len(batch_loss_test)\n",
    "            test_avg_acc = sum(batch_acc_test) / len(batch_acc_test)\n",
    "            test_avg_tpr = sum(batch_tpr_test)/len(batch_tpr_test)\n",
    "            test_avg_fpr = sum(batch_fpr_test) / len(batch_fpr_test)\n",
    "            test_avg_f1 = sum(batch_f1_test)/len(batch_f1_test)\n",
    "            test_avg_auc = sum(batch_auc_test)/len(batch_auc_test)\n",
    "            local_test[\"loss\"].append(test_avg_loss)\n",
    "            local_test[\"acc\"].append(test_avg_acc)\n",
    "            local_test[\"tpr\"].append(test_avg_tpr)\n",
    "            local_test[\"fpr\"].append(test_avg_fpr)\n",
    "            local_test[\"f1\"].append(test_avg_f1)\n",
    "            local_test[\"auc\"].append(test_avg_auc)\n",
    "            print('Client{} Test =>                 \\tLoss: {:.10f} \\tAcc: {:.10f} \\tTPR:{:.10f} \\tFPR:{:.10f} \\tF1:{:.10f} \\tAUC:{:.10f} \\ttest cost: {:}'.format(self.idx, test_avg_loss, test_avg_acc, test_avg_tpr, test_avg_fpr, test_avg_f1, test_avg_auc, elapsed))\n",
    "\n",
    "        return test_avg_loss, test_avg_acc, test_avg_tpr, test_avg_fpr, test_avg_f1, test_avg_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Begin!\n",
      "============== Round 0:  =============\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.4879372154 \tAcc: 0.7875704748 \tTPR:0.9538817064 \tFPR:0.7691308156 \tF1:0.8713824560 \t AUC:0.5923754454\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.4641613697 \tAcc: 0.8064873887 \tTPR:0.9520763546 \tFPR:0.6861730258 \tF1:0.8815640149 \t AUC:0.6329516644\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.4194963934 \tAcc: 0.8369176558 \tTPR:0.9637587437 \tFPR:0.5939000169 \tF1:0.8994514970 \t AUC:0.6849293634\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.3573227299 \tAcc: 0.8639132047 \tTPR:0.9621832905 \tFPR:0.4751156731 \tF1:0.9143203333 \t AUC:0.7435338087\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.3228018213 \tAcc: 0.8769881306 \tTPR:0.9622501235 \tFPR:0.4088403272 \tF1:0.9214503821 \t AUC:0.7767048982\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.2806110941 \tAcc: 0.8916144201 \tTPR:0.9545616252 \tFPR:0.3079715304 \tF1:0.9295228094 \tAUC:0.8232950474 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.2811661548 \tAcc: 0.8806440222 \tTPR:0.4574386928 \tFPR:0.0410176470 \tF1:0.4884417419 \t AUC:0.7058918849\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.2686177288 \tAcc: 0.8837337115 \tTPR:0.4815210214 \tFPR:0.0423239258 \tF1:0.5117641087 \t AUC:0.7195985478\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.2557532581 \tAcc: 0.8918494851 \tTPR:0.5365225092 \tFPR:0.0418931815 \tF1:0.5606636285 \t AUC:0.7453339908\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.2467067509 \tAcc: 0.8953834062 \tTPR:0.5469799896 \tFPR:0.0395278121 \tF1:0.5659829581 \t AUC:0.7517901058\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.2347894055 \tAcc: 0.9013919264 \tTPR:0.5798134696 \tFPR:0.0392385965 \tF1:0.6129175668 \t AUC:0.7702874366\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.2162155297 \tAcc: 0.9106568221 \tTPR:0.5502076688 \tFPR:0.0281040339 \tF1:0.5953993558 \tAUC:0.7595624389 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.3296712176 \tAcc: 0.8513764220 \tTPR:0.8777291916 \tFPR:0.1733471107 \tF1:0.8447860591 \t AUC:0.8521910405\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.3164770007 \tAcc: 0.8615801635 \tTPR:0.8844759653 \tFPR:0.1599893176 \tF1:0.8538660509 \t AUC:0.8622433239\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.3016375386 \tAcc: 0.8751444188 \tTPR:0.8971293286 \tFPR:0.1449352344 \tF1:0.8683968425 \t AUC:0.8760970471\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.2912030778 \tAcc: 0.8752249600 \tTPR:0.8912039392 \tFPR:0.1392393356 \tF1:0.8675090130 \t AUC:0.8759823018\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.2771533778 \tAcc: 0.8848982403 \tTPR:0.9049928831 \tFPR:0.1304994820 \tF1:0.8783872466 \t AUC:0.8872467006\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2561976602 \tAcc: 0.8935046574 \tTPR:0.9258112580 \tFPR:0.1336066038 \tF1:0.8883006104 \tAUC:0.8961023271 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.1673006286 \tAcc: 0.9300638686 \tTPR:0.5912738964 \tFPR:0.0277458368 \tF1:0.5931104705 \t AUC:0.7759338526\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.1557065241 \tAcc: 0.9364507299 \tTPR:0.6539404472 \tFPR:0.0252554368 \tF1:0.6409083712 \t AUC:0.8088701780\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.1486161945 \tAcc: 0.9383211679 \tTPR:0.6608562160 \tFPR:0.0265444608 \tF1:0.6487092267 \t AUC:0.8117929111\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.1446693934 \tAcc: 0.9411496350 \tTPR:0.6737423242 \tFPR:0.0251908989 \tF1:0.6700328215 \t AUC:0.8201242303\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.1358666592 \tAcc: 0.9446167883 \tTPR:0.6919225098 \tFPR:0.0240670777 \tF1:0.6909805571 \t AUC:0.8300075683\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1469077649 \tAcc: 0.9487670068 \tTPR:0.6125026995 \tFPR:0.0099462605 \tF1:0.6623052496 \tAUC:0.7965526426 \ttest cost: 0:00:09\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.2275976281 \tAcc: 0.9125744048 \tTPR:0.9532805373 \tFPR:0.1839124995 \tF1:0.9348780125 \t AUC:0.8846840189\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.2003092133 \tAcc: 0.9232886905 \tTPR:0.9610545258 \tFPR:0.1569753659 \tF1:0.9438819062 \t AUC:0.9020395799\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1917435106 \tAcc: 0.9281994048 \tTPR:0.9638699126 \tFPR:0.1526318258 \tF1:0.9479081557 \t AUC:0.9056190434\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1960119471 \tAcc: 0.9206101190 \tTPR:0.9556182414 \tFPR:0.1608667787 \tF1:0.9418840552 \t AUC:0.8973757314\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1971098127 \tAcc: 0.9196428571 \tTPR:0.9561351474 \tFPR:0.1592851428 \tF1:0.9407195387 \t AUC:0.8984250023\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2055238500 \tAcc: 0.9194595411 \tTPR:0.9323283226 \tFPR:0.1119348204 \tF1:0.9357152984 \tAUC:0.9101967511 \ttest cost: 0:00:01\n",
      "Test =>                 \tLoss: 0.2210911798 \tAcc: 0.9128004895 \tTPR:0.7950823148 \tFPR:0.1183126498 \tF1:0.8022486647 \tAUC:0.8371418414\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 1:  =============\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.2018931390 \tAcc: 0.9159576043 \tTPR:0.6679766561 \tFPR:0.0371325372 \tF1:0.6801134728 \t AUC:0.8144788113\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1963201710 \tAcc: 0.9188621970 \tTPR:0.6839588378 \tFPR:0.0362195684 \tF1:0.6975246772 \t AUC:0.8225190314\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.1886782865 \tAcc: 0.9200069482 \tTPR:0.6901429550 \tFPR:0.0363474148 \tF1:0.6953204008 \t AUC:0.8255735947\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.1783214665 \tAcc: 0.9264568526 \tTPR:0.7290110549 \tFPR:0.0359330939 \tF1:0.7233256308 \t AUC:0.8457691255\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.1748505666 \tAcc: 0.9293756834 \tTPR:0.7253592218 \tFPR:0.0336723607 \tF1:0.7350422555 \t AUC:0.8450632010\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1729394056 \tAcc: 0.9316629720 \tTPR:0.6175217874 \tFPR:0.0137648298 \tF1:0.6954703514 \tAUC:0.8018784788 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.2036778170 \tAcc: 0.9247700297 \tTPR:0.9700114987 \tFPR:0.2261446598 \tF1:0.9508899463 \t AUC:0.8719334194\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1934801943 \tAcc: 0.9309718101 \tTPR:0.9722135933 \tFPR:0.2083419877 \tF1:0.9551858401 \t AUC:0.8819358028\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1871343288 \tAcc: 0.9332344214 \tTPR:0.9713932434 \tFPR:0.1981639528 \tF1:0.9564619484 \t AUC:0.8866146453\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1763530581 \tAcc: 0.9371439169 \tTPR:0.9731850241 \tFPR:0.1831953490 \tF1:0.9589016294 \t AUC:0.8949948375\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1742512646 \tAcc: 0.9346142433 \tTPR:0.9718264108 \tFPR:0.1929803011 \tF1:0.9575048491 \t AUC:0.8894230548\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.2406062382 \tAcc: 0.9090517241 \tTPR:0.9097618911 \tFPR:0.0887217572 \tF1:0.9375689523 \tAUC:0.9105200669 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1813014784 \tAcc: 0.9353991597 \tTPR:0.9707542325 \tFPR:0.2031501237 \tF1:0.9592046947 \t AUC:0.8838020544\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.1537899660 \tAcc: 0.9521402311 \tTPR:0.9829408251 \tFPR:0.1708900227 \tF1:0.9699179570 \t AUC:0.9060254012\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.1670907583 \tAcc: 0.9520089286 \tTPR:0.9875372082 \tFPR:0.1820243249 \tF1:0.9695537132 \t AUC:0.9027564417\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.1722554043 \tAcc: 0.9397321429 \tTPR:0.9715513654 \tFPR:0.1913136467 \tF1:0.9622533642 \t AUC:0.8901188593\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.1524313022 \tAcc: 0.9409795168 \tTPR:0.9777570769 \tFPR:0.2252164502 \tF1:0.9628418429 \t AUC:0.8762703133\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.1514148501 \tAcc: 0.9336939103 \tTPR:0.9667172984 \tFPR:0.1583092833 \tF1:0.9565449754 \tAUC:0.9042040075 \ttest cost: 0:00:00\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1741140189 \tAcc: 0.9368411145 \tTPR:0.9753831010 \tFPR:0.2166583165 \tF1:0.9608037410 \t AUC:0.8793252066\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.1648014692 \tAcc: 0.9383919607 \tTPR:0.9746556178 \tFPR:0.2058452675 \tF1:0.9612822838 \t AUC:0.8844051752\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.1608440503 \tAcc: 0.9414488310 \tTPR:0.9762332454 \tFPR:0.2022701283 \tF1:0.9633096213 \t AUC:0.8869456571\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.1569061988 \tAcc: 0.9446536145 \tTPR:0.9778452485 \tFPR:0.1922919534 \tF1:0.9656170173 \t AUC:0.8927766476\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.1501826174 \tAcc: 0.9459220812 \tTPR:0.9772471678 \tFPR:0.1843828880 \tF1:0.9662483377 \t AUC:0.8963977701\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1835421109 \tAcc: 0.9278846154 \tTPR:0.9359936510 \tFPR:0.1055895982 \tF1:0.9531506368 \tAUC:0.9149766519 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1953815056 \tAcc: 0.9244047619 \tTPR:0.9590420827 \tFPR:0.1432065620 \tF1:0.9439290628 \t AUC:0.9079177604\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1825611972 \tAcc: 0.9266369048 \tTPR:0.9525233490 \tFPR:0.1342837154 \tF1:0.9450274224 \t AUC:0.9091198168\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1845188195 \tAcc: 0.9322916667 \tTPR:0.9657005960 \tFPR:0.1384102650 \tF1:0.9499362973 \t AUC:0.9136451655\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1746257347 \tAcc: 0.9325892857 \tTPR:0.9648497904 \tFPR:0.1421295305 \tF1:0.9503911847 \t AUC:0.9113601299\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1641877254 \tAcc: 0.9401041667 \tTPR:0.9687207923 \tFPR:0.1283075585 \tF1:0.9556875471 \t AUC:0.9202066169\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2013212097 \tAcc: 0.9215353261 \tTPR:0.9250883856 \tFPR:0.0886151657 \tF1:0.9383758679 \tAUC:0.9182366099 \ttest cost: 0:00:01\n",
      "Test =>                 \tLoss: 0.1899647629 \tAcc: 0.9247657096 \tTPR:0.8710166027 \tFPR:0.0910001268 \tF1:0.8962221567 \tAUC:0.8899631630\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 2:  =============\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.2284208800 \tAcc: 0.9070030120 \tTPR:0.9071203080 \tFPR:0.0928422124 \tF1:0.8936085904 \t AUC:0.9071390478\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.2219127047 \tAcc: 0.9060993976 \tTPR:0.9123234756 \tFPR:0.0961695400 \tF1:0.8911971870 \t AUC:0.9080769678\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.2101537065 \tAcc: 0.9125753012 \tTPR:0.9154396440 \tFPR:0.0909473775 \tF1:0.8984823956 \t AUC:0.9122461332\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.2157280578 \tAcc: 0.9088855422 \tTPR:0.9106369606 \tFPR:0.0938729092 \tF1:0.8964004166 \t AUC:0.9083820257\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.2172771111 \tAcc: 0.9151355422 \tTPR:0.9151179646 \tFPR:0.0896695224 \tF1:0.9023292172 \t AUC:0.9127242211\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1926294606 \tAcc: 0.9201388889 \tTPR:0.9013205680 \tFPR:0.0631046252 \tF1:0.9062620399 \tAUC:0.9191079714 \ttest cost: 0:00:01\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1884145693 \tAcc: 0.9258928571 \tTPR:0.9633833883 \tFPR:0.1549050652 \tF1:0.9454588190 \t AUC:0.9042391615\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1774538775 \tAcc: 0.9285714286 \tTPR:0.9616468312 \tFPR:0.1445537772 \tF1:0.9467149415 \t AUC:0.9085465270\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1777193010 \tAcc: 0.9299107143 \tTPR:0.9643316203 \tFPR:0.1442070311 \tF1:0.9488100319 \t AUC:0.9100622946\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1766582070 \tAcc: 0.9344494048 \tTPR:0.9651305838 \tFPR:0.1290732804 \tF1:0.9513884937 \t AUC:0.9180286517\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1747744336 \tAcc: 0.9389136905 \tTPR:0.9692349237 \tFPR:0.1276886969 \tF1:0.9545158208 \t AUC:0.9207731134\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.1961939640 \tAcc: 0.9286684783 \tTPR:0.9380439075 \tFPR:0.0934254403 \tF1:0.9456329288 \tAUC:0.9223092336 \ttest cost: 0:00:01\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1675082606 \tAcc: 0.9393026706 \tTPR:0.9735586719 \tFPR:0.1744867727 \tF1:0.9602621661 \t AUC:0.8995359496\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1616979037 \tAcc: 0.9418063798 \tTPR:0.9763983532 \tFPR:0.1745482921 \tF1:0.9617800904 \t AUC:0.9009250305\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1598941194 \tAcc: 0.9424146884 \tTPR:0.9746732876 \tFPR:0.1677776971 \tF1:0.9622531245 \t AUC:0.9034477953\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1566919406 \tAcc: 0.9446142433 \tTPR:0.9756318638 \tFPR:0.1586970975 \tF1:0.9638751350 \t AUC:0.9084673831\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1524833004 \tAcc: 0.9444176558 \tTPR:0.9770794349 \tFPR:0.1626960577 \tF1:0.9635049474 \t AUC:0.9071916886\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1788993871 \tAcc: 0.9323471787 \tTPR:0.9531624378 \tFPR:0.1324030567 \tF1:0.9547563613 \tAUC:0.9103796906 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.2226904636 \tAcc: 0.9071026262 \tTPR:0.9184004372 \tFPR:0.1044970722 \tF1:0.9008150149 \t AUC:0.9069516825\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.2150502157 \tAcc: 0.9136514620 \tTPR:0.9272208183 \tFPR:0.0979898919 \tF1:0.9082218255 \t AUC:0.9146154632\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.2110075226 \tAcc: 0.9139736269 \tTPR:0.9277701439 \tFPR:0.0986536679 \tF1:0.9080696608 \t AUC:0.9145582380\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.2053150307 \tAcc: 0.9157372023 \tTPR:0.9272818942 \tFPR:0.0933679063 \tF1:0.9097283522 \t AUC:0.9169569939\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.2003518204 \tAcc: 0.9183311856 \tTPR:0.9316262177 \tFPR:0.0954260770 \tF1:0.9129904039 \t AUC:0.9181000704\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2103316460 \tAcc: 0.9167290419 \tTPR:0.8961474669 \tFPR:0.0638192285 \tF1:0.9090566136 \tAUC:0.9161641192 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.1235887258 \tAcc: 0.9482664234 \tTPR:0.7424620554 \tFPR:0.0225612903 \tF1:0.7211614560 \t AUC:0.8558778699\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.1171282651 \tAcc: 0.9516879562 \tTPR:0.7475292550 \tFPR:0.0224109396 \tF1:0.7365386012 \t AUC:0.8599253348\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.1141771771 \tAcc: 0.9526459854 \tTPR:0.7494624030 \tFPR:0.0210158284 \tF1:0.7338578100 \t AUC:0.8602614729\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.1114712054 \tAcc: 0.9544251825 \tTPR:0.7518960723 \tFPR:0.0206725037 \tF1:0.7454688286 \t AUC:0.8630235168\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.1085767559 \tAcc: 0.9552463504 \tTPR:0.7673946451 \tFPR:0.0206632030 \tF1:0.7600799210 \t AUC:0.8695065070\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1175378389 \tAcc: 0.9544005102 \tTPR:0.7507518087 \tFPR:0.0227819001 \tF1:0.7375172796 \tAUC:0.8600494565 \ttest cost: 0:00:09\n",
      "Test =>                 \tLoss: 0.1791184593 \tAcc: 0.9304568196 \tTPR:0.8878852378 \tFPR:0.0751068502 \tF1:0.8906450446 \tAUC:0.9056020943\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 3:  =============\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.2094001491 \tAcc: 0.9167120290 \tTPR:0.9309072692 \tFPR:0.0958005496 \tF1:0.9111512704 \t AUC:0.9175533598\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.2080258689 \tAcc: 0.9147707074 \tTPR:0.9279076067 \tFPR:0.0975507455 \tF1:0.9090774578 \t AUC:0.9151784306\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1991677801 \tAcc: 0.9201836340 \tTPR:0.9311560171 \tFPR:0.0906699803 \tF1:0.9148968576 \t AUC:0.9202430184\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1938597846 \tAcc: 0.9217139175 \tTPR:0.9334705588 \tFPR:0.0882691752 \tF1:0.9161227593 \t AUC:0.9226006918\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1919449993 \tAcc: 0.9217777951 \tTPR:0.9339071157 \tFPR:0.0886797565 \tF1:0.9162203576 \t AUC:0.9226136796\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2631289085 \tAcc: 0.9002619760 \tTPR:0.8344341968 \tFPR:0.0401143747 \tF1:0.8854627148 \tAUC:0.8971599110 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1505072896 \tAcc: 0.9460979228 \tTPR:0.9774827997 \tFPR:0.1571311313 \tF1:0.9649313498 \t AUC:0.9101758342\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1451190658 \tAcc: 0.9485089021 \tTPR:0.9782236919 \tFPR:0.1492091242 \tF1:0.9664114963 \t AUC:0.9145072839\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1444562633 \tAcc: 0.9453820475 \tTPR:0.9775197860 \tFPR:0.1631843387 \tF1:0.9643437125 \t AUC:0.9071677236\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1375855219 \tAcc: 0.9507603858 \tTPR:0.9792090505 \tFPR:0.1456205269 \tF1:0.9679001770 \t AUC:0.9167942618\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1358827568 \tAcc: 0.9515022255 \tTPR:0.9794066930 \tFPR:0.1393391557 \tF1:0.9684576328 \t AUC:0.9200337686\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1958815286 \tAcc: 0.9280760188 \tTPR:0.9381899933 \tFPR:0.1071086768 \tF1:0.9513768331 \tAUC:0.9155406583 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1443939914 \tAcc: 0.9409371013 \tTPR:0.7804026012 \tFPR:0.0287140356 \tF1:0.7819662013 \t AUC:0.8758442828\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1410491024 \tAcc: 0.9416376207 \tTPR:0.7933006416 \tFPR:0.0294306910 \tF1:0.7900613361 \t AUC:0.8816422000\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.1363572569 \tAcc: 0.9438502369 \tTPR:0.7884961163 \tFPR:0.0282616072 \tF1:0.7904656444 \t AUC:0.8792133918\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.1332560224 \tAcc: 0.9463248360 \tTPR:0.7971749374 \tFPR:0.0278325422 \tF1:0.7955881117 \t AUC:0.8838044238\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.1279385791 \tAcc: 0.9493234008 \tTPR:0.8199836338 \tFPR:0.0257981057 \tF1:0.8180486156 \t AUC:0.8968377834\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1328109215 \tAcc: 0.9498355263 \tTPR:0.7752897870 \tFPR:0.0205970354 \tF1:0.7907046482 \tAUC:0.8766023022 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1430801425 \tAcc: 0.9495481928 \tTPR:0.9820197752 \tFPR:0.1818565387 \tF1:0.9686226325 \t AUC:0.9000816182\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.1361512100 \tAcc: 0.9519461776 \tTPR:0.9814983866 \tFPR:0.1725205811 \tF1:0.9700676165 \t AUC:0.9045081601\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.1284649097 \tAcc: 0.9529322648 \tTPR:0.9800123410 \tFPR:0.1526093552 \tF1:0.9704959713 \t AUC:0.9137014929\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.1257923748 \tAcc: 0.9555229848 \tTPR:0.9819849985 \tFPR:0.1511120824 \tF1:0.9722294722 \t AUC:0.9154364581\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.1234607534 \tAcc: 0.9560429217 \tTPR:0.9830925534 \tFPR:0.1546723023 \tF1:0.9723864006 \t AUC:0.9142101256\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1750448004 \tAcc: 0.9334790210 \tTPR:0.9403713771 \tFPR:0.0954344755 \tF1:0.9570268345 \tAUC:0.9229627162 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1811772306 \tAcc: 0.9297972680 \tTPR:0.9600035367 \tFPR:0.1255182871 \tF1:0.9452484033 \t AUC:0.9172426248\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1719597605 \tAcc: 0.9325364990 \tTPR:0.9619274174 \tFPR:0.1205051993 \tF1:0.9471072836 \t AUC:0.9207111090\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1698080556 \tAcc: 0.9346487424 \tTPR:0.9643546450 \tFPR:0.1196883423 \tF1:0.9490912314 \t AUC:0.9223331513\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1636335698 \tAcc: 0.9368206129 \tTPR:0.9646174390 \tFPR:0.1126669035 \tF1:0.9504772452 \t AUC:0.9259752677\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1587007598 \tAcc: 0.9383383926 \tTPR:0.9643155145 \tFPR:0.1082672526 \tF1:0.9514102702 \t AUC:0.9280241309\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1763670813 \tAcc: 0.9332070707 \tTPR:0.9377851271 \tFPR:0.0770025085 \tF1:0.9463836519 \tAUC:0.9303913093 \ttest cost: 0:00:16\n",
      "Test =>                 \tLoss: 0.1886466481 \tAcc: 0.9289719226 \tTPR:0.8852140963 \tFPR:0.0680514142 \tF1:0.9061909365 \tAUC:0.9085313794\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 4:  =============\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1348213009 \tAcc: 0.9446504237 \tTPR:0.7950654650 \tFPR:0.0281887597 \tF1:0.7992587528 \t AUC:0.8831480771\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1266826065 \tAcc: 0.9492322763 \tTPR:0.8072056318 \tFPR:0.0258081705 \tF1:0.8055740471 \t AUC:0.8904256508\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.1251669986 \tAcc: 0.9493262484 \tTPR:0.8137559004 \tFPR:0.0266886520 \tF1:0.8131957284 \t AUC:0.8932698224\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.1213765231 \tAcc: 0.9513566156 \tTPR:0.8121484213 \tFPR:0.0240032979 \tF1:0.8114818467 \t AUC:0.8935388924\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.1171865361 \tAcc: 0.9527661974 \tTPR:0.8320551153 \tFPR:0.0248202068 \tF1:0.8285536054 \t AUC:0.9019234857\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1496318420 \tAcc: 0.9468767878 \tTPR:0.6927187761 \tFPR:0.0132027047 \tF1:0.7474895671 \tAUC:0.8397580357 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1401094459 \tAcc: 0.9516876855 \tTPR:0.9793427799 \tFPR:0.1394126974 \tF1:0.9684030973 \t AUC:0.9199650413\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1302206497 \tAcc: 0.9553041543 \tTPR:0.9818640490 \tFPR:0.1329685361 \tF1:0.9706832414 \t AUC:0.9244672714\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1296860253 \tAcc: 0.9514094955 \tTPR:0.9792334153 \tFPR:0.1458518649 \tF1:0.9684682978 \t AUC:0.9166907752\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1268702934 \tAcc: 0.9550000000 \tTPR:0.9824387978 \tFPR:0.1385093229 \tF1:0.9705823494 \t AUC:0.9219647374\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1243543910 \tAcc: 0.9563649852 \tTPR:0.9818605685 \tFPR:0.1331545556 \tF1:0.9715789814 \t AUC:0.9243530064\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.2110661811 \tAcc: 0.9215517241 \tTPR:0.9202105579 \tFPR:0.0822402119 \tF1:0.9461938979 \tAUC:0.9189851730 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1756220690 \tAcc: 0.9323738797 \tTPR:0.9623210407 \tFPR:0.1208590803 \tF1:0.9472729867 \t AUC:0.9207309802\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1695539065 \tAcc: 0.9342187048 \tTPR:0.9636565777 \tFPR:0.1184981843 \tF1:0.9485091133 \t AUC:0.9225791967\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1630881659 \tAcc: 0.9368188060 \tTPR:0.9634637507 \tFPR:0.1111756467 \tF1:0.9500484673 \t AUC:0.9261440520\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1592545304 \tAcc: 0.9395526164 \tTPR:0.9657953367 \tFPR:0.1080778072 \tF1:0.9525190888 \t AUC:0.9288587647\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1572448162 \tAcc: 0.9389852559 \tTPR:0.9664929835 \tFPR:0.1095514632 \tF1:0.9524169167 \t AUC:0.9284707602\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1901881350 \tAcc: 0.9301136364 \tTPR:0.9308798751 \tFPR:0.0710694988 \tF1:0.9435318703 \tAUC:0.9298352285 \ttest cost: 0:00:16\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1214987812 \tAcc: 0.9579702740 \tTPR:0.9841347328 \tFPR:0.1485461660 \tF1:0.9736364996 \t AUC:0.9177942834\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.1186367595 \tAcc: 0.9583019578 \tTPR:0.9843864152 \tFPR:0.1516295806 \tF1:0.9739294530 \t AUC:0.9163548318\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.1136264455 \tAcc: 0.9593373494 \tTPR:0.9847991009 \tFPR:0.1494003479 \tF1:0.9745447558 \t AUC:0.9176764144\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.1117572291 \tAcc: 0.9602293101 \tTPR:0.9847036356 \tFPR:0.1410224230 \tF1:0.9750805907 \t AUC:0.9218406063\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.1125987811 \tAcc: 0.9589608434 \tTPR:0.9842555475 \tFPR:0.1407852333 \tF1:0.9741697621 \t AUC:0.9216874467\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1584259289 \tAcc: 0.9396853147 \tTPR:0.9478778835 \tFPR:0.0973650087 \tF1:0.9612616460 \tAUC:0.9252564374 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.1097838231 \tAcc: 0.9550638686 \tTPR:0.7794600331 \tFPR:0.0213500449 \tF1:0.7577017160 \t AUC:0.8750512428\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.1039466946 \tAcc: 0.9578923358 \tTPR:0.7809564361 \tFPR:0.0194926213 \tF1:0.7701302692 \t AUC:0.8782799272\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0986032793 \tAcc: 0.9594434307 \tTPR:0.7906801066 \tFPR:0.0190435506 \tF1:0.7730829025 \t AUC:0.8828324837\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0957358626 \tAcc: 0.9606295620 \tTPR:0.7891310393 \tFPR:0.0184206935 \tF1:0.7739791319 \t AUC:0.8834752119\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0941303676 \tAcc: 0.9619981752 \tTPR:0.8088946820 \tFPR:0.0181807463 \tF1:0.7949023685 \t AUC:0.8930717024\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1083990920 \tAcc: 0.9593608277 \tTPR:0.7221817298 \tFPR:0.0120447614 \tF1:0.7465477803 \tAUC:0.8516804565 \ttest cost: 0:00:09\n",
      "Test =>                 \tLoss: 0.1635422358 \tAcc: 0.9395176581 \tTPR:0.8427737645 \tFPR:0.0551844371 \tF1:0.8690049523 \tAUC:0.8931030662\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 5:  =============\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1439561275 \tAcc: 0.9467261905 \tTPR:0.9730926280 \tFPR:0.1109714340 \tF1:0.9608357049 \t AUC:0.9310605970\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1434127498 \tAcc: 0.9434523810 \tTPR:0.9711755653 \tFPR:0.1151756808 \tF1:0.9575258251 \t AUC:0.9279999423\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1345476630 \tAcc: 0.9467261905 \tTPR:0.9691839010 \tFPR:0.1038045288 \tF1:0.9610145117 \t AUC:0.9326896861\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1286051023 \tAcc: 0.9508928571 \tTPR:0.9769849784 \tFPR:0.0986514905 \tF1:0.9633354530 \t AUC:0.9391667439\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1234191161 \tAcc: 0.9523809524 \tTPR:0.9756232045 \tFPR:0.0990556071 \tF1:0.9648873879 \t AUC:0.9382837987\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2438684141 \tAcc: 0.9206672705 \tTPR:0.9014348185 \tFPR:0.0406838532 \tF1:0.9364677212 \tAUC:0.9303754827 \ttest cost: 0:00:01\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1180761801 \tAcc: 0.9578268431 \tTPR:0.9840454355 \tFPR:0.1529360950 \tF1:0.9737274207 \t AUC:0.9155546702\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.1122206310 \tAcc: 0.9604175631 \tTPR:0.9840289447 \tFPR:0.1374843530 \tF1:0.9752778009 \t AUC:0.9232722958\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.1084318413 \tAcc: 0.9603189544 \tTPR:0.9844509119 \tFPR:0.1412154562 \tF1:0.9750584229 \t AUC:0.9216414452\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.1064737089 \tAcc: 0.9619728916 \tTPR:0.9847615510 \tFPR:0.1341795504 \tF1:0.9761717462 \t AUC:0.9252910003\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.1034354388 \tAcc: 0.9637612952 \tTPR:0.9864043730 \tFPR:0.1247088922 \tF1:0.9772554988 \t AUC:0.9308477404\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1815294731 \tAcc: 0.9318181818 \tTPR:0.9360646534 \tFPR:0.0913651880 \tF1:0.9556600728 \tAUC:0.9221246082 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1818436123 \tAcc: 0.9260464806 \tTPR:0.9359935077 \tFPR:0.0823622979 \tF1:0.9201716794 \t AUC:0.9268156049\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1723638179 \tAcc: 0.9315954941 \tTPR:0.9441320298 \tFPR:0.0798394342 \tF1:0.9269023754 \t AUC:0.9321462978\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1648454537 \tAcc: 0.9333035238 \tTPR:0.9428499882 \tFPR:0.0748235705 \tF1:0.9289648942 \t AUC:0.9340132088\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1636605364 \tAcc: 0.9329563633 \tTPR:0.9396145049 \tFPR:0.0736676329 \tF1:0.9270443243 \t AUC:0.9329734360\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1571525227 \tAcc: 0.9372417126 \tTPR:0.9467157539 \tFPR:0.0704677185 \tF1:0.9329603885 \t AUC:0.9381240177\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2264355610 \tAcc: 0.9242140719 \tTPR:0.8977604436 \tFPR:0.0529613212 \tF1:0.9138043218 \tAUC:0.9223995612 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1199593011 \tAcc: 0.9504709996 \tTPR:0.8276657900 \tFPR:0.0266996039 \tF1:0.8214149408 \t AUC:0.8999935072\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1164933683 \tAcc: 0.9516242938 \tTPR:0.8301071653 \tFPR:0.0254696514 \tF1:0.8260303570 \t AUC:0.9018361069\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.1111273961 \tAcc: 0.9540020731 \tTPR:0.8353740186 \tFPR:0.0245300202 \tF1:0.8251042811 \t AUC:0.9044812793\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.1101580673 \tAcc: 0.9566532258 \tTPR:0.8402314101 \tFPR:0.0226658110 \tF1:0.8410837049 \t AUC:0.9085564985\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.1028225693 \tAcc: 0.9563855477 \tTPR:0.8409357905 \tFPR:0.0224485421 \tF1:0.8368740491 \t AUC:0.9090183208\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1365006021 \tAcc: 0.9508634868 \tTPR:0.7852965748 \tFPR:0.0200845078 \tF1:0.8053686216 \tAUC:0.8818950950 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1280333980 \tAcc: 0.9534235905 \tTPR:0.9800374190 \tFPR:0.1375324889 \tF1:0.9693782482 \t AUC:0.9212524650\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1232082567 \tAcc: 0.9561127596 \tTPR:0.9820900631 \tFPR:0.1292011648 \tF1:0.9710530597 \t AUC:0.9264444491\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1147881846 \tAcc: 0.9585905045 \tTPR:0.9829588894 \tFPR:0.1263599243 \tF1:0.9729333145 \t AUC:0.9282994826\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1117922781 \tAcc: 0.9605118694 \tTPR:0.9845351230 \tFPR:0.1204012275 \tF1:0.9743464441 \t AUC:0.9320669478\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1084684546 \tAcc: 0.9599814540 \tTPR:0.9820814283 \tFPR:0.1129877623 \tF1:0.9737810904 \t AUC:0.9345468330\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1526195524 \tAcc: 0.9433189655 \tTPR:0.9641901441 \tFPR:0.1168842460 \tF1:0.9623914047 \tAUC:0.9236529490 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1881907205 \tAcc: 0.9341763953 \tTPR:0.8969493269 \tFPR:0.0643958232 \tF1:0.9147384284 \tAUC:0.9160895392\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 6:  =============\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1086526922 \tAcc: 0.9601800057 \tTPR:0.9837771946 \tFPR:0.1413297044 \tF1:0.9751314314 \t AUC:0.9212237451\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.1017015118 \tAcc: 0.9630979274 \tTPR:0.9857863884 \tFPR:0.1275495569 \tF1:0.9766567071 \t AUC:0.9291441505\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0993630231 \tAcc: 0.9637612952 \tTPR:0.9862169701 \tFPR:0.1272475182 \tF1:0.9774181037 \t AUC:0.9294847260\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0966088864 \tAcc: 0.9655945209 \tTPR:0.9864439494 \tFPR:0.1198668717 \tF1:0.9783505144 \t AUC:0.9332885388\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0963963098 \tAcc: 0.9657379518 \tTPR:0.9866907428 \tFPR:0.1233797327 \tF1:0.9786294170 \t AUC:0.9316555050\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1625311410 \tAcc: 0.9427447552 \tTPR:0.9551136335 \tFPR:0.1082633567 \tF1:0.9633222008 \tAUC:0.9234251384 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1241129770 \tAcc: 0.9545623145 \tTPR:0.9822336078 \tFPR:0.1439430500 \tF1:0.9706032632 \t AUC:0.9191452789\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1136386107 \tAcc: 0.9602188427 \tTPR:0.9837486086 \tFPR:0.1249553859 \tF1:0.9739651476 \t AUC:0.9293966114\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1133723498 \tAcc: 0.9603783383 \tTPR:0.9846531883 \tFPR:0.1193930451 \tF1:0.9741258977 \t AUC:0.9326300716\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.1114332727 \tAcc: 0.9592915430 \tTPR:0.9834862140 \tFPR:0.1235032524 \tF1:0.9734415006 \t AUC:0.9299914808\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.1097056582 \tAcc: 0.9597292285 \tTPR:0.9843114959 \tFPR:0.1214464731 \tF1:0.9736597660 \t AUC:0.9314325114\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1892070172 \tAcc: 0.9381465517 \tTPR:0.9470509007 \tFPR:0.0933266159 \tF1:0.9582287616 \tAUC:0.9268621424 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.1076855443 \tAcc: 0.9552919708 \tTPR:0.7824371452 \tFPR:0.0223794746 \tF1:0.7544622046 \t AUC:0.8745230268\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0980287230 \tAcc: 0.9589872263 \tTPR:0.7921196849 \tFPR:0.0199994073 \tF1:0.7745644583 \t AUC:0.8835742755\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0978710986 \tAcc: 0.9593065693 \tTPR:0.7927578180 \tFPR:0.0191294877 \tF1:0.7713481532 \t AUC:0.8835369921\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0950657993 \tAcc: 0.9618156934 \tTPR:0.7971121539 \tFPR:0.0177992632 \tF1:0.7910243502 \t AUC:0.8872302828\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0903477840 \tAcc: 0.9630474453 \tTPR:0.8107913336 \tFPR:0.0177246971 \tF1:0.7951710457 \t AUC:0.8945594573\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1166092696 \tAcc: 0.9608843537 \tTPR:0.7689301911 \tFPR:0.0160267141 \tF1:0.7608586802 \tAUC:0.8723836081 \ttest cost: 0:00:09\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1563061826 \tAcc: 0.9399898815 \tTPR:0.9659457263 \tFPR:0.1066763688 \tF1:0.9530190841 \t AUC:0.9296346787\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1482714436 \tAcc: 0.9430218271 \tTPR:0.9676235245 \tFPR:0.1011996209 \tF1:0.9556365269 \t AUC:0.9332119518\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1438722021 \tAcc: 0.9452515178 \tTPR:0.9696000007 \tFPR:0.0983561698 \tF1:0.9571415862 \t AUC:0.9356219155\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1402430287 \tAcc: 0.9464422521 \tTPR:0.9700271824 \tFPR:0.0957438559 \tF1:0.9577790071 \t AUC:0.9371416632\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1365067259 \tAcc: 0.9474721740 \tTPR:0.9705021103 \tFPR:0.0933231889 \tF1:0.9588975096 \t AUC:0.9385894607\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1855108423 \tAcc: 0.9313762626 \tTPR:0.9304869155 \tFPR:0.0669618150 \tF1:0.9444886810 \tAUC:0.9316921929 \ttest cost: 0:00:16\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1646193245 \tAcc: 0.9329008176 \tTPR:0.9415234515 \tFPR:0.0762064936 \tF1:0.9282322089 \t AUC:0.9326584789\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1571981674 \tAcc: 0.9365335052 \tTPR:0.9482746448 \tFPR:0.0732564053 \tF1:0.9321368804 \t AUC:0.9375091197\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1514429405 \tAcc: 0.9375722094 \tTPR:0.9465629585 \tFPR:0.0715531240 \tF1:0.9325637037 \t AUC:0.9375049172\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1489358832 \tAcc: 0.9389497423 \tTPR:0.9478277252 \tFPR:0.0683164348 \tF1:0.9346845994 \t AUC:0.9397556452\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1434725138 \tAcc: 0.9408827320 \tTPR:0.9511223182 \tFPR:0.0670596678 \tF1:0.9372306143 \t AUC:0.9420313252\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2085761469 \tAcc: 0.9240061544 \tTPR:0.8816350675 \tFPR:0.0390354578 \tF1:0.9135515659 \tAUC:0.9212998048 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1724868834 \tAcc: 0.9394316155 \tTPR:0.8966433417 \tFPR:0.0647227919 \tF1:0.9080899779 \tAUC:0.9151325773\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 7:  =============\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1096920287 \tAcc: 0.9607232938 \tTPR:0.9836241520 \tFPR:0.1130051123 \tF1:0.9740842256 \t AUC:0.9353095198\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.1032479085 \tAcc: 0.9624851632 \tTPR:0.9845465197 \tFPR:0.1111848257 \tF1:0.9754206965 \t AUC:0.9366808470\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.1009611949 \tAcc: 0.9630007418 \tTPR:0.9841345974 \tFPR:0.1052416179 \tF1:0.9756360069 \t AUC:0.9394464897\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0938121700 \tAcc: 0.9665244807 \tTPR:0.9865102172 \tFPR:0.0996110784 \tF1:0.9781062661 \t AUC:0.9434294953\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0958378159 \tAcc: 0.9645771513 \tTPR:0.9844147342 \tFPR:0.1043620079 \tF1:0.9767770239 \t AUC:0.9400263631\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1628628742 \tAcc: 0.9443965517 \tTPR:0.9593819222 \tFPR:0.1006393989 \tF1:0.9631191072 \tAUC:0.9293712616 \ttest cost: 0:00:05\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1178820035 \tAcc: 0.9511829096 \tTPR:0.8344610349 \tFPR:0.0275927434 \tF1:0.8237817254 \t AUC:0.9029638646\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1083361089 \tAcc: 0.9573622881 \tTPR:0.8469473245 \tFPR:0.0219606548 \tF1:0.8425126364 \t AUC:0.9120585261\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.1044659807 \tAcc: 0.9583304857 \tTPR:0.8464340744 \tFPR:0.0210832839 \tF1:0.8468719693 \t AUC:0.9124578798\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.1006151069 \tAcc: 0.9589484235 \tTPR:0.8594644370 \tFPR:0.0218600386 \tF1:0.8494251594 \t AUC:0.9184029505\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0980483395 \tAcc: 0.9610699153 \tTPR:0.8515465388 \tFPR:0.0199632719 \tF1:0.8426307523 \t AUC:0.9151572169\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1366713315 \tAcc: 0.9525529176 \tTPR:0.7777272253 \tFPR:0.0177990392 \tF1:0.8061491958 \tAUC:0.8799640931 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0967118657 \tAcc: 0.9601733577 \tTPR:0.7879689491 \tFPR:0.0187587480 \tF1:0.7781918223 \t AUC:0.8814166637\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0908996252 \tAcc: 0.9631386861 \tTPR:0.8178693083 \tFPR:0.0178385047 \tF1:0.8022776475 \t AUC:0.8975578663\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0887289831 \tAcc: 0.9638229927 \tTPR:0.8215351639 \tFPR:0.0177659301 \tF1:0.8055245355 \t AUC:0.8989236619\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0865826909 \tAcc: 0.9655565693 \tTPR:0.8201112270 \tFPR:0.0169340932 \tF1:0.8016630940 \t AUC:0.8990225859\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0832322860 \tAcc: 0.9663321168 \tTPR:0.8144496582 \tFPR:0.0155527725 \tF1:0.8025555628 \t AUC:0.8976536851\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1252864751 \tAcc: 0.9613095238 \tTPR:0.7136094914 \tFPR:0.0090426790 \tF1:0.7489396847 \tAUC:0.8477614508 \ttest cost: 0:00:09\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1542767413 \tAcc: 0.9436755952 \tTPR:0.9697705096 \tFPR:0.1144738529 \tF1:0.9586138251 \t AUC:0.9276483284\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1269874642 \tAcc: 0.9497023810 \tTPR:0.9734988832 \tFPR:0.0969902948 \tF1:0.9623369409 \t AUC:0.9382542942\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1287161148 \tAcc: 0.9463541667 \tTPR:0.9727820569 \tFPR:0.1046129116 \tF1:0.9601528904 \t AUC:0.9340845726\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1171568759 \tAcc: 0.9578869048 \tTPR:0.9802003919 \tFPR:0.0859637056 \tF1:0.9685617284 \t AUC:0.9471183432\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.1143810732 \tAcc: 0.9512648810 \tTPR:0.9739254942 \tFPR:0.1020506163 \tF1:0.9633192107 \t AUC:0.9359374389\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.1693961725 \tAcc: 0.9392361111 \tTPR:0.9453452844 \tFPR:0.0696839109 \tF1:0.9523049627 \tAUC:0.9378306868 \ttest cost: 0:00:01\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1403515252 \tAcc: 0.9589679622 \tTPR:0.9899536773 \tFPR:0.1409335704 \tF1:0.9736943992 \t AUC:0.9245100534\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.1210839796 \tAcc: 0.9587053571 \tTPR:0.9889437422 \tFPR:0.1522794078 \tF1:0.9734841099 \t AUC:0.9181274217\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.1196421478 \tAcc: 0.9621848739 \tTPR:0.9901464317 \tFPR:0.1683390023 \tF1:0.9763693190 \t AUC:0.9109037147\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.1125041548 \tAcc: 0.9621848739 \tTPR:0.9861418139 \tFPR:0.1146722325 \tF1:0.9757366866 \t AUC:0.9357347907\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.1007508592 \tAcc: 0.9698660714 \tTPR:0.9946821161 \tFPR:0.1196930054 \tF1:0.9808132582 \t AUC:0.9374945554\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.1671004886 \tAcc: 0.9342948718 \tTPR:0.9540622032 \tFPR:0.1425595238 \tF1:0.9575120232 \tAUC:0.9057513397 \ttest cost: 0:00:00\n",
      "Test =>                 \tLoss: 0.1522634684 \tAcc: 0.9463579952 \tTPR:0.8700252253 \tFPR:0.0679449103 \tF1:0.8856049947 \tAUC:0.9001357664\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 8:  =============\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1257624597 \tAcc: 0.9610688025 \tTPR:0.9913643235 \tFPR:0.1480197382 \tF1:0.9748858098 \t AUC:0.9216722927\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.1214603799 \tAcc: 0.9609375000 \tTPR:0.9845940171 \tFPR:0.1343537415 \tF1:0.9753338283 \t AUC:0.9251201378\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.1168490818 \tAcc: 0.9644170168 \tTPR:0.9889542681 \tFPR:0.1252628324 \tF1:0.9772518493 \t AUC:0.9318457178\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.1040947072 \tAcc: 0.9620535714 \tTPR:0.9891285956 \tFPR:0.1297786539 \tF1:0.9757109822 \t AUC:0.9296749709\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0991162648 \tAcc: 0.9699973739 \tTPR:0.9959772370 \tFPR:0.1434678417 \tF1:0.9809942994 \t AUC:0.9262546977\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.1789709679 \tAcc: 0.9342948718 \tTPR:0.9569187636 \tFPR:0.1278769841 \tF1:0.9566018701 \tAUC:0.9145208897 \ttest cost: 0:00:00\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1602164572 \tAcc: 0.9356947654 \tTPR:0.9486083882 \tFPR:0.0767071613 \tF1:0.9310650637 \t AUC:0.9359506135\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1547168955 \tAcc: 0.9376527506 \tTPR:0.9485663939 \tFPR:0.0715106727 \tF1:0.9325708146 \t AUC:0.9385278606\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1502415146 \tAcc: 0.9380554568 \tTPR:0.9462994813 \tFPR:0.0696465304 \tF1:0.9339439676 \t AUC:0.9383264755\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1436778123 \tAcc: 0.9395051991 \tTPR:0.9460805220 \tFPR:0.0677167020 \tF1:0.9348025680 \t AUC:0.9391819100\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1389432321 \tAcc: 0.9437822165 \tTPR:0.9519438997 \tFPR:0.0633736246 \tF1:0.9387609477 \t AUC:0.9442851375\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2026918029 \tAcc: 0.9320525615 \tTPR:0.9293533982 \tFPR:0.0662482271 \tF1:0.9254941970 \tAUC:0.9315525855 \ttest cost: 0:00:05\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0688419771 \tAcc: 0.9786458333 \tTPR:0.9927472158 \tFPR:0.1766025641 \tF1:0.9882845810 \t AUC:0.9078149002\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0637464035 \tAcc: 0.9818181818 \tTPR:0.9972363031 \tFPR:0.1906060606 \tF1:0.9898432270 \t AUC:0.9031894987\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0542453065 \tAcc: 0.9838541667 \tTPR:0.9967157181 \tFPR:0.1860606061 \tF1:0.9912957797 \t AUC:0.9057464523\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0591739396 \tAcc: 0.9828598485 \tTPR:0.9967551523 \tFPR:0.1568452381 \tF1:0.9905123539 \t AUC:0.9201180876\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0471951172 \tAcc: 0.9869791667 \tTPR:0.9983115719 \tFPR:0.1644654088 \tF1:0.9928745861 \t AUC:0.9168115816\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.1254159018 \tAcc: 0.9603365385 \tTPR:0.9729131524 \tFPR:0.2310606061 \tF1:0.9783760192 \tAUC:0.8698842870 \ttest cost: 0:00:01\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0930653596 \tAcc: 0.9623175182 \tTPR:0.8155364384 \tFPR:0.0198907118 \tF1:0.7883143599 \t AUC:0.8960386175\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0863033741 \tAcc: 0.9645985401 \tTPR:0.8257038582 \tFPR:0.0166144577 \tF1:0.8016569767 \t AUC:0.9015168972\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0826287118 \tAcc: 0.9674270073 \tTPR:0.8310317460 \tFPR:0.0154957795 \tF1:0.8168094131 \t AUC:0.9048327341\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0785942674 \tAcc: 0.9679288321 \tTPR:0.8410929946 \tFPR:0.0154572108 \tF1:0.8184891795 \t AUC:0.9107958716\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0754138551 \tAcc: 0.9692974453 \tTPR:0.8349924690 \tFPR:0.0147874293 \tF1:0.8292673497 \t AUC:0.9086314275\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1176707160 \tAcc: 0.9631164966 \tTPR:0.8068864054 \tFPR:0.0163101790 \tF1:0.7853900229 \tAUC:0.8915350221 \ttest cost: 0:00:09\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1106373611 \tAcc: 0.9558615819 \tTPR:0.8347186555 \tFPR:0.0217993250 \tF1:0.8309190967 \t AUC:0.9062255557\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.1011897928 \tAcc: 0.9600020503 \tTPR:0.8569370052 \tFPR:0.0203487387 \tF1:0.8488283929 \t AUC:0.9176827529\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0945441975 \tAcc: 0.9615084518 \tTPR:0.8627532386 \tFPR:0.0204865559 \tF1:0.8579901304 \t AUC:0.9211333413\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0901454051 \tAcc: 0.9658340168 \tTPR:0.8717256300 \tFPR:0.0172060467 \tF1:0.8776902073 \t AUC:0.9270780999\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0900595921 \tAcc: 0.9637153727 \tTPR:0.8498614066 \tFPR:0.0171446807 \tF1:0.8587876412 \t AUC:0.9163583630\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1329948243 \tAcc: 0.9553865132 \tTPR:0.8188831454 \tFPR:0.0225758865 \tF1:0.8106176976 \tAUC:0.8981536294 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1515488426 \tAcc: 0.9490373963 \tTPR:0.8969909730 \tFPR:0.0928143766 \tF1:0.8912959614 \tAUC:0.9011292828\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 9:  =============\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1119042946 \tAcc: 0.9666491597 \tTPR:0.9898405115 \tFPR:0.1356910946 \tF1:0.9789911151 \t AUC:0.9270747084\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.1033329058 \tAcc: 0.9699973739 \tTPR:0.9915384615 \tFPR:0.1087739641 \tF1:0.9806003047 \t AUC:0.9413822487\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.0962399853 \tAcc: 0.9699973739 \tTPR:0.9915037053 \tFPR:0.1279066172 \tF1:0.9810866173 \t AUC:0.9317985441\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.0850701191 \tAcc: 0.9720982143 \tTPR:0.9945995407 \tFPR:0.1237554113 \tF1:0.9827434137 \t AUC:0.9354220647\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0699676306 \tAcc: 0.9810267857 \tTPR:0.9955854834 \tFPR:0.0632794785 \tF1:0.9876834800 \t AUC:0.9661530024\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2226836681 \tAcc: 0.9368990385 \tTPR:0.9449516291 \tFPR:0.1341269841 \tF1:0.9596807743 \tAUC:0.9054123225 \ttest cost: 0:00:00\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.1007409455 \tAcc: 0.9624435241 \tTPR:0.9861876525 \tFPR:0.1391677265 \tF1:0.9764800987 \t AUC:0.9235099630\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0969901237 \tAcc: 0.9655452166 \tTPR:0.9869494622 \tFPR:0.1238802613 \tF1:0.9784845314 \t AUC:0.9315346004\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0900781032 \tAcc: 0.9679925057 \tTPR:0.9876627764 \tFPR:0.1115615922 \tF1:0.9798433802 \t AUC:0.9380319558\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0852493597 \tAcc: 0.9703949727 \tTPR:0.9890256010 \tFPR:0.1030170382 \tF1:0.9813579145 \t AUC:0.9430042814\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0868030042 \tAcc: 0.9694044033 \tTPR:0.9893689247 \tFPR:0.1145258314 \tF1:0.9808856322 \t AUC:0.9374215466\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.2118883067 \tAcc: 0.9331293706 \tTPR:0.9333667404 \tFPR:0.0658649645 \tF1:0.9560478444 \tAUC:0.9335162638 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1440072154 \tAcc: 0.9456544522 \tTPR:0.9702738122 \tFPR:0.0991766229 \tF1:0.9575258916 \t AUC:0.9355485947\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1363826456 \tAcc: 0.9474450708 \tTPR:0.9700900522 \tFPR:0.0938821532 \tF1:0.9588786729 \t AUC:0.9381039495\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1344298007 \tAcc: 0.9490983666 \tTPR:0.9717661772 \tFPR:0.0911756010 \tF1:0.9600365073 \t AUC:0.9402952881\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1311685366 \tAcc: 0.9492862822 \tTPR:0.9713883232 \tFPR:0.0906747527 \tF1:0.9600871275 \t AUC:0.9403567853\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1265934349 \tAcc: 0.9518375976 \tTPR:0.9731404915 \tFPR:0.0865117459 \tF1:0.9621020111 \t AUC:0.9433143728\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1988148081 \tAcc: 0.9330176768 \tTPR:0.9287138893 \tFPR:0.0591073040 \tF1:0.9455712774 \tAUC:0.9348032927 \ttest cost: 0:00:16\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1559314202 \tAcc: 0.9386295181 \tTPR:0.9424997479 \tFPR:0.0661895760 \tF1:0.9281911424 \t AUC:0.9381550860\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1398957549 \tAcc: 0.9420180723 \tTPR:0.9467084556 \tFPR:0.0626517345 \tF1:0.9337713252 \t AUC:0.9420283605\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1362875179 \tAcc: 0.9446536145 \tTPR:0.9497667132 \tFPR:0.0578335830 \tF1:0.9340924657 \t AUC:0.9459665651\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1354202191 \tAcc: 0.9498493976 \tTPR:0.9574388013 \tFPR:0.0587985428 \tF1:0.9427684301 \t AUC:0.9493201293\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.1366872393 \tAcc: 0.9419427711 \tTPR:0.9425360061 \tFPR:0.0566407107 \tF1:0.9340166292 \t AUC:0.9429476477\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1674750818 \tAcc: 0.9461805556 \tTPR:0.9099738826 \tFPR:0.0282423404 \tF1:0.9359658985 \tAUC:0.9408657711 \ttest cost: 0:00:01\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1244060404 \tAcc: 0.9546130952 \tTPR:0.9787555803 \tFPR:0.1002898556 \tF1:0.9664914387 \t AUC:0.9392328623\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1165679722 \tAcc: 0.9549107143 \tTPR:0.9756494648 \tFPR:0.0916588372 \tF1:0.9659643284 \t AUC:0.9419953138\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1083304341 \tAcc: 0.9560267857 \tTPR:0.9808545250 \tFPR:0.0928086988 \tF1:0.9667591034 \t AUC:0.9440229131\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1070132227 \tAcc: 0.9575892857 \tTPR:0.9793950587 \tFPR:0.0854800885 \tF1:0.9685335058 \t AUC:0.9469574851\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0958419362 \tAcc: 0.9609375000 \tTPR:0.9826292596 \tFPR:0.0847737084 \tF1:0.9710310645 \t AUC:0.9489277756\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.1883163629 \tAcc: 0.9487847222 \tTPR:0.9470679810 \tFPR:0.0554228950 \tF1:0.9597264482 \tAUC:0.9458225430 \ttest cost: 0:00:01\n",
      "Test =>                 \tLoss: 0.1978356455 \tAcc: 0.9396022727 \tTPR:0.9328148245 \tFPR:0.0685528976 \tF1:0.9513984485 \tAUC:0.9320840386\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 10:  =============\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1364208726 \tAcc: 0.9457831325 \tTPR:0.9531247480 \tFPR:0.0584845221 \tF1:0.9383424617 \t AUC:0.9473201130\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1348333288 \tAcc: 0.9491716867 \tTPR:0.9532662130 \tFPR:0.0547303170 \tF1:0.9413217762 \t AUC:0.9492679480\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1285312450 \tAcc: 0.9514307229 \tTPR:0.9537024093 \tFPR:0.0518509397 \tF1:0.9427477738 \t AUC:0.9509257348\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1311563475 \tAcc: 0.9476656627 \tTPR:0.9622973973 \tFPR:0.0637835182 \tF1:0.9410108148 \t AUC:0.9492569395\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.1198426683 \tAcc: 0.9498493976 \tTPR:0.9566817290 \tFPR:0.0567876077 \tF1:0.9418422106 \t AUC:0.9499470607\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1637203388 \tAcc: 0.9470486111 \tTPR:0.9209375298 \tFPR:0.0328678626 \tF1:0.9351420745 \tAUC:0.9440348336 \ttest cost: 0:00:01\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1072678124 \tAcc: 0.9676339286 \tTPR:0.9914743590 \tFPR:0.1190109494 \tF1:0.9788621927 \t AUC:0.9362317048\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.0927251490 \tAcc: 0.9709821429 \tTPR:0.9945872580 \tFPR:0.1221513605 \tF1:0.9822109892 \t AUC:0.9362179487\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.0797953002 \tAcc: 0.9754464286 \tTPR:0.9931137110 \tFPR:0.0929421769 \tF1:0.9845718813 \t AUC:0.9500857671\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.0689391508 \tAcc: 0.9832589286 \tTPR:0.9987684729 \tFPR:0.0699559369 \tF1:0.9893575483 \t AUC:0.9644062680\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0789134090 \tAcc: 0.9754464286 \tTPR:0.9944850718 \tFPR:0.0963435374 \tF1:0.9843385290 \t AUC:0.9490707672\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2057188312 \tAcc: 0.9342948718 \tTPR:0.9528099986 \tFPR:0.1277146465 \tF1:0.9570554045 \tAUC:0.9125476761 \ttest cost: 0:00:00\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1344960363 \tAcc: 0.9497023810 \tTPR:0.9774013536 \tFPR:0.1095867922 \tF1:0.9628734584 \t AUC:0.9339072807\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1108961671 \tAcc: 0.9583333333 \tTPR:0.9792392716 \tFPR:0.0827208010 \tF1:0.9688439285 \t AUC:0.9482592353\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1075838501 \tAcc: 0.9549851190 \tTPR:0.9791025024 \tFPR:0.0972931500 \tF1:0.9674435047 \t AUC:0.9409046762\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.1045577525 \tAcc: 0.9594494048 \tTPR:0.9788132795 \tFPR:0.0847005226 \tF1:0.9699996991 \t AUC:0.9470563785\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0935416351 \tAcc: 0.9668898810 \tTPR:0.9835781570 \tFPR:0.0706812581 \tF1:0.9751254889 \t AUC:0.9564484495\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.1864984548 \tAcc: 0.9428970411 \tTPR:0.9550009360 \tFPR:0.0777354590 \tF1:0.9548389149 \tAUC:0.9386327385 \ttest cost: 0:00:01\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1496780911 \tAcc: 0.9398023685 \tTPR:0.9530685538 \tFPR:0.0700308888 \tF1:0.9357373228 \t AUC:0.9415188325\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1403505858 \tAcc: 0.9419214362 \tTPR:0.9509495536 \tFPR:0.0647108675 \tF1:0.9376552122 \t AUC:0.9431193431\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1312831687 \tAcc: 0.9477204053 \tTPR:0.9552431706 \tFPR:0.0590658329 \tF1:0.9436418914 \t AUC:0.9480886688\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1340432869 \tAcc: 0.9452236269 \tTPR:0.9547926585 \tFPR:0.0637357999 \tF1:0.9411382768 \t AUC:0.9455284293\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1245997386 \tAcc: 0.9482841939 \tTPR:0.9556023302 \tFPR:0.0581590632 \tF1:0.9438571248 \t AUC:0.9487216335\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2204092976 \tAcc: 0.9247546574 \tTPR:0.8965814015 \tFPR:0.0517450928 \tF1:0.9154873342 \tAUC:0.9224181544 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0928065071 \tAcc: 0.9663475330 \tTPR:0.9878576975 \tFPR:0.1260798856 \tF1:0.9790134948 \t AUC:0.9308889059\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0851386775 \tAcc: 0.9685617470 \tTPR:0.9876422439 \tFPR:0.1062730237 \tF1:0.9801154259 \t AUC:0.9406659428\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0801276280 \tAcc: 0.9722326807 \tTPR:0.9901116089 \tFPR:0.1021977103 \tF1:0.9826502912 \t AUC:0.9439569493\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0762932293 \tAcc: 0.9727929575 \tTPR:0.9902949469 \tFPR:0.0960452198 \tF1:0.9829556015 \t AUC:0.9471248635\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0725229920 \tAcc: 0.9731246414 \tTPR:0.9908089234 \tFPR:0.1026242865 \tF1:0.9831048021 \t AUC:0.9441118152\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1858367069 \tAcc: 0.9429632867 \tTPR:0.9582070336 \tFPR:0.1189600306 \tF1:0.9636598970 \tAUC:0.9194763432 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1924367259 \tAcc: 0.9383916936 \tTPR:0.9367073799 \tFPR:0.0818046183 \tF1:0.9452367250 \tAUC:0.9274219492\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 11:  =============\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0847417284 \tAcc: 0.9699243402 \tTPR:0.9883941667 \tFPR:0.1030267064 \tF1:0.9811057277 \t AUC:0.9427381308\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0784058792 \tAcc: 0.9719503012 \tTPR:0.9901564494 \tFPR:0.1031041917 \tF1:0.9824435696 \t AUC:0.9435261289\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0767171576 \tAcc: 0.9712869334 \tTPR:0.9885590619 \tFPR:0.0985642788 \tF1:0.9819066232 \t AUC:0.9449973916\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0699549971 \tAcc: 0.9753836776 \tTPR:0.9914042146 \tFPR:0.0917856491 \tF1:0.9846850956 \t AUC:0.9498092828\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0672239913 \tAcc: 0.9764683735 \tTPR:0.9914599028 \tFPR:0.0858827812 \tF1:0.9852411341 \t AUC:0.9527756603\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.2258386591 \tAcc: 0.9371940559 \tTPR:0.9384307744 \tFPR:0.0658706765 \tF1:0.9590696142 \tAUC:0.9367674812 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.1021043210 \tAcc: 0.9640875371 \tTPR:0.9860598833 \tFPR:0.1181996375 \tF1:0.9766010459 \t AUC:0.9339301229\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0955504663 \tAcc: 0.9671216617 \tTPR:0.9866490748 \tFPR:0.0974034455 \tF1:0.9784061130 \t AUC:0.9446228146\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0894225572 \tAcc: 0.9694658754 \tTPR:0.9879702132 \tFPR:0.0934883201 \tF1:0.9800991926 \t AUC:0.9472409465\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0893290248 \tAcc: 0.9673998516 \tTPR:0.9868890215 \tFPR:0.1006262337 \tF1:0.9787563612 \t AUC:0.9431313939\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0781667788 \tAcc: 0.9726594955 \tTPR:0.9879526130 \tFPR:0.0796827031 \tF1:0.9818544045 \t AUC:0.9541349550\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.2546673983 \tAcc: 0.9259012539 \tTPR:0.9215628044 \tFPR:0.0571628025 \tF1:0.9493853773 \tAUC:0.9325589627 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0849972535 \tAcc: 0.9655565693 \tTPR:0.8358063956 \tFPR:0.0172683631 \tF1:0.8224670804 \t AUC:0.9074310281\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0819155244 \tAcc: 0.9673357664 \tTPR:0.8360329938 \tFPR:0.0158512369 \tF1:0.8200773760 \t AUC:0.9080044719\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0769987286 \tAcc: 0.9682937956 \tTPR:0.8323293940 \tFPR:0.0149361751 \tF1:0.8232562155 \t AUC:0.9064341875\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0753435234 \tAcc: 0.9689324818 \tTPR:0.8399385934 \tFPR:0.0155934715 \tF1:0.8259646304 \t AUC:0.9106243480\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0689388398 \tAcc: 0.9721259124 \tTPR:0.8624672165 \tFPR:0.0135555788 \tF1:0.8476202740 \t AUC:0.9227057760\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1397929282 \tAcc: 0.9556405896 \tTPR:0.8341715258 \tFPR:0.0287367602 \tF1:0.7689343103 \tAUC:0.9015737381 \ttest cost: 0:00:09\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1298156317 \tAcc: 0.9514880952 \tTPR:0.9728452161 \tFPR:0.0926259257 \tF1:0.9636716372 \t AUC:0.9401096452\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1046980213 \tAcc: 0.9590773810 \tTPR:0.9785355945 \tFPR:0.0874430464 \tF1:0.9699958715 \t AUC:0.9455462741\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.1052581049 \tAcc: 0.9609375000 \tTPR:0.9803803751 \tFPR:0.0837180676 \tF1:0.9715219521 \t AUC:0.9483311537\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0981073810 \tAcc: 0.9642857143 \tTPR:0.9818423702 \tFPR:0.0694419269 \tF1:0.9727828696 \t AUC:0.9562002216\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0924682538 \tAcc: 0.9631696429 \tTPR:0.9829669429 \tFPR:0.0786454286 \tF1:0.9726834835 \t AUC:0.9521607571\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2198766846 \tAcc: 0.9314236111 \tTPR:0.9218693960 \tFPR:0.0463326797 \tF1:0.9446121535 \tAUC:0.9377683582 \ttest cost: 0:00:01\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1362612976 \tAcc: 0.9472891566 \tTPR:0.9564804836 \tFPR:0.0579589465 \tF1:0.9408213525 \t AUC:0.9492607685\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1267600043 \tAcc: 0.9509789157 \tTPR:0.9607476300 \tFPR:0.0560413020 \tF1:0.9433379470 \t AUC:0.9523531640\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1277099970 \tAcc: 0.9491716867 \tTPR:0.9584005763 \tFPR:0.0572063937 \tF1:0.9426097093 \t AUC:0.9505970913\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1142245687 \tAcc: 0.9529367470 \tTPR:0.9565003715 \tFPR:0.0485932866 \tF1:0.9461831423 \t AUC:0.9539535425\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.1090483527 \tAcc: 0.9555722892 \tTPR:0.9616433050 \tFPR:0.0489235136 \tF1:0.9512751606 \t AUC:0.9563598957\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1813007001 \tAcc: 0.9399305556 \tTPR:0.8977907782 \tFPR:0.0268358663 \tF1:0.9254648407 \tAUC:0.9354774560 \ttest cost: 0:00:01\n",
      "Test =>                 \tLoss: 0.2042952740 \tAcc: 0.9380180132 \tTPR:0.9027650558 \tFPR:0.0449877570 \tF1:0.9094932592 \tAUC:0.9288291992\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 12:  =============\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0721346892 \tAcc: 0.9735011474 \tTPR:0.9901000646 \tFPR:0.0885249456 \tF1:0.9832936715 \t AUC:0.9507875595\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0636885621 \tAcc: 0.9775037651 \tTPR:0.9914063278 \tFPR:0.0792629392 \tF1:0.9859137392 \t AUC:0.9560716943\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0669459744 \tAcc: 0.9761859940 \tTPR:0.9896562723 \tFPR:0.0821306873 \tF1:0.9848994430 \t AUC:0.9537627925\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0603790023 \tAcc: 0.9781133462 \tTPR:0.9911551574 \tFPR:0.0717180772 \tF1:0.9861069672 \t AUC:0.9597051793\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0601228278 \tAcc: 0.9778264845 \tTPR:0.9916428039 \tFPR:0.0778197539 \tF1:0.9859767084 \t AUC:0.9569115250\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.2093933066 \tAcc: 0.9392482517 \tTPR:0.9452317351 \tFPR:0.0921817741 \tF1:0.9607469114 \tAUC:0.9265798155 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1126540021 \tAcc: 0.9549851190 \tTPR:0.9785999731 \tFPR:0.0940103712 \tF1:0.9667247228 \t AUC:0.9422948009\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.1016539818 \tAcc: 0.9581101190 \tTPR:0.9762790938 \tFPR:0.0845614421 \tF1:0.9688332750 \t AUC:0.9458588258\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0905621830 \tAcc: 0.9683779762 \tTPR:0.9860567357 \tFPR:0.0709039142 \tF1:0.9767756721 \t AUC:0.9575764107\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0867214589 \tAcc: 0.9665178571 \tTPR:0.9823391244 \tFPR:0.0694135327 \tF1:0.9754455638 \t AUC:0.9564627959\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0829846221 \tAcc: 0.9687500000 \tTPR:0.9823807726 \tFPR:0.0607714755 \tF1:0.9762591753 \t AUC:0.9608046486\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2129300192 \tAcc: 0.9444444444 \tTPR:0.9419636620 \tFPR:0.0476563288 \tF1:0.9567364075 \tAUC:0.9471536666 \ttest cost: 0:00:01\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.1083121614 \tAcc: 0.9570860671 \tTPR:0.8530431433 \tFPR:0.0234243858 \tF1:0.8470227009 \t AUC:0.9146012246\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0999117588 \tAcc: 0.9594780846 \tTPR:0.8517973082 \tFPR:0.0205003757 \tF1:0.8492347580 \t AUC:0.9154385474\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0934316964 \tAcc: 0.9638064972 \tTPR:0.8684986859 \tFPR:0.0188170711 \tF1:0.8724505302 \t AUC:0.9246545449\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0891957124 \tAcc: 0.9643304629 \tTPR:0.8647608042 \tFPR:0.0177565402 \tF1:0.8659455905 \t AUC:0.9233105750\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0846063014 \tAcc: 0.9683086158 \tTPR:0.8820295367 \tFPR:0.0158907025 \tF1:0.8838187675 \t AUC:0.9329023201\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1437379969 \tAcc: 0.9551004720 \tTPR:0.7888836675 \tFPR:0.0159280200 \tF1:0.8030283012 \tAUC:0.8864778237 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0783906715 \tAcc: 0.9680656934 \tTPR:0.8400243309 \tFPR:0.0149946384 \tF1:0.8268446693 \t AUC:0.9108459496\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0732624495 \tAcc: 0.9705748175 \tTPR:0.8508318851 \tFPR:0.0140900141 \tF1:0.8348197006 \t AUC:0.9169280892\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0702666681 \tAcc: 0.9719434307 \tTPR:0.8552311436 \tFPR:0.0131908607 \tF1:0.8472130252 \t AUC:0.9189551202\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0649550422 \tAcc: 0.9738138686 \tTPR:0.8712889584 \tFPR:0.0130547912 \tF1:0.8550895631 \t AUC:0.9275779381\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0657105025 \tAcc: 0.9734945255 \tTPR:0.8770588576 \tFPR:0.0128843985 \tF1:0.8609373399 \t AUC:0.9307110227\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1124664082 \tAcc: 0.9601403061 \tTPR:0.8038575748 \tFPR:0.0190917375 \tF1:0.7749416528 \tAUC:0.8889297069 \ttest cost: 0:00:09\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0971272328 \tAcc: 0.9646698813 \tTPR:0.9841067151 \tFPR:0.0982626502 \tF1:0.9766686535 \t AUC:0.9429220325\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0872851095 \tAcc: 0.9687500000 \tTPR:0.9870195032 \tFPR:0.0905661269 \tF1:0.9795269956 \t AUC:0.9482266881\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0792120109 \tAcc: 0.9709755193 \tTPR:0.9880321726 \tFPR:0.0849643350 \tF1:0.9808947205 \t AUC:0.9515339188\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0803719520 \tAcc: 0.9706713650 \tTPR:0.9873649851 \tFPR:0.0889049954 \tF1:0.9808410153 \t AUC:0.9492299948\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0730685425 \tAcc: 0.9741283383 \tTPR:0.9897524566 \tFPR:0.0805991239 \tF1:0.9830669718 \t AUC:0.9545766663\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1968445548 \tAcc: 0.9448471787 \tTPR:0.9572806308 \tFPR:0.0990800196 \tF1:0.9629308284 \tAUC:0.9291003056 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1750744571 \tAcc: 0.9487561306 \tTPR:0.8874434541 \tFPR:0.0547875760 \tF1:0.8916768203 \tAUC:0.9156482637\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 13:  =============\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0911062810 \tAcc: 0.9626532030 \tTPR:0.8775747166 \tFPR:0.0204268649 \tF1:0.8677223979 \t AUC:0.9284005189\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0850720727 \tAcc: 0.9646835703 \tTPR:0.8708667588 \tFPR:0.0185793202 \tF1:0.8696320130 \t AUC:0.9259608110\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0819072922 \tAcc: 0.9680380900 \tTPR:0.8840894824 \tFPR:0.0165121599 \tF1:0.8786025855 \t AUC:0.9336244820\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0758223518 \tAcc: 0.9693650902 \tTPR:0.8880918752 \tFPR:0.0160778092 \tF1:0.8822779536 \t AUC:0.9356891122\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1547733950 \tAcc: 0.9543585526 \tTPR:0.7756540974 \tFPR:0.0142902758 \tF1:0.8170743646 \tAUC:0.8791862714 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0744794602 \tAcc: 0.9694343066 \tTPR:0.8447532152 \tFPR:0.0151472525 \tF1:0.8288557769 \t AUC:0.9128275357\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0685938665 \tAcc: 0.9728102190 \tTPR:0.8630784382 \tFPR:0.0126214547 \tF1:0.8438787627 \t AUC:0.9231695209\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0663214511 \tAcc: 0.9726277372 \tTPR:0.8695174371 \tFPR:0.0141185368 \tF1:0.8544926598 \t AUC:0.9254327591\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0599224606 \tAcc: 0.9763686131 \tTPR:0.8883854183 \tFPR:0.0120191597 \tF1:0.8735486782 \t AUC:0.9369337123\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0584677953 \tAcc: 0.9765054745 \tTPR:0.8844056309 \tFPR:0.0117339113 \tF1:0.8699254479 \t AUC:0.9348649734\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1271200886 \tAcc: 0.9619472789 \tTPR:0.7352540222 \tFPR:0.0116903381 \tF1:0.7595043155 \tAUC:0.8580791011 \ttest cost: 0:00:09\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.1001880271 \tAcc: 0.9630208333 \tTPR:0.9765850339 \tFPR:0.1997863248 \tF1:0.9783682073 \t AUC:0.8868986841\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0565710725 \tAcc: 0.9828125000 \tTPR:0.9960907181 \tFPR:0.1678571429 \tF1:0.9905609463 \t AUC:0.9139771704\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0542782004 \tAcc: 0.9833333333 \tTPR:0.9960524039 \tFPR:0.1809941520 \tF1:0.9909992216 \t AUC:0.9074252418\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0556727450 \tAcc: 0.9828598485 \tTPR:0.9943584828 \tFPR:0.1547169811 \tF1:0.9904941618 \t AUC:0.9194481978\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0425804211 \tAcc: 0.9880208333 \tTPR:0.9977586207 \tFPR:0.1367283951 \tF1:0.9934433410 \t AUC:0.9303905917\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.1495801576 \tAcc: 0.9528044872 \tTPR:0.9621136091 \tFPR:0.1515151515 \tF1:0.9739314889 \tAUC:0.9046959205 \ttest cost: 0:00:01\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1487933027 \tAcc: 0.9416415663 \tTPR:0.9511063158 \tFPR:0.0612065109 \tF1:0.9328337317 \t AUC:0.9449499025\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1252522056 \tAcc: 0.9476656627 \tTPR:0.9526681399 \tFPR:0.0561107838 \tF1:0.9382153015 \t AUC:0.9482786780\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1201478017 \tAcc: 0.9503012048 \tTPR:0.9513246931 \tFPR:0.0516839006 \tF1:0.9420226632 \t AUC:0.9498203963\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1165959172 \tAcc: 0.9521837349 \tTPR:0.9547937593 \tFPR:0.0520441588 \tF1:0.9457071160 \t AUC:0.9513748002\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.1070634099 \tAcc: 0.9566265060 \tTPR:0.9595118436 \tFPR:0.0440894631 \tF1:0.9509830695 \t AUC:0.9577111902\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1665713734 \tAcc: 0.9470486111 \tTPR:0.9152412647 \tFPR:0.0229248164 \tF1:0.9360103025 \tAUC:0.9461582242 \ttest cost: 0:00:01\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1130615439 \tAcc: 0.9564732143 \tTPR:0.9757742740 \tFPR:0.0856254196 \tF1:0.9681722119 \t AUC:0.9450744272\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0992178486 \tAcc: 0.9578869048 \tTPR:0.9792809757 \tFPR:0.0875920046 \tF1:0.9687303553 \t AUC:0.9458444856\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0849260588 \tAcc: 0.9676339286 \tTPR:0.9849176524 \tFPR:0.0699054913 \tF1:0.9756017980 \t AUC:0.9575060805\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0833048960 \tAcc: 0.9672619048 \tTPR:0.9826401676 \tFPR:0.0634276273 \tF1:0.9754533320 \t AUC:0.9596062702\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0745385481 \tAcc: 0.9702380952 \tTPR:0.9858322309 \tFPR:0.0627303123 \tF1:0.9778318903 \t AUC:0.9615509593\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.1967746793 \tAcc: 0.9513888889 \tTPR:0.9518835601 \tFPR:0.0449380866 \tF1:0.9624909347 \tAUC:0.9534727367 \ttest cost: 0:00:01\n",
      "Test =>                 \tLoss: 0.1589639388 \tAcc: 0.9535095637 \tTPR:0.8680293107 \tFPR:0.0490717337 \tF1:0.8898022812 \tAUC:0.9083184508\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 14:  =============\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0784325731 \tAcc: 0.9696327684 \tTPR:0.8950765728 \tFPR:0.0170378435 \tF1:0.8946809074 \t AUC:0.9384198022\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0712029519 \tAcc: 0.9717514124 \tTPR:0.9072157206 \tFPR:0.0157257415 \tF1:0.8985191129 \t AUC:0.9457449895\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0702619707 \tAcc: 0.9727139147 \tTPR:0.9057740439 \tFPR:0.0146427697 \tF1:0.9028424410 \t AUC:0.9452979498\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0663258352 \tAcc: 0.9738700565 \tTPR:0.9132698046 \tFPR:0.0139005707 \tF1:0.9023343584 \t AUC:0.9494382243\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0599109036 \tAcc: 0.9748411017 \tTPR:0.9100744516 \tFPR:0.0132943298 \tF1:0.9092872568 \t AUC:0.9482626876\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1721362823 \tAcc: 0.9507472826 \tTPR:0.8694548872 \tFPR:0.0361844861 \tF1:0.8076119568 \tAUC:0.9162029320 \ttest cost: 0:00:05\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0688286309 \tAcc: 0.9796875000 \tTPR:0.9909987122 \tFPR:0.1595663265 \tF1:0.9887037529 \t AUC:0.9156737361\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0521572689 \tAcc: 0.9859375000 \tTPR:0.9971963361 \tFPR:0.1692528736 \tF1:0.9923143998 \t AUC:0.9139233922\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0405871168 \tAcc: 0.9875000000 \tTPR:0.9983297414 \tFPR:0.1352891156 \tF1:0.9931179448 \t AUC:0.9317396786\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0410958858 \tAcc: 0.9865056818 \tTPR:0.9962243121 \tFPR:0.1342948718 \tF1:0.9926495231 \t AUC:0.9315484085\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0409511383 \tAcc: 0.9890625000 \tTPR:0.9982332908 \tFPR:0.1371345029 \tF1:0.9940447562 \t AUC:0.9305029016\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.1401292003 \tAcc: 0.9565705128 \tTPR:0.9715843214 \tFPR:0.2966666667 \tF1:0.9764752584 \tAUC:0.8375155138 \ttest cost: 0:00:01\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.1133583015 \tAcc: 0.9531250000 \tTPR:0.9754659235 \tFPR:0.0886500647 \tF1:0.9646917275 \t AUC:0.9434079294\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0900700857 \tAcc: 0.9654017857 \tTPR:0.9851108567 \tFPR:0.0742230653 \tF1:0.9742647536 \t AUC:0.9554438957\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0832108154 \tAcc: 0.9680059524 \tTPR:0.9848627497 \tFPR:0.0658285002 \tF1:0.9764478015 \t AUC:0.9595171247\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0796303470 \tAcc: 0.9675595238 \tTPR:0.9851281982 \tFPR:0.0762754937 \tF1:0.9762620329 \t AUC:0.9544263523\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0783452767 \tAcc: 0.9691220238 \tTPR:0.9838745377 \tFPR:0.0638744688 \tF1:0.9769342185 \t AUC:0.9600000345\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.1733332709 \tAcc: 0.9562575483 \tTPR:0.9581760516 \tFPR:0.0498900791 \tF1:0.9659754978 \tAUC:0.9541429863 \ttest cost: 0:00:01\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1323347114 \tAcc: 0.9502891009 \tTPR:0.9725295585 \tFPR:0.0893650118 \tF1:0.9608866318 \t AUC:0.9415822734\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1233329670 \tAcc: 0.9531656548 \tTPR:0.9738139208 \tFPR:0.0852998142 \tF1:0.9632770765 \t AUC:0.9442570533\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1185588686 \tAcc: 0.9542190662 \tTPR:0.9736903177 \tFPR:0.0811493357 \tF1:0.9640585274 \t AUC:0.9462704910\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1153992734 \tAcc: 0.9561976005 \tTPR:0.9752601240 \tFPR:0.0786162946 \tF1:0.9654042644 \t AUC:0.9483219147\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1127141733 \tAcc: 0.9571209164 \tTPR:0.9758757571 \tFPR:0.0772604822 \tF1:0.9661403111 \t AUC:0.9493076375\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1813100018 \tAcc: 0.9363636364 \tTPR:0.9313157646 \tFPR:0.0536132445 \tF1:0.9481862616 \tAUC:0.9388512601 \ttest cost: 0:00:16\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1376213039 \tAcc: 0.9455541237 \tTPR:0.9562238817 \tFPR:0.0647263545 \tF1:0.9425847453 \t AUC:0.9457487636\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1320639724 \tAcc: 0.9475593228 \tTPR:0.9560156336 \tFPR:0.0604283643 \tF1:0.9439372075 \t AUC:0.9477936346\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1250209061 \tAcc: 0.9503699342 \tTPR:0.9595404455 \tFPR:0.0586125443 \tF1:0.9478212951 \t AUC:0.9504639506\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1224865444 \tAcc: 0.9500561011 \tTPR:0.9591309101 \tFPR:0.0572182939 \tF1:0.9459463747 \t AUC:0.9509563081\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1147740809 \tAcc: 0.9538415393 \tTPR:0.9598901164 \tFPR:0.0509632049 \tF1:0.9499512714 \t AUC:0.9544634557\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.1667401976 \tAcc: 0.9428226880 \tTPR:0.9400087322 \tFPR:0.0532076714 \tF1:0.9376491318 \tAUC:0.9434005304 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1667297906 \tAcc: 0.9485523336 \tTPR:0.9341079514 \tFPR:0.0979124295 \tF1:0.9271796213 \tAUC:0.9180226445\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 15:  =============\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0472728144 \tAcc: 0.9838541667 \tTPR:0.9930214321 \tFPR:0.1439655172 \tF1:0.9911167159 \t AUC:0.9244076373\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0411539817 \tAcc: 0.9885416667 \tTPR:0.9977560164 \tFPR:0.1191798942 \tF1:0.9936692207 \t AUC:0.9391633953\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0335517350 \tAcc: 0.9916666667 \tTPR:0.9982098765 \tFPR:0.0997284879 \tF1:0.9953576428 \t AUC:0.9491935858\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0322717128 \tAcc: 0.9901515152 \tTPR:0.9976653015 \tFPR:0.0984848485 \tF1:0.9945720697 \t AUC:0.9497681948\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0261294302 \tAcc: 0.9911458333 \tTPR:0.9977394636 \tFPR:0.0925287356 \tF1:0.9952114278 \t AUC:0.9525663892\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.1425642906 \tAcc: 0.9663461538 \tTPR:0.9846367377 \tFPR:0.3106060606 \tF1:0.9816756225 \tAUC:0.8363289056 \ttest cost: 0:00:01\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0993621561 \tAcc: 0.9593750000 \tTPR:0.9796867544 \tFPR:0.0842564940 \tF1:0.9694101981 \t AUC:0.9477151302\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0837156696 \tAcc: 0.9661458333 \tTPR:0.9838104714 \tFPR:0.0718722065 \tF1:0.9749760938 \t AUC:0.9559691324\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0706638872 \tAcc: 0.9720982143 \tTPR:0.9833659874 \tFPR:0.0506850011 \tF1:0.9792758054 \t AUC:0.9663404932\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0688489946 \tAcc: 0.9732142857 \tTPR:0.9883378901 \tFPR:0.0594656005 \tF1:0.9801706361 \t AUC:0.9644361448\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0595996480 \tAcc: 0.9754464286 \tTPR:0.9882267662 \tFPR:0.0489421293 \tF1:0.9812221684 \t AUC:0.9696423184\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.1920515367 \tAcc: 0.9510492150 \tTPR:0.9483925886 \tFPR:0.0476649893 \tF1:0.9605681420 \tAUC:0.9503637997 \ttest cost: 0:00:01\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0666103913 \tAcc: 0.9733375478 \tTPR:0.9072827753 \tFPR:0.0146573790 \tF1:0.9007001130 \t AUC:0.9461813706\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0616148471 \tAcc: 0.9751913614 \tTPR:0.9192190264 \tFPR:0.0136178683 \tF1:0.9137590522 \t AUC:0.9526861584\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0558938813 \tAcc: 0.9772217286 \tTPR:0.9291553550 \tFPR:0.0130014175 \tF1:0.9232739852 \t AUC:0.9578757055\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0535142376 \tAcc: 0.9787195872 \tTPR:0.9203511100 \tFPR:0.0106811151 \tF1:0.9204945490 \t AUC:0.9546087222\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0530054462 \tAcc: 0.9787252825 \tTPR:0.9268697875 \tFPR:0.0118349884 \tF1:0.9224976009 \t AUC:0.9574138156\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1621667876 \tAcc: 0.9553060641 \tTPR:0.8175673559 \tFPR:0.0220402193 \tF1:0.8153222283 \tAUC:0.8977635683 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1408634503 \tAcc: 0.9469126506 \tTPR:0.9520675164 \tFPR:0.0576206567 \tF1:0.9392228344 \t AUC:0.9472234298\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1272769382 \tAcc: 0.9532379518 \tTPR:0.9623166962 \tFPR:0.0532230639 \tF1:0.9462257112 \t AUC:0.9545468162\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1130043038 \tAcc: 0.9582078313 \tTPR:0.9560589494 \tFPR:0.0412954604 \tF1:0.9505654598 \t AUC:0.9573817445\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1039408131 \tAcc: 0.9603915663 \tTPR:0.9633801435 \tFPR:0.0413386845 \tF1:0.9542947114 \t AUC:0.9610207295\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.1022431656 \tAcc: 0.9570783133 \tTPR:0.9624334010 \tFPR:0.0468829816 \tF1:0.9511661246 \t AUC:0.9577752097\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1562615165 \tAcc: 0.9425347222 \tTPR:0.9169405067 \tFPR:0.0322214684 \tF1:0.9332636514 \tAUC:0.9423595191 \ttest cost: 0:00:01\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0929276907 \tAcc: 0.9663390208 \tTPR:0.9856537486 \tFPR:0.0981502129 \tF1:0.9778582246 \t AUC:0.9437304192\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0852173825 \tAcc: 0.9693991098 \tTPR:0.9872695667 \tFPR:0.0922058844 \tF1:0.9800429632 \t AUC:0.9475128971\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0803778058 \tAcc: 0.9703931751 \tTPR:0.9868457270 \tFPR:0.0839454392 \tF1:0.9805517653 \t AUC:0.9514501439\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0708786108 \tAcc: 0.9746847181 \tTPR:0.9900263442 \tFPR:0.0768634432 \tF1:0.9834475449 \t AUC:0.9565814505\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0697392368 \tAcc: 0.9755192878 \tTPR:0.9898534890 \tFPR:0.0728038083 \tF1:0.9837801190 \t AUC:0.9585097414\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.2136299170 \tAcc: 0.9420258621 \tTPR:0.9494022060 \tFPR:0.0787416032 \tF1:0.9606535618 \tAUC:0.9353303014 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1733348097 \tAcc: 0.9514524034 \tTPR:0.9233878790 \tFPR:0.0982548682 \tF1:0.9302966412 \tAUC:0.9124292188\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 16:  =============\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0667863459 \tAcc: 0.9734945255 \tTPR:0.8714181439 \tFPR:0.0137803479 \tF1:0.8527586838 \t AUC:0.9275751747\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0604331468 \tAcc: 0.9765510949 \tTPR:0.8790377708 \tFPR:0.0113897420 \tF1:0.8687569800 \t AUC:0.9323775304\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0566647542 \tAcc: 0.9781478102 \tTPR:0.8952908122 \tFPR:0.0106185307 \tF1:0.8778189154 \t AUC:0.9408425412\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0536365553 \tAcc: 0.9784671533 \tTPR:0.9051819024 \tFPR:0.0114578760 \tF1:0.8875199619 \t AUC:0.9452148710\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0504024518 \tAcc: 0.9799726277 \tTPR:0.9026960955 \tFPR:0.0105640293 \tF1:0.8907066336 \t AUC:0.9447530869\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1358224837 \tAcc: 0.9622661565 \tTPR:0.8060860058 \tFPR:0.0186778691 \tF1:0.7857333302 \tAUC:0.8909919845 \ttest cost: 0:00:09\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0825593690 \tAcc: 0.9683734940 \tTPR:0.9857378924 \tFPR:0.1046611020 \tF1:0.9800062887 \t AUC:0.9405383952\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0697803692 \tAcc: 0.9751506024 \tTPR:0.9907806244 \tFPR:0.0942582016 \tF1:0.9844118982 \t AUC:0.9482612114\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0651816058 \tAcc: 0.9755719306 \tTPR:0.9910028679 \tFPR:0.0870939385 \tF1:0.9846135134 \t AUC:0.9519544647\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0563518701 \tAcc: 0.9800406985 \tTPR:0.9921850939 \tFPR:0.0689558016 \tF1:0.9872844166 \t AUC:0.9616146462\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0551136562 \tAcc: 0.9800451807 \tTPR:0.9921405655 \tFPR:0.0736261970 \tF1:0.9874039450 \t AUC:0.9592333678\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.2028179604 \tAcc: 0.9429632867 \tTPR:0.9475514325 \tFPR:0.0778911260 \tF1:0.9632877558 \tAUC:0.9348301533 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1166538862 \tAcc: 0.9631696429 \tTPR:0.9891033002 \tFPR:0.1348497732 \tF1:0.9770199405 \t AUC:0.9271267635\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.0912408348 \tAcc: 0.9698660714 \tTPR:0.9945995407 \tFPR:0.1101628530 \tF1:0.9807235474 \t AUC:0.9422183438\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.0768913202 \tAcc: 0.9776785714 \tTPR:0.9956253006 \tFPR:0.0888901773 \tF1:0.9858403954 \t AUC:0.9533675617\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.0831063630 \tAcc: 0.9755777311 \tTPR:0.9944891418 \tFPR:0.0905792620 \tF1:0.9843705229 \t AUC:0.9519549399\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0745492673 \tAcc: 0.9755777311 \tTPR:0.9903927025 \tFPR:0.0829377963 \tF1:0.9845264543 \t AUC:0.9537274531\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2923413577 \tAcc: 0.9226762821 \tTPR:0.9363706510 \tFPR:0.1202380952 \tF1:0.9474113530 \tAUC:0.9080662779 \ttest cost: 0:00:00\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1427620600 \tAcc: 0.9444277108 \tTPR:0.9552177162 \tFPR:0.0627596733 \tF1:0.9364107600 \t AUC:0.9462290215\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1201277572 \tAcc: 0.9484186747 \tTPR:0.9584752597 \tFPR:0.0596784432 \tF1:0.9413814874 \t AUC:0.9493984083\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.1062631839 \tAcc: 0.9574548193 \tTPR:0.9639696236 \tFPR:0.0484823647 \tF1:0.9492959655 \t AUC:0.9577436294\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.1032838843 \tAcc: 0.9578313253 \tTPR:0.9626194205 \tFPR:0.0458692416 \tF1:0.9495784036 \t AUC:0.9583750895\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0957703396 \tAcc: 0.9623493976 \tTPR:0.9679005269 \tFPR:0.0415164177 \tF1:0.9564733322 \t AUC:0.9631920546\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1656711081 \tAcc: 0.9477430556 \tTPR:0.9291681926 \tFPR:0.0344901628 \tF1:0.9377041939 \tAUC:0.9473390149 \ttest cost: 0:00:01\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1232813425 \tAcc: 0.9533228534 \tTPR:0.9746392794 \tFPR:0.0843261557 \tF1:0.9634553620 \t AUC:0.9451565619\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1156790146 \tAcc: 0.9556790257 \tTPR:0.9753794746 \tFPR:0.0797435505 \tF1:0.9651817771 \t AUC:0.9478179621\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1109550333 \tAcc: 0.9583676641 \tTPR:0.9768029434 \tFPR:0.0743470029 \tF1:0.9671698493 \t AUC:0.9512279702\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1077875984 \tAcc: 0.9590452443 \tTPR:0.9771725266 \tFPR:0.0742798465 \tF1:0.9678331473 \t AUC:0.9514463400\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1042952403 \tAcc: 0.9596686181 \tTPR:0.9773628949 \tFPR:0.0724224572 \tF1:0.9683723836 \t AUC:0.9524702188\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.2039745363 \tAcc: 0.9332702020 \tTPR:0.9248391295 \tFPR:0.0528129519 \tF1:0.9457107416 \tAUC:0.9360130888 \ttest cost: 0:00:16\n",
      "Test =>                 \tLoss: 0.2001254892 \tAcc: 0.9417837966 \tTPR:0.9088030823 \tFPR:0.0608220410 \tF1:0.9159694749 \tAUC:0.9234481039\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 17:  =============\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0680972533 \tAcc: 0.9713954574 \tTPR:0.9086191821 \tFPR:0.0162870671 \tF1:0.8974770718 \t AUC:0.9460366229\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0598689264 \tAcc: 0.9767774968 \tTPR:0.9130894294 \tFPR:0.0112356071 \tF1:0.9107539441 \t AUC:0.9508038083\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0552420236 \tAcc: 0.9792520959 \tTPR:0.9263686800 \tFPR:0.0116850823 \tF1:0.9213527216 \t AUC:0.9572375052\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0524308405 \tAcc: 0.9790755422 \tTPR:0.9362253487 \tFPR:0.0124950356 \tF1:0.9268328432 \t AUC:0.9617748242\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0456097174 \tAcc: 0.9827860169 \tTPR:0.9435409380 \tFPR:0.0097731945 \tF1:0.9370596789 \t AUC:0.9666425937\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1629923981 \tAcc: 0.9588815789 \tTPR:0.8256683375 \tFPR:0.0179537262 \tF1:0.8346392771 \tAUC:0.9038573056 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1433064689 \tAcc: 0.9406411082 \tTPR:0.9476930338 \tFPR:0.0643254838 \tF1:0.9354669929 \t AUC:0.9416837750\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1272747112 \tAcc: 0.9473982403 \tTPR:0.9540926145 \tFPR:0.0592184896 \tF1:0.9437001001 \t AUC:0.9474370625\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1219425276 \tAcc: 0.9500644330 \tTPR:0.9575747444 \tFPR:0.0580387816 \tF1:0.9462939268 \t AUC:0.9497679814\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1147766305 \tAcc: 0.9538498711 \tTPR:0.9599919537 \tFPR:0.0519396613 \tF1:0.9501330712 \t AUC:0.9540261462\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1107727288 \tAcc: 0.9546302879 \tTPR:0.9613466994 \tFPR:0.0521782472 \tF1:0.9507024983 \t AUC:0.9545842261\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.1930858323 \tAcc: 0.9381445442 \tTPR:0.9232603017 \tFPR:0.0472484607 \tF1:0.9340207161 \tAUC:0.9380059205 \ttest cost: 0:00:05\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1186963763 \tAcc: 0.9547882336 \tTPR:0.9745421997 \tFPR:0.0811967802 \tF1:0.9644110479 \t AUC:0.9466727097\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1113469311 \tAcc: 0.9576105811 \tTPR:0.9759051113 \tFPR:0.0755790133 \tF1:0.9665947029 \t AUC:0.9501661545\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1067289347 \tAcc: 0.9606425267 \tTPR:0.9777111900 \tFPR:0.0703109711 \tF1:0.9689623321 \t AUC:0.9537001094\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.1024764885 \tAcc: 0.9618350679 \tTPR:0.9790806281 \tFPR:0.0702474586 \tF1:0.9699685568 \t AUC:0.9544165847\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.1023313625 \tAcc: 0.9607798497 \tTPR:0.9785809332 \tFPR:0.0727291862 \tF1:0.9691641926 \t AUC:0.9529258735\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1675231760 \tAcc: 0.9459595960 \tTPR:0.9604856060 \tFPR:0.0803393970 \tF1:0.9571577627 \tAUC:0.9400731045 \ttest cost: 0:00:16\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0485705859 \tAcc: 0.9848958333 \tTPR:0.9949881948 \tFPR:0.1577380952 \tF1:0.9918449239 \t AUC:0.9184460567\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0344828312 \tAcc: 0.9895833333 \tTPR:0.9978459863 \tFPR:0.1321212121 \tF1:0.9943960776 \t AUC:0.9333326592\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0298042608 \tAcc: 0.9916666667 \tTPR:0.9983129403 \tFPR:0.0716981132 \tF1:0.9953872683 \t AUC:0.9631960039\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0263859540 \tAcc: 0.9916666667 \tTPR:0.9966413133 \tFPR:0.0703030303 \tF1:0.9954436839 \t AUC:0.9633005648\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0246177422 \tAcc: 0.9927083333 \tTPR:0.9982127653 \tFPR:0.0767295597 \tF1:0.9959258503 \t AUC:0.9606235778\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.1410569126 \tAcc: 0.9639423077 \tTPR:0.9782173136 \tFPR:0.2318840580 \tF1:0.9803945191 \tAUC:0.8724253657 \ttest cost: 0:00:01\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0625900397 \tAcc: 0.9755018248 \tTPR:0.8855131608 \tFPR:0.0133982225 \tF1:0.8625865997 \t AUC:0.9348631206\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0542060594 \tAcc: 0.9797445255 \tTPR:0.9041507357 \tFPR:0.0105336512 \tF1:0.8942195183 \t AUC:0.9453671999\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0496657895 \tAcc: 0.9804288321 \tTPR:0.9037701309 \tFPR:0.0097996494 \tF1:0.8913775132 \t AUC:0.9458345099\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0453533996 \tAcc: 0.9828467153 \tTPR:0.9190643663 \tFPR:0.0086811683 \tF1:0.9047595580 \t AUC:0.9542237588\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0421896653 \tAcc: 0.9828010949 \tTPR:0.9222390221 \tFPR:0.0092094789 \tF1:0.9068545895 \t AUC:0.9555848944\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1296717817 \tAcc: 0.9632227891 \tTPR:0.8232628766 \tFPR:0.0207495266 \tF1:0.7934565529 \tAUC:0.8994156633 \ttest cost: 0:00:09\n",
      "Test =>                 \tLoss: 0.1588660201 \tAcc: 0.9540301632 \tTPR:0.9021788871 \tFPR:0.0796350337 \tF1:0.8999337656 \tAUC:0.9107554719\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 18:  =============\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0895174370 \tAcc: 0.9639136905 \tTPR:0.9814027802 \tFPR:0.0720461747 \tF1:0.9732726005 \t AUC:0.9546783027\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0789059490 \tAcc: 0.9680059524 \tTPR:0.9832299738 \tFPR:0.0666684836 \tF1:0.9769560945 \t AUC:0.9582807451\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0689918055 \tAcc: 0.9712797619 \tTPR:0.9843815668 \tFPR:0.0561757520 \tF1:0.9788280559 \t AUC:0.9646112301\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0581081505 \tAcc: 0.9758184524 \tTPR:0.9878629686 \tFPR:0.0552387427 \tF1:0.9822386333 \t AUC:0.9663121130\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0555526920 \tAcc: 0.9791666667 \tTPR:0.9875434873 \tFPR:0.0395638918 \tF1:0.9842388625 \t AUC:0.9739897977\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2100296033 \tAcc: 0.9515775966 \tTPR:0.9519331126 \tFPR:0.0530863670 \tF1:0.9621022192 \tAUC:0.9494233728 \ttest cost: 0:00:01\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0863050392 \tAcc: 0.9685385757 \tTPR:0.9860791113 \tFPR:0.0906859626 \tF1:0.9792352375 \t AUC:0.9476965744\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0799997213 \tAcc: 0.9709903561 \tTPR:0.9887978331 \tFPR:0.0902680076 \tF1:0.9809952802 \t AUC:0.9492649127\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0706612912 \tAcc: 0.9752151335 \tTPR:0.9899806823 \tFPR:0.0718230253 \tF1:0.9835646177 \t AUC:0.9590788285\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0706355300 \tAcc: 0.9734792285 \tTPR:0.9885188518 \tFPR:0.0760502150 \tF1:0.9824933637 \t AUC:0.9562343184\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0649626202 \tAcc: 0.9771364985 \tTPR:0.9906569605 \tFPR:0.0675257729 \tF1:0.9848395395 \t AUC:0.9615655938\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1788467885 \tAcc: 0.9450431034 \tTPR:0.9553089271 \tFPR:0.0874434569 \tF1:0.9627441011 \tAUC:0.9339327351 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1159484681 \tAcc: 0.9536897590 \tTPR:0.9604320501 \tFPR:0.0510748833 \tF1:0.9476043242 \t AUC:0.9546785834\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1030362954 \tAcc: 0.9589608434 \tTPR:0.9678684590 \tFPR:0.0477570609 \tF1:0.9526796138 \t AUC:0.9600556991\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0995421882 \tAcc: 0.9627259036 \tTPR:0.9661626556 \tFPR:0.0414934145 \tF1:0.9565987157 \t AUC:0.9623346206\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0903777738 \tAcc: 0.9646084337 \tTPR:0.9695913475 \tFPR:0.0399065718 \tF1:0.9589265681 \t AUC:0.9648423878\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0876109864 \tAcc: 0.9649849398 \tTPR:0.9688022213 \tFPR:0.0387877736 \tF1:0.9592924741 \t AUC:0.9650072239\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1717922794 \tAcc: 0.9522569444 \tTPR:0.9342172585 \tFPR:0.0334028494 \tF1:0.9472562002 \tAUC:0.9504072045 \ttest cost: 0:00:01\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0584757222 \tAcc: 0.9776003650 \tTPR:0.8965108330 \tFPR:0.0118840722 \tF1:0.8757870073 \t AUC:0.9407571523\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0496756028 \tAcc: 0.9798357664 \tTPR:0.8961099525 \tFPR:0.0101584801 \tF1:0.8854760840 \t AUC:0.9414134798\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0458941068 \tAcc: 0.9818886861 \tTPR:0.9086531109 \tFPR:0.0088486427 \tF1:0.9002293330 \t AUC:0.9485992379\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0439786272 \tAcc: 0.9828467153 \tTPR:0.9171498088 \tFPR:0.0091043986 \tF1:0.8973288947 \t AUC:0.9523883311\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0385294038 \tAcc: 0.9856295620 \tTPR:0.9324371452 \tFPR:0.0080383735 \tF1:0.9229084693 \t AUC:0.9615970424\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1385489458 \tAcc: 0.9606717687 \tTPR:0.8312155815 \tFPR:0.0229868880 \tF1:0.7911188585 \tAUC:0.9014493296 \ttest cost: 0:00:09\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1124251519 \tAcc: 0.9575292715 \tTPR:0.9750498315 \tFPR:0.0736715936 \tF1:0.9665003109 \t AUC:0.9506891190\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1034730505 \tAcc: 0.9607545533 \tTPR:0.9785166822 \tFPR:0.0709783084 \tF1:0.9690732621 \t AUC:0.9537691869\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.0990855204 \tAcc: 0.9622723330 \tTPR:0.9786609739 \tFPR:0.0686377782 \tF1:0.9701023952 \t AUC:0.9550115979\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.0957110379 \tAcc: 0.9641677508 \tTPR:0.9795764773 \tFPR:0.0644759584 \tF1:0.9717818329 \t AUC:0.9575502594\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.0934103853 \tAcc: 0.9643321769 \tTPR:0.9804688319 \tFPR:0.0650363496 \tF1:0.9719120580 \t AUC:0.9577162411\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1717119222 \tAcc: 0.9452020202 \tTPR:0.9725769541 \tFPR:0.1047299220 \tF1:0.9569492115 \tAUC:0.9339235160 \ttest cost: 0:00:16\n",
      "Test =>                 \tLoss: 0.1741859078 \tAcc: 0.9509502867 \tTPR:0.9290503667 \tFPR:0.0603298967 \tF1:0.9240341181 \tAUC:0.9338272316\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 19:  =============\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0667305832 \tAcc: 0.9750071715 \tTPR:0.9895746481 \tFPR:0.0893664744 \tF1:0.9843279887 \t AUC:0.9501355440\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0595827745 \tAcc: 0.9776920181 \tTPR:0.9913112682 \tFPR:0.0793452435 \tF1:0.9860657354 \t AUC:0.9559698874\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0527124595 \tAcc: 0.9800900029 \tTPR:0.9918756630 \tFPR:0.0712688750 \tF1:0.9874804686 \t AUC:0.9603033940\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0477448587 \tAcc: 0.9816901535 \tTPR:0.9919635439 \tFPR:0.0613944539 \tF1:0.9884128988 \t AUC:0.9652845450\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0455890861 \tAcc: 0.9830079246 \tTPR:0.9930039524 \tFPR:0.0555440376 \tF1:0.9893689488 \t AUC:0.9687299574\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1941936397 \tAcc: 0.9505244755 \tTPR:0.9606106216 \tFPR:0.0903079244 \tF1:0.9683775572 \tAUC:0.9351513486 \ttest cost: 0:00:05\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0500180403 \tAcc: 0.9833806818 \tTPR:0.9944598937 \tFPR:0.1328840970 \tF1:0.9907774759 \t AUC:0.9304220423\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0397417326 \tAcc: 0.9890625000 \tTPR:0.9970771371 \tFPR:0.1183641975 \tF1:0.9939363767 \t AUC:0.9394834404\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0287159885 \tAcc: 0.9906250000 \tTPR:0.9972004608 \tFPR:0.0800000000 \tF1:0.9948818216 \t AUC:0.9584729786\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0290861436 \tAcc: 0.9911931818 \tTPR:0.9988888889 \tFPR:0.0960606061 \tF1:0.9950790793 \t AUC:0.9513636364\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0242904573 \tAcc: 0.9932291667 \tTPR:0.9983333333 \tFPR:0.0660493827 \tF1:0.9962606556 \t AUC:0.9660493827\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.1335117050 \tAcc: 0.9651442308 \tTPR:0.9810309685 \tFPR:0.2567287785 \tF1:0.9810025404 \tAUC:0.8636313756 \ttest cost: 0:00:01\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0555494225 \tAcc: 0.9779197080 \tTPR:0.8997439462 \tFPR:0.0120096854 \tF1:0.8852249773 \t AUC:0.9430490172\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0463853500 \tAcc: 0.9814781022 \tTPR:0.9081942996 \tFPR:0.0094225784 \tF1:0.8953038871 \t AUC:0.9483581849\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0419996721 \tAcc: 0.9840784672 \tTPR:0.9256604101 \tFPR:0.0088820617 \tF1:0.9095950468 \t AUC:0.9574432363\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0406013897 \tAcc: 0.9840784672 \tTPR:0.9285922836 \tFPR:0.0086888921 \tF1:0.9135149609 \t AUC:0.9591523557\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0373525505 \tAcc: 0.9856751825 \tTPR:0.9355503418 \tFPR:0.0080930633 \tF1:0.9216441564 \t AUC:0.9630562881\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1303558472 \tAcc: 0.9656675170 \tTPR:0.8376039305 \tFPR:0.0179468617 \tF1:0.7991751257 \tAUC:0.9057686327 \ttest cost: 0:00:09\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0663916146 \tAcc: 0.9733403955 \tTPR:0.9045612501 \tFPR:0.0139116796 \tF1:0.9009038995 \t AUC:0.9450536524\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0547858684 \tAcc: 0.9789872654 \tTPR:0.9286699725 \tFPR:0.0113362084 \tF1:0.9226081975 \t AUC:0.9584642399\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0536385628 \tAcc: 0.9796963277 \tTPR:0.9325451855 \tFPR:0.0116060160 \tF1:0.9248942376 \t AUC:0.9602779518\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0440772283 \tAcc: 0.9839279205 \tTPR:0.9459173046 \tFPR:0.0098606595 \tF1:0.9377869084 \t AUC:0.9679517182\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0432878808 \tAcc: 0.9820798023 \tTPR:0.9430610585 \tFPR:0.0104669889 \tF1:0.9311900752 \t AUC:0.9658891627\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1769719674 \tAcc: 0.9544836957 \tTPR:0.8629091669 \tFPR:0.0285239809 \tF1:0.8300476472 \tAUC:0.9167386498 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0928383614 \tAcc: 0.9639136905 \tTPR:0.9792167615 \tFPR:0.0665879755 \tF1:0.9725371196 \t AUC:0.9563143930\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0762283916 \tAcc: 0.9676339286 \tTPR:0.9826232990 \tFPR:0.0639050103 \tF1:0.9760229977 \t AUC:0.9593591443\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0660736882 \tAcc: 0.9761904762 \tTPR:0.9880451751 \tFPR:0.0495689923 \tF1:0.9823088592 \t AUC:0.9692380914\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0585529795 \tAcc: 0.9773065476 \tTPR:0.9876812091 \tFPR:0.0419854665 \tF1:0.9828695131 \t AUC:0.9728478713\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0533133201 \tAcc: 0.9794642857 \tTPR:0.9910946084 \tFPR:0.0438033692 \tF1:0.9847686156 \t AUC:0.9736456196\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.1872854278 \tAcc: 0.9527853261 \tTPR:0.9551530780 \tFPR:0.0526431131 \tF1:0.9624266114 \tAUC:0.9512549825 \ttest cost: 0:00:01\n",
      "Test =>                 \tLoss: 0.1644637174 \tAcc: 0.9577210490 \tTPR:0.9194615531 \tFPR:0.0892301317 \tF1:0.9082058964 \tAUC:0.9145089978\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 20:  =============\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0370956076 \tAcc: 0.9865056818 \tTPR:0.9927135775 \tFPR:0.0826388889 \tF1:0.9925023999 \t AUC:0.9547771149\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0224254917 \tAcc: 0.9927083333 \tTPR:0.9989236111 \tFPR:0.0936363636 \tF1:0.9960760982 \t AUC:0.9528787879\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0249427888 \tAcc: 0.9911458333 \tTPR:0.9971992249 \tFPR:0.0760606061 \tF1:0.9951515706 \t AUC:0.9604420015\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0163779045 \tAcc: 0.9958333333 \tTPR:0.9989044540 \tFPR:0.0447530864 \tF1:0.9977365080 \t AUC:0.9773041720\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0184551495 \tAcc: 0.9942708333 \tTPR:0.9989068100 \tFPR:0.0508771930 \tF1:0.9968595284 \t AUC:0.9739860404\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.1516427492 \tAcc: 0.9627403846 \tTPR:0.9770863324 \tFPR:0.2113333333 \tF1:0.9796635223 \tAUC:0.8824182262 \ttest cost: 0:00:01\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0745842257 \tAcc: 0.9720982143 \tTPR:0.9823613365 \tFPR:0.0479543274 \tF1:0.9794771035 \t AUC:0.9672035045\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0589628207 \tAcc: 0.9799107143 \tTPR:0.9919260697 \tFPR:0.0436030240 \tF1:0.9850549199 \t AUC:0.9741615229\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0535697041 \tAcc: 0.9798363095 \tTPR:0.9872156807 \tFPR:0.0398931639 \tF1:0.9846901599 \t AUC:0.9736612584\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0499689037 \tAcc: 0.9813988095 \tTPR:0.9893661192 \tFPR:0.0356925579 \tF1:0.9858443165 \t AUC:0.9768367806\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0523256135 \tAcc: 0.9784226190 \tTPR:0.9906959681 \tFPR:0.0475608561 \tF1:0.9843795088 \t AUC:0.9715115076\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2109911577 \tAcc: 0.9522569444 \tTPR:0.9499629185 \tFPR:0.0368657804 \tF1:0.9625482702 \tAUC:0.9565485691 \ttest cost: 0:00:01\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1236115975 \tAcc: 0.9654017857 \tTPR:0.9884639482 \tFPR:0.1221384766 \tF1:0.9777045731 \t AUC:0.9331627358\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.0912332863 \tAcc: 0.9765625000 \tTPR:0.9970677926 \tFPR:0.0881867141 \tF1:0.9845913813 \t AUC:0.9544405393\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.0865204371 \tAcc: 0.9720982143 \tTPR:0.9913547141 \tFPR:0.1041524943 \tF1:0.9823618440 \t AUC:0.9436011099\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.0742817964 \tAcc: 0.9743303571 \tTPR:0.9957431233 \tFPR:0.1022339763 \tF1:0.9833671142 \t AUC:0.9467545735\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0672499991 \tAcc: 0.9743303571 \tTPR:0.9900526054 \tFPR:0.0984410431 \tF1:0.9839316485 \t AUC:0.9458057812\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2091993997 \tAcc: 0.9310897436 \tTPR:0.9474777698 \tFPR:0.1317219817 \tF1:0.9534816052 \tAUC:0.9078778940 \ttest cost: 0:00:00\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0478595031 \tAcc: 0.9828689759 \tTPR:0.9928649174 \tFPR:0.0609598567 \tF1:0.9892890577 \t AUC:0.9659525303\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0450341035 \tAcc: 0.9836668101 \tTPR:0.9934969708 \tFPR:0.0556657341 \tF1:0.9897032363 \t AUC:0.9689057951\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0428508183 \tAcc: 0.9844198221 \tTPR:0.9934894612 \tFPR:0.0510218717 \tF1:0.9899889738 \t AUC:0.9713183710\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0386996579 \tAcc: 0.9861634036 \tTPR:0.9941187374 \tFPR:0.0459203280 \tF1:0.9913167370 \t AUC:0.9740992047\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0347868960 \tAcc: 0.9882790089 \tTPR:0.9952140442 \tFPR:0.0393891439 \tF1:0.9926403945 \t AUC:0.9779052206\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1615568851 \tAcc: 0.9619755245 \tTPR:0.9745129242 \tFPR:0.0926124931 \tF1:0.9760178725 \tAUC:0.9408604723 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1202792906 \tAcc: 0.9521084337 \tTPR:0.9523240877 \tFPR:0.0478786125 \tF1:0.9437753136 \t AUC:0.9522227376\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.0925546239 \tAcc: 0.9676204819 \tTPR:0.9745269048 \tFPR:0.0380740989 \tF1:0.9621300881 \t AUC:0.9682264030\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0911547290 \tAcc: 0.9664909639 \tTPR:0.9680175829 \tFPR:0.0367356505 \tF1:0.9606099797 \t AUC:0.9656409662\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0906126248 \tAcc: 0.9649849398 \tTPR:0.9706350792 \tFPR:0.0400445073 \tF1:0.9595593630 \t AUC:0.9652952860\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0731716358 \tAcc: 0.9702560241 \tTPR:0.9712884557 \tFPR:0.0310947820 \tF1:0.9654720329 \t AUC:0.9700968368\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1748318927 \tAcc: 0.9468750000 \tTPR:0.9313819113 \tFPR:0.0415613361 \tF1:0.9389042397 \tAUC:0.9449102876 \ttest cost: 0:00:01\n",
      "Test =>                 \tLoss: 0.1816444169 \tAcc: 0.9509875194 \tTPR:0.9560843712 \tFPR:0.1028189849 \tF1:0.9621231020 \tAUC:0.9265230898\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 21:  =============\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0428138083 \tAcc: 0.9846726190 \tTPR:0.9932244112 \tFPR:0.0310536686 \tF1:0.9882044200 \t AUC:0.9810853713\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0420502523 \tAcc: 0.9836309524 \tTPR:0.9918031445 \tFPR:0.0333076150 \tF1:0.9877968102 \t AUC:0.9792477648\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0400622032 \tAcc: 0.9862351190 \tTPR:0.9921387208 \tFPR:0.0281212718 \tF1:0.9896937404 \t AUC:0.9820087245\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0386058707 \tAcc: 0.9847470238 \tTPR:0.9920627179 \tFPR:0.0327205367 \tF1:0.9889463612 \t AUC:0.9796710906\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0290490744 \tAcc: 0.9877232143 \tTPR:0.9927606701 \tFPR:0.0231767571 \tF1:0.9911451984 \t AUC:0.9847919565\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2149966245 \tAcc: 0.9557291667 \tTPR:0.9647292852 \tFPR:0.0618800567 \tF1:0.9658608284 \tAUC:0.9514246143 \ttest cost: 0:00:01\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0422994716 \tAcc: 0.9845139487 \tTPR:0.9936149336 \tFPR:0.0493057729 \tF1:0.9902575019 \t AUC:0.9721545804\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0398154553 \tAcc: 0.9855045181 \tTPR:0.9936085288 \tFPR:0.0467182917 \tF1:0.9908540864 \t AUC:0.9734451185\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0397635553 \tAcc: 0.9855045181 \tTPR:0.9940074170 \tFPR:0.0500813862 \tF1:0.9909016008 \t AUC:0.9719630154\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0344874371 \tAcc: 0.9881400602 \tTPR:0.9943723378 \tFPR:0.0437177281 \tF1:0.9926013839 \t AUC:0.9753273049\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0321773565 \tAcc: 0.9885165663 \tTPR:0.9948959975 \tFPR:0.0402721609 \tF1:0.9927411168 \t AUC:0.9773119183\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.2080351442 \tAcc: 0.9484265734 \tTPR:0.9573783232 \tFPR:0.0869164482 \tF1:0.9669569563 \tAUC:0.9350808612 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0499716529 \tAcc: 0.9807025547 \tTPR:0.9106059553 \tFPR:0.0107287133 \tF1:0.8911557213 \t AUC:0.9485943497\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0440950321 \tAcc: 0.9823448905 \tTPR:0.9117923763 \tFPR:0.0087136726 \tF1:0.9035591761 \t AUC:0.9502811350\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0384081292 \tAcc: 0.9846259124 \tTPR:0.9214262542 \tFPR:0.0079222324 \tF1:0.9127871252 \t AUC:0.9555095044\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0356738791 \tAcc: 0.9865419708 \tTPR:0.9330645348 \tFPR:0.0069371730 \tF1:0.9248531748 \t AUC:0.9621605037\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0324274542 \tAcc: 0.9870437956 \tTPR:0.9386757039 \tFPR:0.0067076478 \tF1:0.9274809471 \t AUC:0.9653442813\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1514441685 \tAcc: 0.9650297619 \tTPR:0.7985287766 \tFPR:0.0153322540 \tF1:0.7985162452 \tAUC:0.8865614807 \ttest cost: 0:00:09\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1908259038 \tAcc: 0.9363839286 \tTPR:0.9510634075 \tFPR:0.1135204082 \tF1:0.9586600781 \t AUC:0.9187714997\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.0950980921 \tAcc: 0.9709821429 \tTPR:0.9951733954 \tFPR:0.1198850752 \tF1:0.9810538249 \t AUC:0.9376441601\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.0878834503 \tAcc: 0.9676339286 \tTPR:0.9837293843 \tFPR:0.1103896104 \tF1:0.9798411958 \t AUC:0.9366698870\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.0808219476 \tAcc: 0.9733455882 \tTPR:0.9898521577 \tFPR:0.0791524943 \tF1:0.9825196710 \t AUC:0.9553498317\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0805922307 \tAcc: 0.9733455882 \tTPR:0.9945377037 \tFPR:0.1062100598 \tF1:0.9829191491 \t AUC:0.9441638219\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2246453273 \tAcc: 0.9336939103 \tTPR:0.9417019868 \tFPR:0.1099206349 \tF1:0.9559222688 \tAUC:0.9158906759 \ttest cost: 0:00:00\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0860686781 \tAcc: 0.9668694362 \tTPR:0.9855615662 \tFPR:0.0953484942 \tF1:0.9781924784 \t AUC:0.9451065360\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0684330400 \tAcc: 0.9749629080 \tTPR:0.9894706173 \tFPR:0.0751566353 \tF1:0.9834121751 \t AUC:0.9571569910\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0666528544 \tAcc: 0.9752818991 \tTPR:0.9896062356 \tFPR:0.0725595539 \tF1:0.9838090783 \t AUC:0.9585233409\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0625693265 \tAcc: 0.9780897626 \tTPR:0.9912655506 \tFPR:0.0646456947 \tF1:0.9855522562 \t AUC:0.9633099279\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0576308265 \tAcc: 0.9799703264 \tTPR:0.9917106899 \tFPR:0.0619531804 \tF1:0.9868808977 \t AUC:0.9648787548\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1761902052 \tAcc: 0.9553879310 \tTPR:0.9746674671 \tFPR:0.1087335270 \tF1:0.9702703392 \tAUC:0.9329669700 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1950622940 \tAcc: 0.9516534687 \tTPR:0.9274011678 \tFPR:0.0765565842 \tF1:0.9315053276 \tAUC:0.9243849204\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 22:  =============\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1270834663 \tAcc: 0.9498950187 \tTPR:0.9581059043 \tFPR:0.0573182479 \tF1:0.9456564070 \t AUC:0.9503938282\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.1175676380 \tAcc: 0.9520696321 \tTPR:0.9597614585 \tFPR:0.0545946164 \tF1:0.9482439578 \t AUC:0.9525834210\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.1103204472 \tAcc: 0.9561772352 \tTPR:0.9640502175 \tFPR:0.0495737843 \tF1:0.9531611061 \t AUC:0.9572382166\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.1035309969 \tAcc: 0.9581990979 \tTPR:0.9645677861 \tFPR:0.0475709497 \tF1:0.9546576314 \t AUC:0.9584984182\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.1000552355 \tAcc: 0.9588267641 \tTPR:0.9673380145 \tFPR:0.0480154955 \tF1:0.9557714163 \t AUC:0.9596612595\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.1816549932 \tAcc: 0.9408682635 \tTPR:0.9321490352 \tFPR:0.0492989510 \tF1:0.9351498606 \tAUC:0.9414250421 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0420729708 \tAcc: 0.9828923358 \tTPR:0.9184254432 \tFPR:0.0087402823 \tF1:0.9074559207 \t AUC:0.9539294324\tTrain cost: 0:01:06\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0340277617 \tAcc: 0.9866788321 \tTPR:0.9346750087 \tFPR:0.0070241329 \tF1:0.9230820505 \t AUC:0.9629439912\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0298896440 \tAcc: 0.9885948905 \tTPR:0.9530147144 \tFPR:0.0061741295 \tF1:0.9403776253 \t AUC:0.9727863081\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0296955239 \tAcc: 0.9889598540 \tTPR:0.9438599235 \tFPR:0.0058978138 \tF1:0.9410112638 \t AUC:0.9683953909\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0277969848 \tAcc: 0.9885492701 \tTPR:0.9415004055 \tFPR:0.0058918823 \tF1:0.9312853572 \t AUC:0.9671047149\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1520618475 \tAcc: 0.9627976190 \tTPR:0.8370464853 \tFPR:0.0227724689 \tF1:0.8035701897 \tAUC:0.9057273757 \ttest cost: 0:00:09\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1297406688 \tAcc: 0.9540662651 \tTPR:0.9534832335 \tFPR:0.0449880725 \tF1:0.9479222029 \t AUC:0.9542475805\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.1073342555 \tAcc: 0.9589608434 \tTPR:0.9604978443 \tFPR:0.0413452600 \tF1:0.9517476717 \t AUC:0.9595762922\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0956382698 \tAcc: 0.9623493976 \tTPR:0.9649510952 \tFPR:0.0400559058 \tF1:0.9549058478 \t AUC:0.9624475947\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0891536806 \tAcc: 0.9690512048 \tTPR:0.9722194315 \tFPR:0.0309546934 \tF1:0.9642047474 \t AUC:0.9706323690\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0821279280 \tAcc: 0.9679216867 \tTPR:0.9734179947 \tFPR:0.0366626517 \tF1:0.9621316237 \t AUC:0.9683776715\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1960095691 \tAcc: 0.9447916667 \tTPR:0.9231040703 \tFPR:0.0358047742 \tF1:0.9330530561 \tAUC:0.9436496480 \ttest cost: 0:00:01\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0818400135 \tAcc: 0.9699554896 \tTPR:0.9870135904 \tFPR:0.0882806363 \tF1:0.9803284762 \t AUC:0.9493664770\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0683075766 \tAcc: 0.9744065282 \tTPR:0.9887728170 \tFPR:0.0744341918 \tF1:0.9831401837 \t AUC:0.9571693126\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0635891436 \tAcc: 0.9773738872 \tTPR:0.9903110792 \tFPR:0.0669230921 \tF1:0.9850658326 \t AUC:0.9616939935\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0611518311 \tAcc: 0.9778375371 \tTPR:0.9914044536 \tFPR:0.0661748077 \tF1:0.9854428446 \t AUC:0.9626148229\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0551139693 \tAcc: 0.9811758160 \tTPR:0.9921298548 \tFPR:0.0557801661 \tF1:0.9876036579 \t AUC:0.9681748444\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1734549857 \tAcc: 0.9523706897 \tTPR:0.9679023290 \tFPR:0.0938288818 \tF1:0.9683133832 \tAUC:0.9370367236 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.0871636447 \tAcc: 0.9688813025 \tTPR:0.9869452076 \tFPR:0.1135770975 \tF1:0.9810694953 \t AUC:0.9366840550\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.0809162348 \tAcc: 0.9765625000 \tTPR:0.9970833333 \tFPR:0.0966411565 \tF1:0.9849033569 \t AUC:0.9502210884\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.0822690292 \tAcc: 0.9732142857 \tTPR:0.9930976207 \tFPR:0.0963873428 \tF1:0.9830950911 \t AUC:0.9483551389\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.0668322164 \tAcc: 0.9780724790 \tTPR:0.9912724775 \tFPR:0.0714405634 \tF1:0.9853633437 \t AUC:0.9599159571\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0571582989 \tAcc: 0.9843750000 \tTPR:0.9987244898 \tFPR:0.0706632653 \tF1:0.9901809749 \t AUC:0.9640306122\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2246470707 \tAcc: 0.9342948718 \tTPR:0.9461345599 \tFPR:0.0888618326 \tF1:0.9555688350 \tAUC:0.9286363636 \ttest cost: 0:00:00\n",
      "Test =>                 \tLoss: 0.1855656932 \tAcc: 0.9470246221 \tTPR:0.9212672959 \tFPR:0.0581133817 \tF1:0.9191310649 \tAUC:0.9312950306\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 23:  =============\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.1056452688 \tAcc: 0.9577158505 \tTPR:0.9642903588 \tFPR:0.0479316275 \tF1:0.9543392069 \t AUC:0.9581793656\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.0967016364 \tAcc: 0.9610097094 \tTPR:0.9665266040 \tFPR:0.0439531054 \tF1:0.9575534499 \t AUC:0.9612867493\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.0902383428 \tAcc: 0.9642313589 \tTPR:0.9721693352 \tFPR:0.0424386372 \tF1:0.9609839410 \t AUC:0.9648653490\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.0834951915 \tAcc: 0.9664865135 \tTPR:0.9721604710 \tFPR:0.0383360305 \tF1:0.9638645510 \t AUC:0.9669122203\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.0792772881 \tAcc: 0.9691527062 \tTPR:0.9757925588 \tFPR:0.0381449468 \tF1:0.9669746816 \t AUC:0.9688238060\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2310844245 \tAcc: 0.9377702927 \tTPR:0.9133378587 \tFPR:0.0428202691 \tF1:0.9283934436 \tAUC:0.9352587948 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0515180625 \tAcc: 0.9817394578 \tTPR:0.9928743476 \tFPR:0.0686978433 \tF1:0.9886182720 \t AUC:0.9620882521\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0427173061 \tAcc: 0.9852221386 \tTPR:0.9933671047 \tFPR:0.0464133925 \tF1:0.9906692325 \t AUC:0.9734768561\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0383631243 \tAcc: 0.9872929217 \tTPR:0.9945989630 \tFPR:0.0456398251 \tF1:0.9920578203 \t AUC:0.9744714103\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0323449991 \tAcc: 0.9873870482 \tTPR:0.9942870083 \tFPR:0.0378432561 \tF1:0.9919916414 \t AUC:0.9782218761\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0290807674 \tAcc: 0.9910579819 \tTPR:0.9966949308 \tFPR:0.0327358468 \tF1:0.9942986523 \t AUC:0.9819795420\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.2028278814 \tAcc: 0.9525786713 \tTPR:0.9627551398 \tFPR:0.0875394613 \tF1:0.9694430690 \tAUC:0.9376078393 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0660196132 \tAcc: 0.9741815476 \tTPR:0.9848586439 \tFPR:0.0451691166 \tF1:0.9803053637 \t AUC:0.9698447637\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0485492589 \tAcc: 0.9802827381 \tTPR:0.9891181109 \tFPR:0.0379708255 \tF1:0.9857477420 \t AUC:0.9755736427\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0452558153 \tAcc: 0.9824404762 \tTPR:0.9906764829 \tFPR:0.0352567406 \tF1:0.9868822196 \t AUC:0.9777098712\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0440130750 \tAcc: 0.9843750000 \tTPR:0.9924754995 \tFPR:0.0338645860 \tF1:0.9884575119 \t AUC:0.9793054568\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0398114094 \tAcc: 0.9836309524 \tTPR:0.9912136091 \tFPR:0.0325727183 \tF1:0.9876227898 \t AUC:0.9793204454\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2266372846 \tAcc: 0.9519172705 \tTPR:0.9487933250 \tFPR:0.0372010819 \tF1:0.9623534708 \tAUC:0.9557961216 \ttest cost: 0:00:01\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0419287215 \tAcc: 0.9869791667 \tTPR:0.9933954780 \tFPR:0.0736363636 \tF1:0.9928554826 \t AUC:0.9604316244\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0303989529 \tAcc: 0.9895833333 \tTPR:0.9966379608 \tFPR:0.0826406926 \tF1:0.9941914965 \t AUC:0.9576980869\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0288989129 \tAcc: 0.9896306818 \tTPR:0.9967567379 \tFPR:0.1012820513 \tF1:0.9943580526 \t AUC:0.9483619875\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0297781043 \tAcc: 0.9901515152 \tTPR:0.9982871701 \tFPR:0.1096969697 \tF1:0.9946556628 \t AUC:0.9445013352\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0200595266 \tAcc: 0.9942708333 \tTPR:0.9988492063 \tFPR:0.0473684211 \tF1:0.9968191079 \t AUC:0.9757101086\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.1193384304 \tAcc: 0.9699519231 \tTPR:0.9856201640 \tFPR:0.2463768116 \tF1:0.9836442773 \tAUC:0.8686838608 \ttest cost: 0:00:01\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1206938109 \tAcc: 0.9687500000 \tTPR:0.9915888278 \tFPR:0.1184266131 \tF1:0.9800324265 \t AUC:0.9365811074\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.0893551717 \tAcc: 0.9755777311 \tTPR:0.9930605704 \tFPR:0.0906887755 \tF1:0.9846653057 \t AUC:0.9511858974\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.0753936340 \tAcc: 0.9776785714 \tTPR:0.9931640773 \tFPR:0.0713577098 \tF1:0.9857900155 \t AUC:0.9609031838\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.0617026591 \tAcc: 0.9799107143 \tTPR:0.9960222070 \tFPR:0.0908877131 \tF1:0.9875878180 \t AUC:0.9530722878\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0616908186 \tAcc: 0.9800420168 \tTPR:0.9932336182 \tFPR:0.0581992412 \tF1:0.9868760253 \t AUC:0.9675171885\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2388383788 \tAcc: 0.9362980769 \tTPR:0.9556851636 \tFPR:0.1284812410 \tF1:0.9585263763 \tAUC:0.9136019613 \ttest cost: 0:00:00\n",
      "Test =>                 \tLoss: 0.2037452799 \tAcc: 0.9497032469 \tTPR:0.9532383302 \tFPR:0.1084837730 \tF1:0.9604721274 \tAUC:0.9221897155\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 24:  =============\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1115948552 \tAcc: 0.9581325301 \tTPR:0.9588571928 \tFPR:0.0427468736 \tF1:0.9499011336 \t AUC:0.9580551596\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.0897703158 \tAcc: 0.9653614458 \tTPR:0.9730336872 \tFPR:0.0413113735 \tF1:0.9606820042 \t AUC:0.9658611569\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0821713853 \tAcc: 0.9683734940 \tTPR:0.9695590035 \tFPR:0.0320315223 \tF1:0.9637919628 \t AUC:0.9687637406\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0651850172 \tAcc: 0.9774096386 \tTPR:0.9800493757 \tFPR:0.0258228828 \tF1:0.9734536895 \t AUC:0.9771132465\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0661368766 \tAcc: 0.9743975904 \tTPR:0.9803127780 \tFPR:0.0294245281 \tF1:0.9701083203 \t AUC:0.9754441250\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1891732062 \tAcc: 0.9496527778 \tTPR:0.9284979183 \tFPR:0.0299494267 \tF1:0.9425813851 \tAUC:0.9492742458 \ttest cost: 0:00:01\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.0841112676 \tAcc: 0.9656005599 \tTPR:0.9711943530 \tFPR:0.0393671057 \tF1:0.9635395411 \t AUC:0.9659136237\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.0752560613 \tAcc: 0.9702552879 \tTPR:0.9769245315 \tFPR:0.0352630748 \tF1:0.9676715985 \t AUC:0.9708307284\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.0700507295 \tAcc: 0.9722771507 \tTPR:0.9769566282 \tFPR:0.0323995134 \tF1:0.9704968249 \t AUC:0.9722785574\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.0678328465 \tAcc: 0.9733158550 \tTPR:0.9785249085 \tFPR:0.0304822722 \tF1:0.9710160899 \t AUC:0.9740213181\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.0599236590 \tAcc: 0.9770290837 \tTPR:0.9794995313 \tFPR:0.0251172687 \tF1:0.9749225698 \t AUC:0.9771911313\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2464448681 \tAcc: 0.9385187957 \tTPR:0.9240832127 \tFPR:0.0503876360 \tF1:0.9306716320 \tAUC:0.9368477883 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0658508918 \tAcc: 0.9720982143 \tTPR:0.9840947011 \tFPR:0.0529387940 \tF1:0.9793383842 \t AUC:0.9655779536\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0460967769 \tAcc: 0.9810267857 \tTPR:0.9891684652 \tFPR:0.0354659278 \tF1:0.9862173983 \t AUC:0.9768512687\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0375621872 \tAcc: 0.9869791667 \tTPR:0.9935094262 \tFPR:0.0260311001 \tF1:0.9900357536 \t AUC:0.9837391631\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0434462525 \tAcc: 0.9828869048 \tTPR:0.9899928324 \tFPR:0.0340568162 \tF1:0.9876041776 \t AUC:0.9779680081\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0301398964 \tAcc: 0.9903273810 \tTPR:0.9945456612 \tFPR:0.0187152445 \tF1:0.9926720943 \t AUC:0.9879152084\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2177097184 \tAcc: 0.9515775966 \tTPR:0.9519377039 \tFPR:0.0456397225 \tF1:0.9617189762 \tAUC:0.9531489907 \ttest cost: 0:00:01\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0701468882 \tAcc: 0.9718368416 \tTPR:0.9038539144 \tFPR:0.0155978475 \tF1:0.8962965713 \t AUC:0.9441280335\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0616738468 \tAcc: 0.9765183616 \tTPR:0.9173882693 \tFPR:0.0127680824 \tF1:0.9095902629 \t AUC:0.9519570518\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0543061471 \tAcc: 0.9802259887 \tTPR:0.9324970855 \tFPR:0.0108189542 \tF1:0.9262367627 \t AUC:0.9603555204\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0481959737 \tAcc: 0.9810204802 \tTPR:0.9329245221 \tFPR:0.0095819708 \tF1:0.9276710327 \t AUC:0.9615762679\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0444956474 \tAcc: 0.9828714461 \tTPR:0.9440105249 \tFPR:0.0095220094 \tF1:0.9402912077 \t AUC:0.9671649526\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1523074279 \tAcc: 0.9576480263 \tTPR:0.8813700919 \tFPR:0.0276732160 \tF1:0.8377589902 \tAUC:0.9264556237 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0789806280 \tAcc: 0.9713464392 \tTPR:0.9867493014 \tFPR:0.0766004273 \tF1:0.9811189522 \t AUC:0.9550744371\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0693072008 \tAcc: 0.9744732938 \tTPR:0.9888274717 \tFPR:0.0718674583 \tF1:0.9832498530 \t AUC:0.9584800067\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0598689815 \tAcc: 0.9786461424 \tTPR:0.9909483799 \tFPR:0.0589467493 \tF1:0.9858325691 \t AUC:0.9659873456\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0540479012 \tAcc: 0.9800630564 \tTPR:0.9915629565 \tFPR:0.0603205894 \tF1:0.9869160711 \t AUC:0.9656211835\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0519763880 \tAcc: 0.9805267062 \tTPR:0.9905797483 \tFPR:0.0545261695 \tF1:0.9870753616 \t AUC:0.9680267894\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1728298000 \tAcc: 0.9506465517 \tTPR:0.9637301300 \tFPR:0.0899786803 \tF1:0.9667918744 \tAUC:0.9368757249 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1956930041 \tAcc: 0.9496087496 \tTPR:0.9299238114 \tFPR:0.0487257363 \tF1:0.9279045716 \tAUC:0.9405204747\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 25:  =============\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.0727169761 \tAcc: 0.9701914104 \tTPR:0.9740478232 \tFPR:0.0332904586 \tF1:0.9680448604 \t AUC:0.9703786823\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.0638939389 \tAcc: 0.9752655084 \tTPR:0.9797324170 \tFPR:0.0283803749 \tF1:0.9731909390 \t AUC:0.9756760211\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.0605795715 \tAcc: 0.9758292970 \tTPR:0.9784911197 \tFPR:0.0261125283 \tF1:0.9739243472 \t AUC:0.9761892957\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.0511525723 \tAcc: 0.9797758176 \tTPR:0.9839899652 \tFPR:0.0239027797 \tF1:0.9782994619 \t AUC:0.9800435928\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.0526743954 \tAcc: 0.9794536527 \tTPR:0.9824101734 \tFPR:0.0228942280 \tF1:0.9772456487 \t AUC:0.9797579727\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2497175367 \tAcc: 0.9401197605 \tTPR:0.9349995716 \tFPR:0.0578170749 \tF1:0.9344719134 \tAUC:0.9385912483 \ttest cost: 0:00:05\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0751424513 \tAcc: 0.9732418398 \tTPR:0.9889419048 \tFPR:0.0790681211 \tF1:0.9823457948 \t AUC:0.9549368918\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0638209938 \tAcc: 0.9761424332 \tTPR:0.9894935977 \tFPR:0.0676379588 \tF1:0.9842262176 \t AUC:0.9609278194\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0599709241 \tAcc: 0.9766988131 \tTPR:0.9891971696 \tFPR:0.0655811548 \tF1:0.9844473527 \t AUC:0.9618080074\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0537376440 \tAcc: 0.9808048961 \tTPR:0.9919903495 \tFPR:0.0574456207 \tF1:0.9873589775 \t AUC:0.9672723644\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0482082710 \tAcc: 0.9833753709 \tTPR:0.9925492641 \tFPR:0.0519691321 \tF1:0.9890352728 \t AUC:0.9702900660\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1853263051 \tAcc: 0.9551724138 \tTPR:0.9709593380 \tFPR:0.0989086201 \tF1:0.9700731744 \tAUC:0.9360253589 \ttest cost: 0:00:05\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1166710330 \tAcc: 0.9642857143 \tTPR:0.9822601751 \tFPR:0.1191043084 \tF1:0.9773906528 \t AUC:0.9315779334\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.0805687282 \tAcc: 0.9733455882 \tTPR:0.9947201608 \tFPR:0.1167942177 \tF1:0.9828131682 \t AUC:0.9389629716\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.0790180706 \tAcc: 0.9776785714 \tTPR:0.9958241758 \tFPR:0.0912853020 \tF1:0.9858326265 \t AUC:0.9522694369\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.0686593954 \tAcc: 0.9810267857 \tTPR:0.9970238095 \tFPR:0.0848368893 \tF1:0.9878689235 \t AUC:0.9560934601\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0653438738 \tAcc: 0.9790572479 \tTPR:0.9945323129 \tFPR:0.0832482993 \tF1:0.9867881780 \t AUC:0.9556420068\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2306911691 \tAcc: 0.9342948718 \tTPR:0.9461010789 \tFPR:0.1036435786 \tF1:0.9567614851 \tAUC:0.9212287501 \ttest cost: 0:00:00\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0461227582 \tAcc: 0.9821167883 \tTPR:0.9173409275 \tFPR:0.0099234640 \tF1:0.8999325152 \t AUC:0.9523373143\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0369405198 \tAcc: 0.9850364964 \tTPR:0.9238738269 \tFPR:0.0073186585 \tF1:0.9167024312 \t AUC:0.9571328297\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0336174391 \tAcc: 0.9862226277 \tTPR:0.9324452555 \tFPR:0.0072692330 \tF1:0.9176695192 \t AUC:0.9615197510\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0283256568 \tAcc: 0.9889142336 \tTPR:0.9487237337 \tFPR:0.0056695217 \tF1:0.9421775010 \t AUC:0.9708352224\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0283350275 \tAcc: 0.9884580292 \tTPR:0.9462356621 \tFPR:0.0059827441 \tF1:0.9304629341 \t AUC:0.9693179727\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1486898253 \tAcc: 0.9625850340 \tTPR:0.8533244250 \tFPR:0.0252159698 \tF1:0.7999648972 \tAUC:0.9120028210 \ttest cost: 0:00:09\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0665598144 \tAcc: 0.9728422619 \tTPR:0.9827806497 \tFPR:0.0427388914 \tF1:0.9793370801 \t AUC:0.9700208791\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0524240360 \tAcc: 0.9791666667 \tTPR:0.9895067581 \tFPR:0.0410919273 \tF1:0.9844824240 \t AUC:0.9742074154\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0436404046 \tAcc: 0.9820684524 \tTPR:0.9884357066 \tFPR:0.0308807595 \tF1:0.9864063194 \t AUC:0.9787774736\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0379402413 \tAcc: 0.9851190476 \tTPR:0.9922652433 \tFPR:0.0269805261 \tF1:0.9889435980 \t AUC:0.9826423586\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0323163844 \tAcc: 0.9869791667 \tTPR:0.9944075665 \tFPR:0.0313685323 \tF1:0.9904850909 \t AUC:0.9815195171\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2182647744 \tAcc: 0.9531250000 \tTPR:0.9569734688 \tFPR:0.0613836010 \tF1:0.9636016849 \tAUC:0.9477949339 \ttest cost: 0:00:01\n",
      "Test =>                 \tLoss: 0.2065379221 \tAcc: 0.9490594160 \tTPR:0.9324715765 \tFPR:0.0693937689 \tF1:0.9249746310 \tAUC:0.9311286225\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 26:  =============\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0477951887 \tAcc: 0.9806547619 \tTPR:0.9892532227 \tFPR:0.0383249191 \tF1:0.9854972185 \t AUC:0.9754641518\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0404647417 \tAcc: 0.9828869048 \tTPR:0.9897797683 \tFPR:0.0318471872 \tF1:0.9873549507 \t AUC:0.9789662905\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0303350334 \tAcc: 0.9880952381 \tTPR:0.9940164076 \tFPR:0.0265205694 \tF1:0.9913559947 \t AUC:0.9837479191\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0250501513 \tAcc: 0.9895833333 \tTPR:0.9942217304 \tFPR:0.0209500651 \tF1:0.9924445431 \t AUC:0.9866358326\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0236308698 \tAcc: 0.9892113095 \tTPR:0.9934700674 \tFPR:0.0198166252 \tF1:0.9918928643 \t AUC:0.9868267211\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2468066798 \tAcc: 0.9484450483 \tTPR:0.9500537805 \tFPR:0.0483263106 \tF1:0.9601631049 \tAUC:0.9508637350 \ttest cost: 0:00:01\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.0629180840 \tAcc: 0.9773679124 \tTPR:0.9811616456 \tFPR:0.0256167816 \tF1:0.9754313802 \t AUC:0.9777724320\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.0517525786 \tAcc: 0.9805090206 \tTPR:0.9833000878 \tFPR:0.0218749594 \tF1:0.9790181544 \t AUC:0.9807125642\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.0489644822 \tAcc: 0.9808145219 \tTPR:0.9835149511 \tFPR:0.0211461857 \tF1:0.9794065725 \t AUC:0.9811843827\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.0458308951 \tAcc: 0.9825225515 \tTPR:0.9860672559 \tFPR:0.0208785063 \tF1:0.9813709431 \t AUC:0.9825943748\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.0422224480 \tAcc: 0.9835695876 \tTPR:0.9842039122 \tFPR:0.0169055713 \tF1:0.9820968437 \t AUC:0.9836491704\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2634075807 \tAcc: 0.9412425150 \tTPR:0.9292932331 \tFPR:0.0481147187 \tF1:0.9351288542 \tAUC:0.9405892572 \ttest cost: 0:00:05\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1058132208 \tAcc: 0.9634789157 \tTPR:0.9666588914 \tFPR:0.0399072844 \tF1:0.9575788706 \t AUC:0.9633758035\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.0816747065 \tAcc: 0.9664909639 \tTPR:0.9695969934 \tFPR:0.0353513031 \tF1:0.9601146184 \t AUC:0.9671228452\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0710147587 \tAcc: 0.9747740964 \tTPR:0.9765489612 \tFPR:0.0271576677 \tF1:0.9718752361 \t AUC:0.9746956467\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0666294161 \tAcc: 0.9762048193 \tTPR:0.9789948470 \tFPR:0.0254391055 \tF1:0.9713786676 \t AUC:0.9767778708\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0717009829 \tAcc: 0.9728915663 \tTPR:0.9792017530 \tFPR:0.0310945587 \tF1:0.9695223904 \t AUC:0.9740535971\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1749728924 \tAcc: 0.9513888889 \tTPR:0.9487302350 \tFPR:0.0461517433 \tF1:0.9435578925 \tAUC:0.9512892459 \ttest cost: 0:00:01\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0637880532 \tAcc: 0.9772811573 \tTPR:0.9900705172 \tFPR:0.0714918283 \tF1:0.9851270220 \t AUC:0.9592893444\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0514408210 \tAcc: 0.9814540059 \tTPR:0.9921687140 \tFPR:0.0515679589 \tF1:0.9878333673 \t AUC:0.9703003776\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0493677482 \tAcc: 0.9830971810 \tTPR:0.9928803849 \tFPR:0.0474440166 \tF1:0.9888744740 \t AUC:0.9727181841\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0469428198 \tAcc: 0.9843026706 \tTPR:0.9934481427 \tFPR:0.0456861964 \tF1:0.9897236630 \t AUC:0.9738809732\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0428616612 \tAcc: 0.9843954006 \tTPR:0.9933361619 \tFPR:0.0458141759 \tF1:0.9895811956 \t AUC:0.9737609930\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1937150947 \tAcc: 0.9498040752 \tTPR:0.9662598664 \tFPR:0.1031862965 \tF1:0.9669430651 \tAUC:0.9315367849 \ttest cost: 0:00:05\n",
      "Client2 Local Train => Local Epoch: 0 \tLoss: 0.0473886041 \tAcc: 0.9825372920 \tTPR:0.9920731595 \tFPR:0.0513302795 \tF1:0.9888658547 \t AUC:0.9703714400\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 1 \tLoss: 0.0423668973 \tAcc: 0.9847022017 \tTPR:0.9928472981 \tFPR:0.0542528338 \tF1:0.9904561585 \t AUC:0.9692972321\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 2 \tLoss: 0.0352290806 \tAcc: 0.9867281627 \tTPR:0.9940146115 \tFPR:0.0422897217 \tF1:0.9915724485 \t AUC:0.9758624449\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 3 \tLoss: 0.0315437704 \tAcc: 0.9883283133 \tTPR:0.9948899553 \tFPR:0.0377841519 \tF1:0.9926280349 \t AUC:0.9785847652\tTrain cost: 0:00:32\n",
      "Client2 Local Train => Local Epoch: 4 \tLoss: 0.0299819432 \tAcc: 0.9890813253 \tTPR:0.9945277551 \tFPR:0.0327968802 \tF1:0.9931103545 \t AUC:0.9808654375\tTrain cost: 0:00:32\n",
      "Client2 Test =>                 \tLoss: 0.1795772946 \tAcc: 0.9552010490 \tTPR:0.9672372502 \tFPR:0.0962563258 \tF1:0.9711663357 \tAUC:0.9353751004 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.2116959084 \tAcc: 0.9492163153 \tTPR:0.9523148731 \tFPR:0.0684070790 \tF1:0.9553918505 \tAUC:0.9419308247\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 27:  =============\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0369513907 \tAcc: 0.9843005952 \tTPR:0.9892915008 \tFPR:0.0271130160 \tF1:0.9880402785 \t AUC:0.9810892424\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0261831801 \tAcc: 0.9895833333 \tTPR:0.9956531275 \tFPR:0.0218877567 \tF1:0.9919618173 \t AUC:0.9868826854\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0256299411 \tAcc: 0.9899553571 \tTPR:0.9934075525 \tFPR:0.0178731652 \tF1:0.9926437203 \t AUC:0.9877671937\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0179570012 \tAcc: 0.9947916667 \tTPR:0.9962025970 \tFPR:0.0069969713 \tF1:0.9959939629 \t AUC:0.9946028129\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0183563655 \tAcc: 0.9918154762 \tTPR:0.9937290368 \tFPR:0.0160572562 \tF1:0.9940058732 \t AUC:0.9888358903\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2378966740 \tAcc: 0.9545214372 \tTPR:0.9657818177 \tFPR:0.0730797752 \tF1:0.9651714775 \tAUC:0.9463510213 \ttest cost: 0:00:01\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.1161496492 \tAcc: 0.9567432784 \tTPR:0.9749177115 \tFPR:0.0766098423 \tF1:0.9658375470 \t AUC:0.9491539346\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.1050824026 \tAcc: 0.9593722897 \tTPR:0.9765966504 \tFPR:0.0718589455 \tF1:0.9679411591 \t AUC:0.9523688525\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.1003655132 \tAcc: 0.9615911391 \tTPR:0.9782600239 \tFPR:0.0696695990 \tF1:0.9696358346 \t AUC:0.9542952125\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.0944097917 \tAcc: 0.9647369182 \tTPR:0.9795557047 \tFPR:0.0626071216 \tF1:0.9720994604 \t AUC:0.9584742915\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.0907496795 \tAcc: 0.9655229112 \tTPR:0.9802383612 \tFPR:0.0602112749 \tF1:0.9726611419 \t AUC:0.9600135432\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1617301486 \tAcc: 0.9473484848 \tTPR:0.9523427730 \tFPR:0.0617819911 \tF1:0.9574217103 \tAUC:0.9457382278 \ttest cost: 0:00:16\n",
      "Client9 Local Train => Local Epoch: 0 \tLoss: 0.0381681163 \tAcc: 0.9875000000 \tTPR:0.9961457175 \tFPR:0.1342767296 \tF1:0.9931689972 \t AUC:0.9306799659\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 1 \tLoss: 0.0224546548 \tAcc: 0.9932291667 \tTPR:0.9988697318 \tFPR:0.0844780220 \tF1:0.9962995288 \t AUC:0.9571089112\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 2 \tLoss: 0.0160543460 \tAcc: 0.9953125000 \tTPR:0.9989247312 \tFPR:0.0433862434 \tF1:0.9974298185 \t AUC:0.9777095067\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 3 \tLoss: 0.0183069291 \tAcc: 0.9937500000 \tTPR:0.9988492063 \tFPR:0.0733918129 \tF1:0.9965928890 \t AUC:0.9626984127\tTrain cost: 0:00:06\n",
      "Client9 Local Train => Local Epoch: 4 \tLoss: 0.0127123429 \tAcc: 0.9942708333 \tTPR:0.9966450377 \tFPR:0.0318181818 \tF1:0.9968580760 \t AUC:0.9822609297\tTrain cost: 0:00:06\n",
      "Client9 Test =>                 \tLoss: 0.1438117098 \tAcc: 0.9675480769 \tTPR:0.9844653018 \tFPR:0.2812500000 \tF1:0.9824905015 \tAUC:0.8516114135 \ttest cost: 0:00:01\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.0778845273 \tAcc: 0.9691527062 \tTPR:0.9727689792 \tFPR:0.0335963870 \tF1:0.9670910613 \t AUC:0.9695862961\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.0594957260 \tAcc: 0.9759181701 \tTPR:0.9801947738 \tFPR:0.0283068857 \tF1:0.9740292631 \t AUC:0.9759439440\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.0525046509 \tAcc: 0.9797036082 \tTPR:0.9821421252 \tFPR:0.0226556247 \tF1:0.9781413325 \t AUC:0.9797432503\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.0470736209 \tAcc: 0.9814755155 \tTPR:0.9827905735 \tFPR:0.0195573399 \tF1:0.9801609159 \t AUC:0.9816166168\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.0437058964 \tAcc: 0.9836417970 \tTPR:0.9864488453 \tFPR:0.0189404408 \tF1:0.9822907807 \t AUC:0.9837542023\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2392341442 \tAcc: 0.9436751497 \tTPR:0.9322734832 \tFPR:0.0456334234 \tF1:0.9375992029 \tAUC:0.9433200299 \ttest cost: 0:00:05\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0425906817 \tAcc: 0.9834397810 \tTPR:0.9222164292 \tFPR:0.0087446885 \tF1:0.9112205888 \t AUC:0.9557461093\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0342048724 \tAcc: 0.9865875912 \tTPR:0.9423183872 \tFPR:0.0069072113 \tF1:0.9334094714 \t AUC:0.9668828022\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0293631634 \tAcc: 0.9884580292 \tTPR:0.9452605302 \tFPR:0.0060917987 \tF1:0.9389882966 \t AUC:0.9688457522\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0271467381 \tAcc: 0.9892335766 \tTPR:0.9432846715 \tFPR:0.0054176644 \tF1:0.9361189892 \t AUC:0.9682986305\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0251415254 \tAcc: 0.9907390511 \tTPR:0.9544328583 \tFPR:0.0047924312 \tF1:0.9478498957 \t AUC:0.9742053646\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1420682090 \tAcc: 0.9633290816 \tTPR:0.8195969658 \tFPR:0.0186314324 \tF1:0.8005782516 \tAUC:0.8976342977 \ttest cost: 0:00:09\n",
      "Test =>                 \tLoss: 0.1849481771 \tAcc: 0.9552844461 \tTPR:0.9308920683 \tFPR:0.0960753244 \tF1:0.9286522288 \tAUC:0.9169309980\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 28:  =============\n",
      "Client1 Local Train => Local Epoch: 0 \tLoss: 0.0372693186 \tAcc: 0.9848996350 \tTPR:0.9273624146 \tFPR:0.0077965145 \tF1:0.9084836700 \t AUC:0.9588586694\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 1 \tLoss: 0.0292093174 \tAcc: 0.9886405109 \tTPR:0.9441530529 \tFPR:0.0061063006 \tF1:0.9382667065 \t AUC:0.9684407702\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 2 \tLoss: 0.0263839710 \tAcc: 0.9894160584 \tTPR:0.9574956552 \tFPR:0.0061478767 \tF1:0.9446562011 \t AUC:0.9751656160\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 3 \tLoss: 0.0223162266 \tAcc: 0.9916058394 \tTPR:0.9556708377 \tFPR:0.0043555845 \tF1:0.9475595506 \t AUC:0.9750253037\tTrain cost: 0:01:05\n",
      "Client1 Local Train => Local Epoch: 4 \tLoss: 0.0223054524 \tAcc: 0.9913777372 \tTPR:0.9606992237 \tFPR:0.0042920453 \tF1:0.9529844394 \t AUC:0.9774901107\tTrain cost: 0:01:05\n",
      "Client1 Test =>                 \tLoss: 0.1477571607 \tAcc: 0.9638605442 \tTPR:0.8581673145 \tFPR:0.0208440640 \tF1:0.8166843549 \tAUC:0.9169319584 \ttest cost: 0:00:09\n",
      "Client3 Local Train => Local Epoch: 0 \tLoss: 0.1173580745 \tAcc: 0.9600903614 \tTPR:0.9548399294 \tFPR:0.0352684990 \tF1:0.9528531100 \t AUC:0.9597857152\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 1 \tLoss: 0.0759744881 \tAcc: 0.9682981928 \tTPR:0.9667467523 \tFPR:0.0319452964 \tF1:0.9624676437 \t AUC:0.9674007280\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 2 \tLoss: 0.0685388830 \tAcc: 0.9751506024 \tTPR:0.9743371657 \tFPR:0.0251722711 \tF1:0.9711339287 \t AUC:0.9745824473\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 3 \tLoss: 0.0628767592 \tAcc: 0.9766566265 \tTPR:0.9805474336 \tFPR:0.0263294923 \tF1:0.9730857845 \t AUC:0.9771089707\tTrain cost: 0:00:08\n",
      "Client3 Local Train => Local Epoch: 4 \tLoss: 0.0586939911 \tAcc: 0.9777861446 \tTPR:0.9793635309 \tFPR:0.0230749539 \tF1:0.9737334439 \t AUC:0.9781442885\tTrain cost: 0:00:08\n",
      "Client3 Test =>                 \tLoss: 0.1696878901 \tAcc: 0.9435763889 \tTPR:0.9416028189 \tFPR:0.0542132541 \tF1:0.9337923411 \tAUC:0.9436947824 \ttest cost: 0:00:01\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0494192192 \tAcc: 0.9821428571 \tTPR:0.9893338372 \tFPR:0.0322104084 \tF1:0.9866083638 \t AUC:0.9785617144\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0323518350 \tAcc: 0.9888392857 \tTPR:0.9942287598 \tFPR:0.0224043910 \tF1:0.9912492094 \t AUC:0.9859121844\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0300063544 \tAcc: 0.9895833333 \tTPR:0.9937522308 \tFPR:0.0207673279 \tF1:0.9921095570 \t AUC:0.9864924514\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0227396552 \tAcc: 0.9902529762 \tTPR:0.9947079491 \tFPR:0.0216306841 \tF1:0.9930636116 \t AUC:0.9865386325\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0239749386 \tAcc: 0.9910714286 \tTPR:0.9934989984 \tFPR:0.0130069005 \tF1:0.9933319519 \t AUC:0.9902460490\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2038662086 \tAcc: 0.9583333333 \tTPR:0.9517878682 \tFPR:0.0274688660 \tF1:0.9662545704 \tAUC:0.9621595011 \ttest cost: 0:00:01\n",
      "Client0 Local Train => Local Epoch: 0 \tLoss: 0.1108744552 \tAcc: 0.9698660714 \tTPR:0.9890068305 \tFPR:0.0927721088 \tF1:0.9808903823 \t AUC:0.9481173608\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 1 \tLoss: 0.0728697424 \tAcc: 0.9799107143 \tTPR:0.9926755387 \tFPR:0.0835390947 \tF1:0.9873853558 \t AUC:0.9544325839\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 2 \tLoss: 0.0582361822 \tAcc: 0.9832589286 \tTPR:0.9956547619 \tFPR:0.0843112245 \tF1:0.9897062427 \t AUC:0.9556717687\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 3 \tLoss: 0.0638900652 \tAcc: 0.9822741597 \tTPR:0.9946072446 \tFPR:0.0684523810 \tF1:0.9890159857 \t AUC:0.9630774318\tTrain cost: 0:00:03\n",
      "Client0 Local Train => Local Epoch: 4 \tLoss: 0.0630425505 \tAcc: 0.9799107143 \tTPR:0.9945755089 \tFPR:0.0662981859 \tF1:0.9871699547 \t AUC:0.9641386615\tTrain cost: 0:00:03\n",
      "Client0 Test =>                 \tLoss: 0.2184857889 \tAcc: 0.9375000000 \tTPR:0.9585735963 \tFPR:0.1457671958 \tF1:0.9600702849 \tAUC:0.9064032003 \ttest cost: 0:00:00\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0607810408 \tAcc: 0.9782752226 \tTPR:0.9913949292 \tFPR:0.0627490380 \tF1:0.9856366421 \t AUC:0.9643229456\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0509814065 \tAcc: 0.9824740356 \tTPR:0.9922424336 \tFPR:0.0508450336 \tF1:0.9884528522 \t AUC:0.9706987000\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0499382709 \tAcc: 0.9822366469 \tTPR:0.9924118514 \tFPR:0.0542104615 \tF1:0.9882062608 \t AUC:0.9691006950\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0450010255 \tAcc: 0.9843026706 \tTPR:0.9923878694 \tFPR:0.0425444569 \tF1:0.9895475261 \t AUC:0.9749217063\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0389915554 \tAcc: 0.9874814540 \tTPR:0.9942303031 \tFPR:0.0343898743 \tF1:0.9916626843 \t AUC:0.9799581315\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1890091797 \tAcc: 0.9562500000 \tTPR:0.9705289284 \tFPR:0.0895746399 \tF1:0.9707288745 \tAUC:0.9404771443 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1857612456 \tAcc: 0.9519040533 \tTPR:0.9361321053 \tFPR:0.0675736040 \tF1:0.9295060852 \tAUC:0.9339333173\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "============== Round 29:  =============\n",
      "Client4 Local Train => Local Epoch: 0 \tLoss: 0.0963812510 \tAcc: 0.9626499711 \tTPR:0.9789153587 \tFPR:0.0670615143 \tF1:0.9706674647 \t AUC:0.9559269222\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 1 \tLoss: 0.0905711757 \tAcc: 0.9648200347 \tTPR:0.9801373150 \tFPR:0.0621085186 \tF1:0.9722080157 \t AUC:0.9590143982\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 2 \tLoss: 0.0840889834 \tAcc: 0.9679893033 \tTPR:0.9823631973 \tFPR:0.0583226004 \tF1:0.9746777453 \t AUC:0.9620202985\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 3 \tLoss: 0.0810255580 \tAcc: 0.9691258312 \tTPR:0.9822337543 \tFPR:0.0541241235 \tF1:0.9756978526 \t AUC:0.9640548154\tTrain cost: 0:01:50\n",
      "Client4 Local Train => Local Epoch: 4 \tLoss: 0.0753241057 \tAcc: 0.9714061145 \tTPR:0.9837755715 \tFPR:0.0504172206 \tF1:0.9773821037 \t AUC:0.9666791755\tTrain cost: 0:01:50\n",
      "Client4 Test =>                 \tLoss: 0.1602952733 \tAcc: 0.9500631313 \tTPR:0.9663859051 \tFPR:0.0803727583 \tF1:0.9603566558 \tAUC:0.9429725510 \ttest cost: 0:00:16\n",
      "Client5 Local Train => Local Epoch: 0 \tLoss: 0.0679404676 \tAcc: 0.9735169492 \tTPR:0.9093620833 \tFPR:0.0153305375 \tF1:0.8992175124 \t AUC:0.9467582788\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 1 \tLoss: 0.0554143036 \tAcc: 0.9781073446 \tTPR:0.9258864880 \tFPR:0.0125954322 \tF1:0.9229261099 \t AUC:0.9561146288\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 2 \tLoss: 0.0527092805 \tAcc: 0.9804025424 \tTPR:0.9333935603 \tFPR:0.0111383907 \tF1:0.9297015955 \t AUC:0.9610332414\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 3 \tLoss: 0.0430762451 \tAcc: 0.9840190450 \tTPR:0.9412203145 \tFPR:0.0081271151 \tF1:0.9405292612 \t AUC:0.9663796120\tTrain cost: 0:00:34\n",
      "Client5 Local Train => Local Epoch: 4 \tLoss: 0.0405342119 \tAcc: 0.9847281073 \tTPR:0.9488868711 \tFPR:0.0084642374 \tF1:0.9409845962 \t AUC:0.9699928847\tTrain cost: 0:00:34\n",
      "Client5 Test =>                 \tLoss: 0.1556025214 \tAcc: 0.9594179062 \tTPR:0.8558322891 \tFPR:0.0236125264 \tF1:0.8326254162 \tAUC:0.9151487633 \ttest cost: 0:00:05\n",
      "Client6 Local Train => Local Epoch: 0 \tLoss: 0.0739044373 \tAcc: 0.9702719517 \tTPR:0.9724696171 \tFPR:0.0319373826 \tF1:0.9675489691 \t AUC:0.9702661173\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 1 \tLoss: 0.0600738428 \tAcc: 0.9764736269 \tTPR:0.9796499887 \tFPR:0.0265447227 \tF1:0.9742669579 \t AUC:0.9765526330\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 2 \tLoss: 0.0523475272 \tAcc: 0.9785760309 \tTPR:0.9827312566 \tFPR:0.0249028910 \tF1:0.9765610740 \t AUC:0.9789141828\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 3 \tLoss: 0.0492809795 \tAcc: 0.9799452320 \tTPR:0.9824667462 \tFPR:0.0214013231 \tF1:0.9779350151 \t AUC:0.9805327115\tTrain cost: 0:00:37\n",
      "Client6 Local Train => Local Epoch: 4 \tLoss: 0.0446758429 \tAcc: 0.9831668814 \tTPR:0.9855824291 \tFPR:0.0183448502 \tF1:0.9818614827 \t AUC:0.9836187895\tTrain cost: 0:00:37\n",
      "Client6 Test =>                 \tLoss: 0.2447400208 \tAcc: 0.9410553892 \tTPR:0.9281322789 \tFPR:0.0479316433 \tF1:0.9355575540 \tAUC:0.9401003178 \ttest cost: 0:00:05\n",
      "Client7 Local Train => Local Epoch: 0 \tLoss: 0.0407448543 \tAcc: 0.9839285714 \tTPR:0.9916568533 \tFPR:0.0278156246 \tF1:0.9876578000 \t AUC:0.9819206144\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 1 \tLoss: 0.0336468904 \tAcc: 0.9873511905 \tTPR:0.9919642348 \tFPR:0.0198994293 \tF1:0.9903680227 \t AUC:0.9860324028\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 2 \tLoss: 0.0277115222 \tAcc: 0.9884672619 \tTPR:0.9928278127 \tFPR:0.0232239982 \tF1:0.9915173195 \t AUC:0.9848019072\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 3 \tLoss: 0.0217972941 \tAcc: 0.9918154762 \tTPR:0.9963134586 \tFPR:0.0179828983 \tF1:0.9938849578 \t AUC:0.9891652802\tTrain cost: 0:00:08\n",
      "Client7 Local Train => Local Epoch: 4 \tLoss: 0.0222240001 \tAcc: 0.9921875000 \tTPR:0.9953175688 \tFPR:0.0163771744 \tF1:0.9940262181 \t AUC:0.9894701972\tTrain cost: 0:00:08\n",
      "Client7 Test =>                 \tLoss: 0.2235190155 \tAcc: 0.9600694444 \tTPR:0.9587246960 \tFPR:0.0355610666 \tF1:0.9681462339 \tAUC:0.9615818147 \ttest cost: 0:00:01\n",
      "Client8 Local Train => Local Epoch: 0 \tLoss: 0.0617536349 \tAcc: 0.9790170623 \tTPR:0.9899823619 \tFPR:0.0584186214 \tF1:0.9862139433 \t AUC:0.9657818702\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 1 \tLoss: 0.0550102993 \tAcc: 0.9796921365 \tTPR:0.9915180211 \tFPR:0.0596040335 \tF1:0.9866984888 \t AUC:0.9659569938\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 2 \tLoss: 0.0462503656 \tAcc: 0.9821958457 \tTPR:0.9914144466 \tFPR:0.0473488513 \tF1:0.9880583605 \t AUC:0.9720327977\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 3 \tLoss: 0.0425356914 \tAcc: 0.9846735905 \tTPR:0.9934891432 \tFPR:0.0446922514 \tF1:0.9900069677 \t AUC:0.9743984459\tTrain cost: 0:00:32\n",
      "Client8 Local Train => Local Epoch: 4 \tLoss: 0.0409365373 \tAcc: 0.9851632047 \tTPR:0.9932460747 \tFPR:0.0406854273 \tF1:0.9901800284 \t AUC:0.9762803237\tTrain cost: 0:00:32\n",
      "Client8 Test =>                 \tLoss: 0.1924099745 \tAcc: 0.9582092476 \tTPR:0.9706866802 \tFPR:0.0866582077 \tF1:0.9721529586 \tAUC:0.9420142363 \ttest cost: 0:00:05\n",
      "Test =>                 \tLoss: 0.1953133611 \tAcc: 0.9537630238 \tTPR:0.9359523699 \tFPR:0.0548272404 \tF1:0.9337677637 \tAUC:0.9403635366\n",
      "-----------------------------------------------------------\n",
      "-------------- FedServer: Federation process  -------------\n",
      "-----------------------------------------------------------\n",
      "Training and Test completed! total time cost: 6:55:02\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and Test Begin!\")\n",
    "net_glob.train()\n",
    "w_net_glob = net_glob.state_dict()\n",
    "t0 = time.time()\n",
    "\n",
    "lr = 2e-6\n",
    "\n",
    "local_test[\"loss\"] = []\n",
    "local_test[\"acc\"] = []\n",
    "local_test[\"tpr\"] = []\n",
    "local_test[\"fpr\"] = []\n",
    "local_test[\"f1\"] = []\n",
    "local_test[\"auc\"] = []\n",
    "\n",
    "local_testing[\"loss\"] = []\n",
    "local_testing[\"acc\"] = []\n",
    "local_testing[\"tpr\"] = []\n",
    "local_testing[\"fpr\"] = []\n",
    "local_testing[\"f1\"] = []\n",
    "local_testing[\"auc\"] = []\n",
    "\n",
    "for iter in range(epochs):\n",
    "    print(\"============== Round {}:  =============\".format(iter))\n",
    "    idx_collect = []\n",
    "    m = max(int(frac * num_users) ,1)\n",
    "    idxs_users = np.random.choice(range(num_users), m, replace = False)\n",
    "    w_locals_client = []\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local = Client(device, idx, lr, local_epochs, batch_size, dataset_train, dict_user_train[idx], dict_user_test[idx])\n",
    "        w_client, client_loss, client_acc = local.train(net = copy.deepcopy(net_glob).to(device))\n",
    "        w_locals_client.append(copy.deepcopy(w_client))\n",
    "        loss, acc, tpr, fpr, f1, auc = local.evaluate(net = copy.deepcopy(net_glob).to(device), ell=iter)\n",
    "        loss_list.append(loss)\n",
    "        acc_list.append(acc)\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "    local_testing[\"loss\"].append(sum(loss_list)/len(loss_list))\n",
    "    local_testing[\"acc\"].append(sum(acc_list)/len(acc_list))\n",
    "    local_testing[\"tpr\"].append(sum(tpr_list)/len(tpr_list))\n",
    "    local_testing[\"fpr\"].append(sum(fpr_list)/len(fpr_list))\n",
    "    local_testing[\"f1\"].append(sum(f1_list)/len(f1_list))\n",
    "    local_testing[\"auc\"].append(sum(auc_list)/len(auc_list))\n",
    "    print(\"Test =>                 \\tLoss: {:.10f} \\tAcc: {:.10f} \\tTPR:{:.10f} \\tFPR:{:.10f} \\tF1:{:.10f} \\tAUC:{:.10f}\".format(sum(loss_list)/len(loss_list), sum(acc_list)/len(acc_list), sum(tpr_list)/len(tpr_list), sum(fpr_list)/len(fpr_list), sum(f1_list)/len(f1_list), sum(auc_list)/len(auc_list)  ))\n",
    "    \n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"-------------- FedServer: Federation process  -------------\")\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    w_net_glob = FedAvg(w_locals_client)\n",
    "    net_glob.load_state_dict(w_net_glob)\n",
    "elapsed = format_time(time.time()-t0)\n",
    "print(\"Training and Test completed! total time cost: {:}\".format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Final Result =============\n",
      "Client0 Test =>                 \tLoss: 0.2389818509 \tAcc: 0.9368990385 \tTPR:0.9343643005 \tFPR:0.0489417989 \tF1:0.9563451662 \tAUC:0.9427112508 \ttest cost: 0:00:00\n",
      "Client1 Test =>                 \tLoss: 0.2210538279 \tAcc: 0.9462159864 \tTPR:0.9435941043 \tFPR:0.0529677392 \tF1:0.7447151154 \tAUC:0.9441130571 \ttest cost: 0:00:09\n",
      "Client2 Test =>                 \tLoss: 0.2164640588 \tAcc: 0.9394667832 \tTPR:0.9396798669 \tFPR:0.0595200254 \tF1:0.9604867284 \tAUC:0.9400799207 \ttest cost: 0:00:05\n",
      "Client3 Test =>                 \tLoss: 0.1912853598 \tAcc: 0.9442708333 \tTPR:0.9371735019 \tFPR:0.0558400358 \tF1:0.9309470248 \tAUC:0.9406667330 \ttest cost: 0:00:01\n",
      "Client4 Test =>                 \tLoss: 0.2320341593 \tAcc: 0.9426136364 \tTPR:0.9403129818 \tFPR:0.0526076851 \tF1:0.9534674579 \tAUC:0.9438526483 \ttest cost: 0:00:16\n",
      "Client5 Test =>                 \tLoss: 0.2136635811 \tAcc: 0.9470823799 \tTPR:0.9401368003 \tFPR:0.0511443548 \tF1:0.8149667015 \tAUC:0.9440971348 \ttest cost: 0:00:05\n",
      "Client6 Test =>                 \tLoss: 0.2176686410 \tAcc: 0.9447979042 \tTPR:0.9433157945 \tFPR:0.0539315696 \tF1:0.9387279591 \tAUC:0.9446921125 \ttest cost: 0:00:05\n",
      "Client7 Test =>                 \tLoss: 0.2108087024 \tAcc: 0.9472373188 \tTPR:0.9374283924 \tFPR:0.0278801060 \tF1:0.9587482775 \tAUC:0.9547741432 \ttest cost: 0:00:01\n",
      "Client8 Test =>                 \tLoss: 0.2123484124 \tAcc: 0.9422413793 \tTPR:0.9436831938 \tFPR:0.0692754005 \tF1:0.9609974721 \tAUC:0.9370083522 \ttest cost: 0:00:05\n",
      "Client9 Test =>                 \tLoss: 0.2415254200 \tAcc: 0.9275641026 \tTPR:0.9296532138 \tFPR:0.1159420290 \tF1:0.9589547617 \tAUC:0.9063438455 \ttest cost: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "idx_collect = [i for i in range(num_users)]\n",
    "print(\"============= Final Result =============\")\n",
    "for idx in idx_collect:\n",
    "    loss_test_collect[idx] = []\n",
    "    acc_test_collect[idx] = []\n",
    "    TPR_test_collect[idx] = []\n",
    "    FPR_test_collect[idx] = []\n",
    "    f1_test_collect[idx] = []\n",
    "    AUC_test_collect[idx] = []\n",
    "    local = Client(device, idx, lr, local_epochs, batch_size, dataset_train, dict_user_train[idx], dict_user_test[idx])\n",
    "    loss, acc, tpr, fpr, f1, auc = local.evaluate(net = copy.deepcopy(net_glob).to(device), ell=0)\n",
    "    loss_test_collect[idx].append(loss)\n",
    "    acc_test_collect[idx].append(acc)\n",
    "    TPR_test_collect[idx].append(tpr)\n",
    "    FPR_test_collect[idx].append(fpr)\n",
    "    f1_test_collect[idx].append(f1)\n",
    "    AUC_test_collect[idx].append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "f = xlwt.Workbook('encoding = utf-8')\n",
    "sheet1 = f.add_sheet('sheet1',cell_overwrite_ok=True)\n",
    "for i in range(len(loss_collect)):\n",
    "    sheet1.write(i+1,0,loss_collect[i])\n",
    "for i in range(len(acc_collect)):\n",
    "    sheet1.write(i+1,1,acc_collect[i])\n",
    "for i in range(len(TPR_collect)):\n",
    "    sheet1.write(i+1,2,TPR_collect[i])\n",
    "for i in range(len(FPR_collect)):\n",
    "    sheet1.write(i+1,3,FPR_collect[i])\n",
    "for i in range(len(F1_collect)):\n",
    "    sheet1.write(i+1,4,F1_collect[i])\n",
    "for i in range(len(AUC_collect)):\n",
    "    sheet1.write(i+1,5,AUC_collect[i])\n",
    "\n",
    "f.save('result.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(num_users):\n",
    "    f = xlwt.Workbook('encoding = utf-8')\n",
    "    sheet1 = f.add_sheet('sheet1', cell_overwrite_ok=True)\n",
    "    for j in range(len(loss_train_collect[i])):\n",
    "        sheet1.write(j+1, 0, loss_train_collect[i][j])\n",
    "    for j in range(len(acc_train_collect[i])):\n",
    "        sheet1.write(j+1, 1, acc_train_collect[i][j])\n",
    "    for j in range(len(TPR_train_collect[i])):\n",
    "        sheet1.write(j+1, 2, TPR_train_collect[i][j])\n",
    "    for j in range(len(FPR_train_collect[i])):\n",
    "        sheet1.write(j+1, 3, FPR_train_collect[i][j])\n",
    "    for j in range(len(f1_train_collect[i])):\n",
    "        sheet1.write(j+1, 4, f1_train_collect[i][j])\n",
    "    for j in range(len(AUC_train_collect[i])):\n",
    "        sheet1.write(j+1, 5, AUC_train_collect[i][j])\n",
    "\n",
    "    f.save('result_client{:}.xls'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(num_users):\n",
    "    f = xlwt.Workbook('encoding = utf-8')\n",
    "    sheet1 = f.add_sheet('sheet1', cell_overwrite_ok=True)\n",
    "    for j in range(len(loss_test_collect[i])):\n",
    "        sheet1.write(j+1, 0, loss_test_collect[i][j])\n",
    "    for j in range(len(acc_test_collect[i])):\n",
    "        sheet1.write(j+1, 1, acc_test_collect[i][j])\n",
    "    for j in range(len(TPR_test_collect[i])):\n",
    "        sheet1.write(j+1, 2, TPR_test_collect[i][j])\n",
    "    for j in range(len(FPR_test_collect[i])):\n",
    "        sheet1.write(j+1, 3, FPR_test_collect[i][j])\n",
    "    for j in range(len(f1_test_collect[i])):\n",
    "        sheet1.write(j+1, 4, f1_test_collect[i][j])\n",
    "    for j in range(len(AUC_test_collect[i])):\n",
    "        sheet1.write(j+1, 5, AUC_test_collect[i][j])\n",
    "\n",
    "    f.save('result_test_client{:}.xls'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "f = xlwt.Workbook('encoding = utf-8')\n",
    "sheet1 = f.add_sheet('sheet1',cell_overwrite_ok=True)\n",
    "for i in range(len(local_test[\"loss\"])):\n",
    "    sheet1.write(i+1,0,local_test[\"loss\"][i])\n",
    "for i in range(len(local_test[\"acc\"])):\n",
    "    sheet1.write(i+1,1,local_test[\"acc\"][i])\n",
    "for i in range(len(local_test[\"tpr\"])):\n",
    "    sheet1.write(i+1,2,local_test[\"tpr\"][i])\n",
    "for i in range(len(local_test[\"fpr\"])):\n",
    "    sheet1.write(i+1,3,local_test[\"fpr\"][i])\n",
    "for i in range(len(local_test[\"f1\"])):\n",
    "    sheet1.write(i+1,4,local_test[\"f1\"][i])\n",
    "for i in range(len(local_test[\"auc\"])):\n",
    "    sheet1.write(i+1,5,local_test[\"auc\"][i])\n",
    "\n",
    "f.save('Local_Test.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt\n",
    "f = xlwt.Workbook('encoding = utf-8')\n",
    "sheet1 = f.add_sheet('sheet1',cell_overwrite_ok=True)\n",
    "for i in range(len(local_testing[\"loss\"])):\n",
    "    sheet1.write(i+1,0,local_testing[\"loss\"][i])\n",
    "for i in range(len(local_testing[\"acc\"])):\n",
    "    sheet1.write(i+1,1,local_testing[\"acc\"][i])\n",
    "for i in range(len(local_testing[\"tpr\"])):\n",
    "    sheet1.write(i+1,2,local_testing[\"tpr\"][i])\n",
    "for i in range(len(local_testing[\"fpr\"])):\n",
    "    sheet1.write(i+1,3,local_testing[\"fpr\"][i])\n",
    "for i in range(len(local_testing[\"f1\"])):\n",
    "    sheet1.write(i+1,4,local_testing[\"f1\"][i])\n",
    "for i in range(len(local_testing[\"auc\"])):\n",
    "    sheet1.write(i+1,5,local_testing[\"auc\"][i])\n",
    "\n",
    "f.save('Local_Testing.xls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
